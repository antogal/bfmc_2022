{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 1.10.0+cu113\n",
      "Torchvision version: 0.11.1+cu113\n",
      "OpenCV version: 4.5.4\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "\n",
    "print(\"Pytorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"OpenCV version:\", cv.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dnn_Net 0x7efcc84a48b0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 14.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# TESTING, DONT RUN THIS CELL\n",
    "# Model\n",
    "from helper_functions import *\n",
    "# #get sleep package\n",
    "# from time import sleep\n",
    "# # model = torch.hub.load('ultralytics/yolov5', 'yolov5n', pretrained=True) #faster but less accurate\n",
    "# yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True) \n",
    "# # model = torch.hub.load('ultralytics/yolov5', 'yolov5n6', pretrained=True) \n",
    "# # model = torch.hub.load('ultralytics/yolov3', 'yolov3') #bad \n",
    "\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "#load classes\n",
    "class_list = []\n",
    "with open(\"models/classes.txt\", \"r\") as f:\n",
    "    class_list = [cname.strip() for cname in f.readlines()]\n",
    "\n",
    "def unwrap_detection(output_data):\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    rows = output_data.shape[0]\n",
    "    for r in range(rows):\n",
    "        row = output_data[r]\n",
    "        confidence = row[4]\n",
    "        if confidence >= 0.4:\n",
    "            classes_scores = row[5:]\n",
    "            _, _, _, max_indx = cv.minMaxLoc(classes_scores)\n",
    "            class_id = max_indx[1]\n",
    "            if (classes_scores[class_id] > .25):\n",
    "                confidences.append(confidence)\n",
    "                class_ids.append(class_id)\n",
    "                x, y, w, h = row[0].item(), row[1].item(), row[2].item(), row[3].item() \n",
    "                box = [int(x), int(y), int(w), int(h)]\n",
    "                boxes.append(box)\n",
    "    indexes = cv.dnn.NMSBoxes(boxes, confidences, 0.25, 0.45) \n",
    "    result_class_ids = []\n",
    "    result_confidences = []\n",
    "    result_boxes = []\n",
    "    for i in indexes:\n",
    "        result_confidences.append(confidences[i])\n",
    "        result_class_ids.append(class_ids[i])\n",
    "        result_boxes.append(boxes[i])\n",
    "    return result_class_ids, result_confidences, result_boxes\n",
    "\n",
    "onnx_yolo_path = \"models/yolov5s_128_320.onnx\"\n",
    "\n",
    "yolo =  cv.dnn.readNetFromONNX(onnx_yolo_path) \n",
    "print(yolo)\n",
    "\n",
    "images = [cv.imread(f\"test_imgs/img_{i+1}.png\") for i in range(100)]\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, (320, 240))\n",
    "    #crop to 320x128\n",
    "    image = image[:128,:]\n",
    "    assert image.shape == (128, 320, 3), f\"Image shape is {image.shape}\"\n",
    "    cv.imshow(\"original image\", image)\n",
    "    cv.waitKey(1)\n",
    "    input = cv.dnn.blobFromImage(image, 1/255.0, (320, 128), (0, 0, 0), swapRB=True, crop=False)\n",
    "    # print(input.shape)\n",
    "    yolo.setInput(input)\n",
    "    preds = yolo.forward()\n",
    "    output = preds[0]\n",
    "    result_class_ids, result_confidences, result_boxes = unwrap_detection(output)\n",
    "    # print(result_class_ids, result_confidences, result_boxes)\n",
    "    for i in range(len(result_boxes)):\n",
    "        box = result_boxes[i]\n",
    "        cv.rectangle(image, (box[0], box[1]), (box[0] + box[2], box[1] + box[3]), (0, 255, 0), 2)\n",
    "        cv.putText(image, class_list[result_class_ids[i]], (box[0], box[1]), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv.imshow(\"image\", image)\n",
    "    cv.waitKey(1)\n",
    "\n",
    "cv.waitKey(1)\n",
    "\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.4) /tmp/pip-req-build-3129w7z7/opencv/modules/dnn/src/darknet/darknet_importer.cpp:207: error: (-212:Parsing error) Failed to parse NetParameter file: yolov10_tiny-custom.cfg in function 'readNetFromDarknet'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4350/3130164949.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Loading Yolo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadNetFromDarknet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolov10_tiny-custom.cfg\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mr\"models/yolov3_tiny-custom_total.weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"coco.names\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.4) /tmp/pip-req-build-3129w7z7/opencv/modules/dnn/src/darknet/darknet_importer.cpp:207: error: (-212:Parsing error) Failed to parse NetParameter file: yolov10_tiny-custom.cfg in function 'readNetFromDarknet'\n"
     ]
    }
   ],
   "source": [
    "# import cv2 \n",
    "# import numpy as np \n",
    "# import time \n",
    "# #Loading Yolo \n",
    "# net = cv2.dnn.readNetFromDarknet(\"yolov10_tiny-custom.cfg\",r\"models/yolov3_tiny-custom_total.weights\")\n",
    "# classes = [] \n",
    "# with open(\"coco.names\", \"r\") as f: \n",
    "#   classes = [line.strip() for line in f.readlines()] \n",
    "# layer_names = net.getLayerNames() \n",
    "# outputlayers=[layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()] \n",
    "# print(outputlayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# model = torch.hub.load('ultralytics/yolov5', 'yolov5n', pretrained=True) #faster but less accurate\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True) \n",
    "# model = torch.hub.load('ultralytics/yolov5', 'yolov5n6', pretrained=True) \n",
    "# model = torch.hub.load('ultralytics/yolov3', 'yolov3') #bad \n",
    "model.to(device)\n",
    "\n",
    "# Analyze network\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# for param_name, param in model.named_parameters():\n",
    "#     print(param_name)\n",
    "\n",
    "# for i, (k, v) in enumerate(model.named_parameters()):\n",
    "#     print(f'{i} - {k}')\n",
    "\n",
    "#https://github.com/ultralytics/yolov5/issues/1314\n",
    "\n",
    "#backnbone is layers 0->9\n",
    "\n",
    "backbone_layers = [f'model.{x}' for x in range(10)]\n",
    "\n",
    "backbone = nn.Sequential(\n",
    "    model.model.model.model[0],\n",
    "    model.model.model.model[1],\n",
    "    model.model.model.model[2],\n",
    "    model.model.model.model[3],\n",
    "    model.model.model.model[4],\n",
    "    model.model.model.model[5],\n",
    "    model.model.model.model[6],\n",
    "    model.model.model.model[7],\n",
    "    model.model.model.model[8],\n",
    "    model.model.model.model[9],\n",
    "    model.model.model.model[10],\n",
    "    )\n",
    "\n",
    "# print(backbone)\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, backbone): #(default for 640x320)\n",
    "        super().__init__()\n",
    "\n",
    "        ## Pretrained layers\n",
    "        self.pretrained = backbone\n",
    "\n",
    "        ### Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutions\n",
    "        x = self.pretrained(x)\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        return x\n",
    "\n",
    "class Detector(nn.Module):\n",
    "    def __init__(self, add_inputs=4, regr_out=22, class_out=13, features=76800): #(default for 640x320)\n",
    "        super().__init__()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=features+add_inputs, out_features=512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=512, out_features=regr_out+class_out),\n",
    "        )\n",
    "        # #regression output\n",
    "        # self.lin_regr = nn.Sequential(\n",
    "        #     # First linear layer\n",
    "        #     nn.Linear(in_features=1024, out_features=512),\n",
    "        #     nn.ReLU(True),\n",
    "        #     # nn.Dropout(p=0.5),\n",
    "        #     # Second linear\n",
    "        #     nn.Linear(in_features=512, out_features=regr_out)\n",
    "        # )\n",
    "        # # classification output\n",
    "        # self.lin_class = nn.Sequential(\n",
    "        #     # First linear layer\n",
    "        #     nn.Linear(in_features=1024, out_features=512),\n",
    "        #     nn.ReLU(True),\n",
    "        #     # nn.Dropout(p=0.5),\n",
    "        #     # Second linear\n",
    "        #     nn.Linear(in_features=512, out_features=class_out)\n",
    "        # )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "feature_extractor=FeatureExtractor(backbone)\n",
    "#define detector\n",
    "detector = Detector(add_inputs=4, regr_out=22-16+2, class_out=13, features=20480)\n",
    "\n",
    "#freeeze backbone\n",
    "for param in feature_extractor.pretrained.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "feature_extractor.to(device)\n",
    "detector.to(device)\n",
    "\n",
    "# #check\n",
    "# for param_name, param in detector.named_parameters():\n",
    "#     print('%s \\t- requires_grad=%s' % (param_name, param.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test backbone\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "#resize to 480 x 640\n",
    "img = cv.resize(img, (320, 240))\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float().permute(2, 0, 1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "detector.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    data = torch.zeros(1, 4).to(device)\n",
    "    feat = feature_extractor(img)\n",
    "    input = torch.cat((feat, data), dim=1)\n",
    "    print(input.shape)\n",
    "    out = detector(input) \n",
    "    print(out.shape) # (320, 240)->torch.Size([1, 20480])\n",
    "                    # (640, 480)->torch.Size([1, 76800])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, in_ram=False):\n",
    "        self.transform = transform\n",
    "        self.in_ram = in_ram\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        class_labels = []\n",
    "        with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            labels = []\n",
    "            for i in tqdm(range(len(lines))):\n",
    "                line = lines[i]\n",
    "                sample = line.split(',')\n",
    "                #convert to float\n",
    "                label = np.array([float(s) for s in sample])\n",
    "                #convert to tensor\n",
    "                label = torch.from_numpy(label).float()\n",
    "                # img = img.unsqueeze(0)\n",
    "                class_labels.append(label)\n",
    "\n",
    "        input_data = []\n",
    "        with open(folder+'/input_data.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            labels = []\n",
    "            for i in tqdm(range(len(lines))):\n",
    "                line = lines[i]\n",
    "                sample = line.split(',')\n",
    "                #convert to float\n",
    "                label = np.array([float(s) for s in sample])\n",
    "                #convert to tensor\n",
    "                label = torch.from_numpy(label).float()\n",
    "                # img = img.unsqueeze(0)\n",
    "                input_data.append(label)\n",
    "\n",
    "        #load labels\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            labels = []\n",
    "            for i in tqdm(range(len(lines))):\n",
    "                line = lines[i]\n",
    "                sample = line.split(',')\n",
    "                #convert to float\n",
    "                label = np.array([float(s) for s in sample])\n",
    "                #convert to tensor\n",
    "                label = torch.from_numpy(label).float()\n",
    "                #load img\n",
    "                # img = cv.imread(folder+f'/img_{i+1}.png')\n",
    "                # img = cv.resize(img, (320, 240))\n",
    "                if self.in_ram:\n",
    "                    img = cv.imread(folder+f'/img_{i+1}.png')\n",
    "                    img = cv.resize(img, (320, 240))\n",
    "                    img = torch.from_numpy(img).float().permute(2, 0, 1)\n",
    "                    self.data.append((img, input_data[i], label, class_labels[i]))  \n",
    "                else:\n",
    "                    self.data.append((input_data[i], label, class_labels[i])) #no image\n",
    "                    #save img to disk\n",
    "                    # cv.imwrite(folder+f'/img_{i+1}.png', img)\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Our sample is the element idx of the list self.data\n",
    "        sample = self.data[idx]\n",
    "        if not self.in_ram:\n",
    "            # img = read_image(read_image(img_path))\n",
    "            img = cv.imread(self.folder+f'/img_{idx+1}.png')\n",
    "            img = cv.resize(img, (320, 240))\n",
    "            img = torch.from_numpy(img).float().permute(2, 0, 1)\n",
    "            sample = (img, sample[0], sample[1], sample[2])\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "#create dataset\n",
    "train_dataset = CsvDataset(folder='training_imgs')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)\n",
    "print(sample[2].shape)\n",
    "print(sample[3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(ext, det, dataloader, class_loss_fn, regr_loss_fn, optimizer, device):\n",
    "    # Set the model to training mode\n",
    "    ext.eval() #dont train the extractor\n",
    "    det.train() #train detector\n",
    "    # Initialize the loss\n",
    "    train_loss_class = []\n",
    "    train_loss_regr = []\n",
    "\n",
    "    err_losses = []\n",
    "    dist_losses = []\n",
    "    curv_losses = []\n",
    "    bb_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (img, input_data, regr_label, class_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        img, input_data, regr_label, class_label = img.to(device), input_data.to(device), regr_label.to(device), class_label.to(device)\n",
    "        # Compute the features\n",
    "        features = ext(img)\n",
    "        #concatenate features and input_data\n",
    "        input = torch.cat((features, input_data), dim=1)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = det(input)\n",
    "\n",
    "        #regression 22 values\n",
    "        #classification: 3 states, 3 next states, 7 signs\n",
    "        regr_out = output[:, :22-16+2]\n",
    "        err_out = regr_out[:, :2]\n",
    "        dist_out = regr_out[:, 2]\n",
    "        curv_out = regr_out[:, 3]\n",
    "        bb_out = regr_out[:, 4:8]\n",
    "\n",
    "        err_label = regr_label[:, :2]\n",
    "        dist_label = regr_label[:, 2]\n",
    "        curv_label = regr_label[:, 3]\n",
    "        bb_label = regr_label[:, 4:8]\n",
    "\n",
    "        state_out = output[:, 22-16+2:25-16+2]\n",
    "        next_out = output[:, 25-16+2:28-16+2]\n",
    "        sign_out = output[:, 28-16+2:]\n",
    "        \n",
    "        state_label = class_label[:, :3]\n",
    "        next_label = class_label[:, 3:6]\n",
    "        sign_label = class_label[:, 6:]\n",
    "\n",
    "        # Compute the losses\n",
    "        # regr_loss = regr_loss_fn(regr_out, regr_label)\n",
    "\n",
    "        err_loss = 50.0*regr_loss_fn(err_out, err_label)\n",
    "        dist_loss = 0.1*regr_loss_fn(dist_out, dist_label)\n",
    "        curv_loss = 0.1*regr_loss_fn(curv_out, curv_label)\n",
    "        bb_loss = 1.0*regr_loss_fn(bb_out, bb_label)\n",
    "\n",
    "\n",
    "        state_loss = 0.5*class_loss_fn(state_out, state_label)\n",
    "        next_loss = 0.1*class_loss_fn(next_out, next_label)\n",
    "        sign_loss = 5.0*class_loss_fn(sign_out, sign_label)\n",
    "        loss = err_loss + dist_loss + curv_loss + bb_loss + state_loss + next_loss + sign_loss\n",
    "\n",
    "        \n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        #batch loss\n",
    "        c_loss = (state_loss + next_loss + sign_loss).detach().cpu().numpy()\n",
    "        train_loss_class.append(c_loss)\n",
    "        err_losses.append(err_loss.detach().cpu().numpy())\n",
    "        dist_losses.append(dist_loss.detach().cpu().numpy())\n",
    "        curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "        bb_losses.append(bb_loss.detach().cpu().numpy())\n",
    "    # Return the average training loss\n",
    "    train_loss_c = np.mean(train_loss_class)\n",
    "    err_loss = np.mean(err_losses)\n",
    "    dist_loss = np.mean(dist_losses)\n",
    "    curv_loss = np.mean(curv_losses)\n",
    "    bb_loss = np.mean(bb_losses)\n",
    "    # print(f\"Training loss: {train_loss}\")\n",
    "    return train_loss_c, err_loss, dist_loss, curv_loss, bb_loss\n",
    "\n",
    "def get_avg_loss(ext, det, dataloader, class_loss_fn, regr_loss_fn, device):\n",
    "    ext.eval()\n",
    "    det.eval()\n",
    "    class_losses = []\n",
    "    regr_losses = []\n",
    "    with torch.no_grad():\n",
    "        for (img, input_data, regr_label, class_label) in tqdm(dataloader):\n",
    "            # Move the input and target data to the selected device\n",
    "            img, input_data, regr_label, class_label = img.to(device), input_data.to(device), regr_label.to(device), class_label.to(device)\n",
    "            # Compute the features\n",
    "            features = ext(img)\n",
    "            #concatenate features and input_data\n",
    "            input = torch.cat((features, input_data), dim=1)\n",
    "            # Compute the output\n",
    "            output = det(input)\n",
    "            \n",
    "            #regression 22 values\n",
    "            #classification: 3 states, 3 next states, 7 signs\n",
    "            regr_out = output[:, :22]\n",
    "            state_out = output[:, 22:25]\n",
    "            next_out = output[:, 25:28]\n",
    "            sign_out = output[:, 28:]\n",
    "            \n",
    "            state_label = class_label[:, :3]\n",
    "            next_label = class_label[:, 3:6]\n",
    "            sign_label = class_label[:, 6:]\n",
    "\n",
    "            # Compute the losses\n",
    "            regr_loss = regr_loss_fn(regr_out, regr_label)\n",
    "            state_loss = class_loss_fn(state_out, state_label)\n",
    "            next_loss = class_loss_fn(next_out, next_label)\n",
    "            sign_loss = class_loss_fn(sign_out, sign_label)\n",
    "            class_loss = state_loss + next_loss + sign_loss\n",
    "\n",
    "            class_losses.append(class_loss.detach().cpu().numpy())\n",
    "            regr_losses.append(regr_loss.detach().cpu().numpy())\n",
    "    # Return the accuracy and test loss\n",
    "    class_loss = np.mean(class_losses)\n",
    "    regr_loss = np.mean(regr_losses)\n",
    "    return class_loss, regr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load models\n",
    "# detector.load_state_dict(torch.load('detector.pt'))\n",
    "# feature_extractor.load_state_dict(torch.load('feature_extractor.pt'))\n",
    "\n",
    "#parameters\n",
    "lr = 0.001\n",
    "epochs = 3\n",
    "optimizer = torch.optim.Adam(detector.parameters(), lr=lr)\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "class_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train_loss_c, err_loss, dist_loss, curv_loss, bb_loss = train_epoch(feature_extractor, detector, train_dataloader, class_loss_fn, regr_loss_fn, optimizer, device)\n",
    "    print(f\"err_loss: {err_loss}\")\n",
    "    print(f\"dist_loss: {dist_loss}\")\n",
    "    print(f\"curv_loss: {curv_loss}\")\n",
    "    print(f\"bb_loss: {bb_loss}\")\n",
    "    print(f\"Classification loss: {train_loss_c}\")\n",
    "    torch.save(detector.state_dict(), 'models/detector.pt')\n",
    "    torch.save(feature_extractor.state_dict(), 'models/feature_extractor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "test_dataset = CsvDataset(folder='test_imgs')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "#get accuracy\n",
    "train_class_loss, train_regr_loss = get_avg_loss(feature_extractor, detector, train_dataloader, class_loss_fn, regr_loss_fn, device)\n",
    "test_class_loss, test_regr_loss = get_avg_loss(feature_extractor, detector, test_dataloader, class_loss_fn, regr_loss_fn, device)\n",
    "\n",
    "print(f\"Training classification loss: {train_class_loss}\")\n",
    "print(f\"Training regression loss: {train_regr_loss}\\n\")\n",
    "print(f\"Testing classification loss: {test_class_loss}\")\n",
    "print(f\"Testing regression loss: {test_regr_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset.data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.load_state_dict(torch.load('models/detector.pt'))\n",
    "feature_extractor.load_state_dict(torch.load('models/feature_extractor.pt'))\n",
    "\n",
    "# #save pytorch model\n",
    "# torch.save(detector.state_dict(), 'detector.pt')\n",
    "# torch.save(feature_extractor.state_dict(), 'feature_extractor.pt')\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "detector.to(device)\n",
    "feature_extractor.to(device)\n",
    " \n",
    "onnx_detector_path = \"models/detector.onnx\"\n",
    "onnx_feature_extractor_path = \"models/feature_extractor.onnx\"\n",
    "\n",
    "# set the model to inference mode\n",
    "detector.eval()\n",
    "feature_extractor.eval()\n",
    " \n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, 3, 240, 320)\n",
    "dummy_input2 = torch.randn(1, 20484)\n",
    "torch.onnx.export(feature_extractor, dummy_input, onnx_feature_extractor_path, verbose=True)\n",
    "torch.onnx.export(detector, dummy_input2, onnx_detector_path, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with opencv\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "det =  cv.dnn.readNetFromONNX(onnx_detector_path) \n",
    "ext = cv.dnn.readNetFromONNX(onnx_feature_extractor_path)\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, (320, 240),(0, 0, 0), swapRB=True, crop=False)\n",
    "    ext.setInput(blob)\n",
    "    features = ext.forward()\n",
    "    # print(features.shape)\n",
    "    action_vec = np.ones((1,4))\n",
    "    input = np.concatenate((features, action_vec), axis=1)\n",
    "    # print(input.shape)\n",
    "    det.setInput(input)\n",
    "    preds = det.forward()\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
