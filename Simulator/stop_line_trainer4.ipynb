{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/stop_line_estimator.pt'\n",
    "onnx_model_path = \"models/stop_line_estimator.onnx\"\n",
    "max_load = 250_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE\n",
    "\n",
    "# class StopLineEstimator(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             nn.Linear(in_features=4*4*4, out_features=32),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(in_features=32, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# stop_line_estimator = StopLineEstimator(out_dim=3,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class StopLineEstimator(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        prob = 0.2\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 28\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=14\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(8, 8, kernel_size=5, stride=1), #out = 10\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=5\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(8, 128, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            #normalize\n",
    "            # nn.BatchNorm1d(3*3*4),\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=1*1*128, out_features=32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=32, out_features=16),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(in_features=16, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "stop_line_estimator = StopLineEstimator(out_dim=3,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "stop_line_estimator.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = stop_line_estimator(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from time import time, sleep\n",
    "\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "    #convert to gray\n",
    "    img = cv.resize(img, (4*SIZE[1], 4*SIZE[0]))\n",
    "\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "        axes_length = (randint(10//4, 50//4), randint(50//4, 300//4))\n",
    "        angle = randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (50,50))\n",
    "    noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = np.clip(light, 0, 51)\n",
    "    light *= 5\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 5), randint(1, 5)))\n",
    "\n",
    "    # cut the top third of the image, \n",
    "    img = img[int(img.shape[0]*(2/5)):,:] ################################# 2/5 frame[int(frame.shape[0]*(2/5)):,:]\n",
    "\n",
    "    #edges\n",
    "    img = cv.resize(img, (2*SIZE[1], 2*SIZE[0]))\n",
    "\n",
    "    # erosion and dilation\n",
    "    r = randint(0, 5)\n",
    "    if r == 0:\n",
    "        #dilate\n",
    "        kernel = np.ones((randint(1, 3), randint(1, 3)), np.uint8) #kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.dilate(img, kernel, iterations=1)\n",
    "    elif r == 1:\n",
    "        #erode\n",
    "        kernel = np.ones((randint(1, 3), randint(1, 3)), np.uint8) #kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.erode(img, kernel, iterations=1)\n",
    "\n",
    "    #edges    \n",
    "    img = cv.Canny(img, 100, 200)\n",
    "\n",
    "    #blur\n",
    "    img = cv.blur(img, (3,3))\n",
    "\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    # #get max brightness\n",
    "    # max_brightness = np.max(img)\n",
    "    # ratio = 255.0/max_brightness\n",
    "    # #normalize\n",
    "    # img = (img*ratio).astype(np.uint8)\n",
    "\n",
    "    # #add random tilt\n",
    "    # max_offset = 1\n",
    "    # offset = randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset, axis=0)\n",
    "    # if offset > 0:\n",
    "    #     img[:offset, :] = 0 #randint(0,255)\n",
    "    # elif offset < 0:\n",
    "    #     img[offset:, :] = 0 # randint(0,255)\n",
    "    \n",
    "    #add noise \n",
    "    std = 60\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(100)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "#TODO add negative examples: inside intersection, in normal road with high curvature\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        junction_images_indexes = []\n",
    "        tot_lines = 0\n",
    "        with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            tot_lines = len(lines)\n",
    "            for i,line in enumerate(lines):\n",
    "                if line == '1,0,0,0,0,0,0,1,0,0,0,0,0,0,0':\n",
    "                    junction_images_indexes.append(i)\n",
    "        junction_imgs_mask = np.zeros(tot_lines, dtype=bool)\n",
    "        junction_imgs_mask[junction_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(200, max_load)): #start from 200 since first imgs have wrong labels\n",
    "            # for i in range(max_load):\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                line = line.split(',')\n",
    "                #x stopline, y stopline, yaw stopline\n",
    "                label = np.array([float(line[4]), float(line[5]), float(line[6])], dtype=np.float32)\n",
    "                dist_label = float(line[3]) #dist is used only to filter images\n",
    "                \n",
    "                MAX_DIST = 0.85\n",
    "                MIN_DIST = 0.25\n",
    "                #keep only small distanaces, avoid junctions\n",
    "                if dist_label < MAX_DIST and not junction_imgs_mask[i]:  #if  dist_label < 0.6 and not junction_imgs_mask[i]: \n",
    "                    # print(f'Sample {i},  idx = {all_img_idx},  dist = {dist_label}')\n",
    "                    if dist_label < MIN_DIST:\n",
    "                        dist_label = MAX_DIST+MIN_DIST  - dist_label\n",
    "                    #img \n",
    "                    img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "                    img = load_and_augment_img(img)\n",
    "                    if all_img_idx < 1000:\n",
    "                        # cv.putText(img, f'{dist_label[0]:.2f}', (5,10), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                        # cv.putText(img, f'{np.rad2deg(angle_label[0]):.0f}', (5,25), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                        cv.imshow('img', img)\n",
    "                        # print(label)\n",
    "                        cv.waitKey(1)\n",
    "                        if all_img_idx == 999:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(label)\n",
    "                    \n",
    "                    all_img_idx += 1\n",
    "\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "            self.data = np.array(self.data)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195568/195568 [02:10<00:00, 1495.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all imgs: torch.Size([60641, 32, 32, 1])\n",
      "data: (60641, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 1, 32, 32])\n",
      "torch.Size([8192, 3])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    pos_losses = []\n",
    "    angle_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        pos = output[:, 0:1]\n",
    "        angle = output[:, 2]\n",
    "\n",
    "        pos_label = regr_label[:, 0:1]\n",
    "        angle_label = regr_label[:, 2]\n",
    "\n",
    "        # Compute the losses\n",
    "        pos_loss = 1.0*regr_loss_fn(pos, pos_label)\n",
    "        angle_loss = 1.0*regr_loss_fn(angle, angle_label)\n",
    "    \n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = pos_loss + angle_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        pos_losses.append(pos_loss.detach().cpu().numpy())\n",
    "        angle_losses.append(angle_loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(pos_losses), np.mean(angle_losses)\n",
    "\n",
    "# VALIDATION FUNCTION\n",
    "def val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device=device):\n",
    "    stop_line_estimator.eval()\n",
    "    pos_losses = []\n",
    "    angle_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = stop_line_estimator(input)\n",
    "        regr_out = output\n",
    "        pos = regr_out[:, 0:1]\n",
    "        angle = regr_out[:, 2]\n",
    "        dist_label = regr_label[:, 0:1]\n",
    "        angle_label = regr_label[:, 2]\n",
    "        pos_loss = 1.0*regr_loss_fn(pos, dist_label)\n",
    "        angle_loss = 1.0*regr_loss_fn(angle, angle_label)\n",
    "        pos_losses.append(pos_loss.detach().cpu().numpy())\n",
    "        angle_losses.append(angle_loss.detach().cpu().numpy())\n",
    "    return np.mean(pos_losses), np.mean(angle_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  500/500 \n",
      "Pos loss: 0.1320 --- val pos loss: 0.1729\n",
      "angle loss: 0.6531 --- val angle loss: 0.6921\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.003 #0.005\n",
    "epochs = 500\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = .1*9e-4 #0.001 \n",
    "L2_lambda = .1*1e-2 #2e-2\n",
    "optimizer = torch.optim.Adam(stop_line_estimator.parameters(), lr=lr, weight_decay=9e-5) #wd = 2e-3# 3e-5\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "best_val_loss = 100.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        pos_loss, angle_loss = train_epoch(stop_line_estimator, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_pos_loss, val_angle_loss = val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Epoch  {epoch+1}/{epochs} \\nPos loss: {pos_loss:.4f} --- val pos loss: {val_pos_loss:.4f}\")\n",
    "        print(f\"angle loss: {np.rad2deg(angle_loss):.4f} --- val angle loss: {np.rad2deg(val_angle_loss):.4f}\")\n",
    "        if val_pos_loss < best_val_loss:\n",
    "            best_val_loss = val_pos_loss\n",
    "            torch.save(stop_line_estimator.state_dict(), model_name)\n",
    "            print(f'Saved model ')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [00:00<00:00, 272.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val pos_loss: (0.151962, 0.012528005)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "stop_line_estimator.load_state_dict(torch.load(model_name))\n",
    "print(f\"Val pos_loss: {val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 5, 5)\n",
      "(8, 8, 5, 5)\n",
      "(128, 8, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAJ5CAYAAACqmupFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdhklEQVR4nO3df6zfBX3v8denPaelh54WSlva0kLBexGGIs4F7gKKcoPROaKbzhu8m16dZgthW25cZAtMprmTu+h0Lsxkud5L9G4gd3dsgovOrJvxx3Bgl5BpYJWJtYRfpaUtLW05bT/3j3ITcnaaeHJ5H+b7Ph4JCXz59PX9nO/5nO958jknYRjHMQAAnS16sU8AAKCa4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3iAF90wDG8ahuHrwzDsGYbhsWEYPj0Mw/SLfV5AH4IH+NdgZZL/kmRDkvOTnJHkoy/qGQGtCB5gTsMwbBqG4Y5hGHYOw7BrGIabh2FYNAzDDcMwbB+G4YlhGD47DMPK547fPAzDOAzDu4Zh+MEwDE8Ow3D9c/9uwzAMB4dhWPW8/Vc+d8zkOI63juP4pXEcnxnH8akk/y3JpS/ORw50JHiAf2EYhsVJvpBke5LNOX7H5XNJ/tNzf70uyTlJlie5edYfvyzJS5P8+yQfHIbh/HEcH0lyd5K3Pu+4dyT53+M4zsxxCq9J8p0X5qMBSAb/Ly1gtmEYfjLJnUnWj+N45HmPb0nyZ+M4fuq5f35pkm8nWZZkY5KHkmwax/Hh5/79PUk+Po7j54ZheG+Sd4zjeMUwDEOSHyT5j+M4fnXWc1+Z5H8luWQcx23VHyvw/wd3eIC5bEqy/fmx85wNOX7X5//anmQiyenPe+yx5/39Mzl+FyhJ/izJTw7DsD7H7+AcS/K1548Pw/Dvktya5G1iB3ghTbzYJwD8q7QjyZnDMEzMip5Hkpz1vH8+M8mRJI/n+B2eExrH8alhGL6c5D/k+C8mf2583i3mYRhemeN3ld4zjuOWF+bDADjOHR5gLvckeTTJfx2G4eRhGE4ahuHSJLcl+c/DMJw9DMPyJB9Jcvscd4JO5NYk70zytuf+PkkyDMPLknwpya+M43jXC/mBACSCB5jDOI5Hk1yV5N/k+O/aPJzjd2b+R5L/meSrOf77OoeS/Mo8pu9M8m+TPDaO433Pe/z9SdYk+e/DMOx/7i+/tAy8YPzSMgDQnjs8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtDcxn4NPOeWUcd26dVXnkomJeZ3OvO3fv79se9myZWXbSfLAAw+U7o/jOJQ+wXMmJibGycnJsv3Vq1eXbSfJ4cOHy7Z3795dtp0kR48eLd1fqGsoOf5etH79+rL9mZmZsu0kmZqaKtvet29f2XaSbN++vXR/oa6j6enpsfL94siRI2XbSe33s8r36CQ588wzS/e3bt365DiOa2Y/Pq/CWLduXT796U+/cGc1y5o1/+L8XlBf//rXy7Zf/vKXl20nySWXXFK6v1AmJyezefPmsv33ve99ZdtJ8t3vfrds+7bbbivbTpK9e/eW7i+k9evX55Zbbinbf/zxx8u2k+RVr3pV2faXvvSlsu2k/mtsoaxevTo33nhj2f6uXbvKtpPk7rvvLtuuvLGRJDfffHPp/jAMc1a5H2kBAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0N7EfA4+dOhQtm3bVnUu5ZYvX162PTU1VbadJL/5m79Ztn3LLbeUbc82OTmZTZs2le3fdtttZdtJ8upXv7ps+7WvfW3ZdpK8/e1vL9u+4YYbyrbnsmTJkmzevLls/5lnninbTpItW7aUba9cubJsO0l+//d/v2z7937v98q257JoUd1/8//zP/9z2XaSnHPOOWXbH/3oR8u2k+Tyyy8v3T8Rd3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoL2J+Rw8jmMOHz5cdS4Zx7FsO0l+9md/tmx7cnKybDtJ3vGOd5Rtf/7zny/bnm3ZsmX5sR/7sbL9yuszSU4//fSy7de97nVl20ny0pe+tGx7amqqbHsuk5OTWbduXdn+Rz7ykbLtJLnsssvKts8777yy7ST58z//87LtAwcOlG3Ptnjx4qxYsaJs/+677y7bTpILLrigbHvTpk1l20nyxS9+sXT/RNzhAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtDcxn4OXLVuWCy64oOpcMj09XbadJDt27CjbfuCBB8q2k+TCCy8s2168eHHZ9mwnnXRSzj333LL9P/qjPyrbTpKDBw+WbU9NTZVtJ8mKFSvKtmdmZsq257Jv37781V/9Vdn+I488UradJA899FDZ9urVq8u2k+S1r31t2fY3vvGNsu3Znnnmmdx3331l+1deeWXZdpLcf//9Zduvec1ryraT5B//8R9L90/EHR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaG9iPgc/++yzeeSRR6rOJWvWrCnbTpJDhw6VbS9aVNuOGzZsKNtesmRJ2fZsjz/+eD75yU+W7V977bVl20nyqU99qmz7qquuKttOkssvv7xse3p6umz7RBYvXly2/VM/9VNl20ly1113lW2fccYZZdtJ8pa3vKVsu/Lra7ZhGDIMQ9n+S17ykrLtJNmxY0fZ9oEDB8q2k+TSSy8t3f/Wt7415+Pu8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe8M4jj/8wcOwM8n2utPhRXLWOI5rFuKJXENtLdg1lLiOGvNexAthzutoXsEDAPCjyI+0AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7U3M5+Dp6enxtNNOqzqXLFmypGw7SY4cOVK2vXv37rLtJNm7d2/p/jiOQ+kTPGf16tXj5s2by/afffbZsu0kOXDgQOl+pcnJybLtxx57LHv27FmQayhJJicnx6VLl1bul20nSeX76J49e8q2k2TXrl2l+wv1XrRs2bJxenq6bH/Tpk1l20nyT//0T2Xb1ed+8sknl+5v3br1yXEc18x+fF7Bc9ppp+WDH/zgC3dWs2zcuLFsO0mefPLJsu3bb7+9bDtJ7rzzztL9hbJ58+bce++9Zfs7duwo206Se+65p2z72LFjZdtJ7dfXe97znrLtuSxdujQXXXRR2f7atWvLtpPkF37hF8q277rrrrLtJLnllltK9xfK9PR03v72t5ftf+ITnyjbTpJXv/rVZdt/8Ad/ULadJBdffHHp/jAM2+d63I+0AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhvYj4H79mzJ5///OerziUbNmwo206SAwcOlG3ffPPNZdtJ8qu/+qtl29dcc03Z9mwHDx7Md77znbL9l73sZWXbSbJ///6y7b/7u78r206Sn/iJnyjbPvnkk8u25zI9PZ3LLrusbP+3f/u3y7aT5Nvf/nbZ9hVXXFG2nSRvfOMby7Z/4zd+o2x7ttNPPz2/9mu/Vrb/ta99rWw7Sc4///yy7RtuuKFsO0luuumm0v0TcYcHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANqbmM/Bq1atytVXX111Lvmd3/mdsu0k+bmf+7my7QcffLBsO0k2btxYtj05OVm2PduePXvyF3/xF2X7W7ZsKdtOkve9731l2/fcc0/ZdpLS133Pnj1l23M5dOhQtm3bVrb/la98pWw7Sc4555yy7enp6bLtJHnzm99ctv27v/u7ZduzHTp0KA888EDpfqXzzz+/bPuP//iPy7aTZMeOHaX7J+IODwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoL2J+Rx89OjR7Nu3r+pcsmHDhrLtJDn77LPLtleuXFm2nSRPP/102faxY8fKtmdbunRpNm/eXLa/atWqsu0k+d73vle2ffHFF5dtJ8kFF1xQur+QVq1alauvvrps//HHHy/bTpLt27eXbV944YVl20nygQ98oGz74YcfLtuebWJiIqtXry7b/8pXvlK2nSTLly8v277hhhvKtpPkH/7hH0r3T8QdHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBob2I+By9ZsiTr16+vOpdcc801ZdtJsmvXrrLtJ554omw7Sd7whjeUbU9NTZVtzzY5OZmNGzeW7V9wwQVl20ly9913l20fPXq0bDtJ3vrWt5Ztb9mypWx7LkuXLs1LXvKSsv1HHnmkbDtJzj///LLtz372s2XbSfLoo4+Wbc/MzJRtz3bkyJE89dRTZfu33npr2XaSXHnllWXbn/jEJ8q2k+T6668v3T8Rd3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoL1hHMcf/uBh2Jlke93p8CI5axzHNQvxRK6hthbsGkpcR415L+KFMOd1NK/gAQD4UeRHWgBAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0NzGfg6empsaVK1dWnUuOHj1atp0ky5cvL9tetKi2HY8cOVK2vWvXrjz99NND2RM8z/Lly8dVq1aV7a9du7ZsO0m2bt1atl15fSbJhg0byrYfe+yx7N27d0GuoSRZuXLlePrpp5ftT05Olm0nye7du8u216xZU7adJDMzM2Xbjz76aPbs2bMg19GSJUvGqampsv1NmzaVbSfJgw8+WLa9YsWKsu2k/rXZunXrk+M4/osvhHkFz8qVK/OLv/iLL9xZzbJr166y7SS59NJLy7arL5DK1+ZDH/pQ2fZsq1atygc+8IGy/WuvvbZsO0mGoe69+KKLLirbTpIPf/jDZdu//Mu/XLY9l9NPPz1/+Id/WLZfHc6333572fYv/dIvlW0nx6Okyrvf/e6y7dmmpqZy+eWXl+1//OMfL9tOkp/+6Z8u237jG99Ytp3UvzbDMGyf63E/0gIA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgvYn5HHz06NHs3r276lzKvfzlLy/b3rRpU9l2kmzZsqVse/HixWXbs83MzOSxxx4r27/mmmvKtpPki1/8Ytn2+vXry7aT5O///u/Ltg8dOlS2PZe9e/eWfi4q3yuS5Kabbirbfu9731u2nSSnnXZa2fbExLy+Jf0/OXz4cLZt21a2v2rVqrLtpPb9ovrz8LGPfax0/0Tc4QEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9ibmc/DU1FRe+cpXVp1L7rvvvrLtJHnooYfKtl/xileUbSfJihUryrYXL15ctj3bkSNHsnPnzrL9s846q2w7Sf7kT/6kbLv63K+44oqy7ZNOOqlsey5Lly7NOeecU7Z/9tlnl20nyd/+7d+WbVe+LknyN3/zN2Xbzz77bNn2bKecckp+5md+pmx//fr1ZdtJ8vrXv75s+6Mf/WjZdpLce++9pfsn4g4PANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgvYn5HLxkyZJs2rSp6lxy7733lm0nyQ9+8IOy7XvuuadsO0kWL15cur9QTjnllFx11VVl+0eOHCnbTpLdu3eXbe/bt69sO0l27txZtl39us92yimn5M1vfnPZfuVrlSSrVq0q2/7mN79Ztp0ku3btKtteyOto3bp1ue6668r2K1+npPZ7wtq1a8u2k2Tv3r2l+yfiDg8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtDcxn4MXLVqUk08+uepcsmhRbX+tWrWqbHtiYl4v5bxdeeWVpfsL5dChQ/nud79btn/s2LGy7SRZsWJF2fYwDGXbSbJz586y7ZmZmbLtuRw9ejRPPfVU2f5FF11Utp0kn/nMZ8q23/a2t5VtJ8mePXvKthcvXly2Pdv999+fiy++uGx/amqqbDtJLrzwwrLtt7zlLWXbSfKNb3yjdP9E3OEBANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPaGcRx/+IOHYWeS7XWnw4vkrHEc1yzEE7mG2lqwayhxHTXmvYgXwpzX0byCBwDgR5EfaQEA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANqbmM/BwzCMwzBUnUvGcSzbTpIzzzyzbHvp0qVl20myd+/esu19+/bl4MGDdZ/Y55mamhpXrlxZtr9///6y7aT281z5OU6S9evXl23v3r07+/fvX5BrKElOPfXU8Ywzzijbn5ycLNtOksOHD5dtT01NlW0nydatW0v3x3FckOtoenp6XL169UI8VYnK75fV1/+hQ4dK9x9++OEnx3FcM/vx+QZPJibm9UfmZWZmpmw7Sa677rqy7XPPPbdsO0nuvPPOsu3bb7+9bHu2lStX5l3velfZ/je/+c2y7SQ555xzyrb/8i//smw7SX7913+9bPtjH/tY2fZczjjjjNxxxx1l+2vXri3bTpLvfe97Zds//uM/XradHP8+0MHq1atz4403lu0vWlT7A5TKaN64cWPZdpLcf//9pfvvf//7t8/1uB9pAQDtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANDexHwO3rRpU66//vqqc8mpp55atp0k5557btn2kSNHyraT5J3vfGfZ9l//9V+XbS+0p59+unR/9+7dZdu/9Vu/VbadJJ/5zGfKtnft2lW2PZdhGLJ48eKy/XvvvbdsO0l27NhRtj0zM1O2nSR/+qd/WrZ93XXXlW3P9v3vfz/vfve7y/ZvvfXWsu0k+eQnP1m2/fM///Nl28nxr98Xgzs8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANDexHwOXrp0aTZv3lx0KsnBgwfLtpNk165dZdu7d+8u206S0047rWz76NGjZduzDcOQJUuWlO1/6EMfKttOks997nNl23fccUfZdpJ861vfKt1fSDMzM3n00UfL9h944IGy7SQ59dRTy7YvvPDCsu0kedWrXlW2fdNNN5Vtz7Z58+bceOONZfvPPvts2XaSvOlNbyrbvuSSS8q2k2Tv3r2l+yfiDg8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKC9ifkcvGLFirz+9a+vOpd8//vfL9tOknEcy7ZPOumksu0k2b9/f9l25esy24YNG/LhD3+4bP/qq68u206SM844o2z7vPPOK9tOkp07d5ZtP/jgg2Xbczl27FgOHjxYtr9x48ay7SRZtmxZ2fYXvvCFsu0kecUrXlG2ffjw4bLt2Q4dOpRt27aV7Ve/r15++eVl21/96lfLtpPkwIEDpfsn4g4PANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQ3MZ+D9+3bly9/+ctV55KJiXmdzrydd955ZdvPPPNM2XaSHDx4sGz72LFjZduz3XfffVm3bl3Z/rXXXlu2ndReo9u3by/bTpIrr7yybPuJJ54o257LsWPHSr8mtm3bVradJJs3by7bfvzxx8u2k2Tt2rVl2zMzM2Xbsw3DkKVLl5bt79q1q2w7Sc4888yy7Te84Q1l20lyzTXXlO6fiDs8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANDeMI7jD3/wMOxMsr3udHiRnDWO45qFeCLXUFsLdg0lrqPGvBfxQpjzOppX8AAA/CjyIy0AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKC9/wM8B8cWi6teoAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(stop_line_estimator.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 2, 4, 'conv0', size=(10,5))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopLineEstimator(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout(p=0.2, inplace=False)\n",
       "    (11): Conv2d(8, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=16, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "stop_line_estimator.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "stop_line_estimator.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "stop_line_estimator.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(stop_line_estimator, dummy_input, onnx_model_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "stop_line_estimator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 6575.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[ 6.0918427e-01 -1.1403767e-41  1.5882933e-01]]\n",
      "Predictions shape: (1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_model_path)\n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
