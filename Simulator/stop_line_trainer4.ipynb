{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/stop_line_estimator_advanced.pt'\n",
    "onnx_model_path = \"models/stop_line_estimator_advanced.onnx\"\n",
    "max_load = 250_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE\n",
    "\n",
    "# class StopLineEstimator(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             nn.Linear(in_features=4*4*4, out_features=32),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(in_features=32, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# stop_line_estimator = StopLineEstimator(out_dim=3,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class StopLineEstimator(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        prob = 0.2\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 4, kernel_size=5, stride=1), #out = 28\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=14\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(4, 8, kernel_size=5, stride=1), #out = 10\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=5\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(8, 64, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            #normalize\n",
    "            # nn.BatchNorm1d(3*3*4),\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=1*1*64, out_features=32),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Linear(in_features=32, out_features=16),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(in_features=32, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "stop_line_estimator = StopLineEstimator(out_dim=3,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "stop_line_estimator.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = stop_line_estimator(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from time import time, sleep\n",
    "\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "    #convert to gray\n",
    "    img = cv.resize(img, (4*SIZE[1], 4*SIZE[0]))\n",
    "\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "        axes_length = (randint(10//4, 50//4), randint(50//4, 300//4))\n",
    "        angle = randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (50,50))\n",
    "    noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = np.clip(light, 0, 51)\n",
    "    light *= 5\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 5), randint(1, 5)))\n",
    "\n",
    "    # cut the top third of the image, \n",
    "    img = img[int(img.shape[0]*(2/5)):,:] ################################# 2/5 frame[int(frame.shape[0]*(2/5)):,:]\n",
    "\n",
    "    #edges\n",
    "    img = cv.resize(img, (2*SIZE[1], 2*SIZE[0]))\n",
    "\n",
    "    # erosion and dilation\n",
    "    r = randint(0, 5)\n",
    "    if r == 0:\n",
    "        #dilate\n",
    "        kernel = np.ones((randint(1, 3), randint(1, 3)), np.uint8) #kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.dilate(img, kernel, iterations=1)\n",
    "    elif r == 1:\n",
    "        #erode\n",
    "        kernel = np.ones((randint(1, 3), randint(1, 3)), np.uint8) #kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.erode(img, kernel, iterations=1)\n",
    "\n",
    "    #edges    \n",
    "    img = cv.Canny(img, 100, 200)\n",
    "\n",
    "    #blur\n",
    "    img = cv.blur(img, (3,3))\n",
    "\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    # #get max brightness\n",
    "    # max_brightness = np.max(img)\n",
    "    # ratio = 255.0/max_brightness\n",
    "    # #normalize\n",
    "    # img = (img*ratio).astype(np.uint8)\n",
    "\n",
    "    # #add random tilt\n",
    "    # max_offset = 1\n",
    "    # offset = randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset, axis=0)\n",
    "    # if offset > 0:\n",
    "    #     img[:offset, :] = 0 #randint(0,255)\n",
    "    # elif offset < 0:\n",
    "    #     img[offset:, :] = 0 # randint(0,255)\n",
    "    \n",
    "    #add noise \n",
    "    std = 60\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(100)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "#TODO add negative examples: inside intersection, in normal road with high curvature\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(200, max_load)): #start from 200 since first imgs have wrong labels\n",
    "            # for i in range(max_load):\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                line = line.split(',')\n",
    "                #x stopline, y stopline, yaw stopline\n",
    "                label = np.array([float(line[4]), float(line[5]), float(line[6])], dtype=np.float32)\n",
    "                dist_label = float(line[4]) #dist is used only to filter images\n",
    "                \n",
    "                MAX_DIST = 9.0\n",
    "                #keep only small distanaces, avoid junctions\n",
    "                if dist_label < MAX_DIST: #and not junction_imgs_mask[i]:  #if  dist_label < 0.6 and not junction_imgs_mask[i]: \n",
    "                    # print(f'Sample {i},  idx = {all_img_idx},  dist = {dist_label}')\n",
    "                    # if dist_label < MIN_DIST:\n",
    "                    #     dist_label = MAX_DIST+MIN_DIST  - dist_label\n",
    "                    #img \n",
    "                    img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "                    img = load_and_augment_img(img)\n",
    "                    if all_img_idx < 1000:\n",
    "                        # cv.putText(img, f'{dist_label[0]:.2f}', (5,10), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                        # cv.putText(img, f'{np.rad2deg(angle_label[0]):.0f}', (5,25), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                        cv.imshow('img', img)\n",
    "                        # print(label)\n",
    "                        cv.waitKey(1)\n",
    "                        if all_img_idx == 999:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(label)\n",
    "                    \n",
    "                    all_img_idx += 1\n",
    "                else:\n",
    "                    MAX_VISIBLE_DIST = 0.65\n",
    "                    label[0] = MAX_VISIBLE_DIST\n",
    "                    img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "                    img = load_and_augment_img(img)\n",
    "                    if all_img_idx < 1000:\n",
    "                        # cv.putText(img, f'{dist_label[0]:.2f}', (5,10), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                        # cv.putText(img, f'{np.rad2deg(angle_label[0]):.0f}', (5,25), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                        cv.imshow('img', img)\n",
    "                        # print(label)\n",
    "                        cv.waitKey(1)\n",
    "                        if all_img_idx == 999:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(label)\n",
    "                    \n",
    "                    all_img_idx += 1\n",
    "\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "            self.data = np.array(self.data)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249800/249800 [09:25<00:00, 441.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all imgs: torch.Size([249800, 32, 32, 1])\n",
      "data: (249800, 3)\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 1, 32, 32])\n",
      "torch.Size([8192, 3])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    pos_losses = []\n",
    "    angle_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        pos = output[:, 0:1]\n",
    "        angle = output[:, 2]\n",
    "\n",
    "        pos_label = regr_label[:, 0:1]\n",
    "        angle_label = regr_label[:, 2]\n",
    "\n",
    "        # Compute the losses\n",
    "        pos_loss = 1.0*regr_loss_fn(pos, pos_label)\n",
    "        angle_loss = 0*1.0*regr_loss_fn(angle, angle_label)\n",
    "    \n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = pos_loss + angle_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        pos_losses.append(pos_loss.detach().cpu().numpy())\n",
    "        angle_losses.append(angle_loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(pos_losses), np.mean(angle_losses)\n",
    "\n",
    "# VALIDATION FUNCTION\n",
    "def val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device=device):\n",
    "    stop_line_estimator.eval()\n",
    "    pos_losses = []\n",
    "    angle_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = stop_line_estimator(input)\n",
    "        regr_out = output\n",
    "        pos = regr_out[:, 0:1]\n",
    "        angle = regr_out[:, 2]\n",
    "        dist_label = regr_label[:, 0:1]\n",
    "        angle_label = regr_label[:, 2]\n",
    "        pos_loss = 1.0*regr_loss_fn(pos, dist_label)\n",
    "        angle_loss = 1.0*regr_loss_fn(angle, angle_label)\n",
    "        pos_losses.append(pos_loss.detach().cpu().numpy())\n",
    "        angle_losses.append(angle_loss.detach().cpu().numpy())\n",
    "    return np.mean(pos_losses), np.mean(angle_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  51/500 \n",
      "Pos loss: 0.0189 --- val pos loss: 0.0225\n",
      "angle loss: 0.0000 --- val angle loss: 2.2626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 18/28 [00:09<00:05,  1.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2707/2508240346.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# if True:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mpos_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_line_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mval_pos_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_angle_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_line_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2707/1330815447.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m#batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mpos_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mangle_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mangle_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.003 #0.005\n",
    "epochs = 500\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 0*.1*9e-4 #0.001 \n",
    "L2_lambda = 0*.1*1e-2 #2e-2\n",
    "optimizer = torch.optim.Adam(stop_line_estimator.parameters(), lr=lr, weight_decay=9e-5) #wd = 2e-3# 3e-5\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "best_val_loss = 100.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        pos_loss, angle_loss = train_epoch(stop_line_estimator, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_pos_loss, val_angle_loss = val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Epoch  {epoch+1}/{epochs} \\nPos loss: {pos_loss:.4f} --- val pos loss: {val_pos_loss:.4f}\")\n",
    "        print(f\"angle loss: {np.rad2deg(angle_loss):.4f} --- val angle loss: {np.rad2deg(val_angle_loss):.4f}\")\n",
    "        if val_pos_loss < best_val_loss:\n",
    "            best_val_loss = val_pos_loss\n",
    "            torch.save(stop_line_estimator.state_dict(), model_name)\n",
    "            print(f'Saved model ')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 271.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val pos_loss: (0.022217076, 0.03951788)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "stop_line_estimator.load_state_dict(torch.load(model_name))\n",
    "print(f\"Val pos_loss: {val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 5, 5)\n",
      "(8, 4, 5, 5)\n",
      "(64, 8, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(stop_line_estimator.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 2, 4, 'conv0', size=(10,5))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopLineEstimator(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout(p=0.2, inplace=False)\n",
       "    (11): Conv2d(8, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "stop_line_estimator.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "stop_line_estimator.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "stop_line_estimator.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(stop_line_estimator, dummy_input, onnx_model_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "stop_line_estimator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2486.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[ 6.1105800e-01 -5.9140326e-33 -5.3395472e-33]]\n",
      "Predictions shape: (1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_model_path)\n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
