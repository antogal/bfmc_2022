{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/stop_line_estimator_advanced.pt'\n",
    "onnx_model_path = \"models/stop_line_estimator_advanced.onnx\"\n",
    "max_load = 250_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE\n",
    "\n",
    "# class StopLineEstimator(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             nn.Linear(in_features=4*4*4, out_features=32),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(in_features=32, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# stop_line_estimator = StopLineEstimator(out_dim=3,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class StopLineEstimator(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        prob = 0.2\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 4, kernel_size=5, stride=1), #out = 28\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=14\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(4, 8, kernel_size=5, stride=1), #out = 10\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=5\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(8, 64, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            #normalize\n",
    "            # nn.BatchNorm1d(3*3*4),\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=1*1*64, out_features=32),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Linear(in_features=32, out_features=16),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(in_features=32, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "stop_line_estimator = StopLineEstimator(out_dim=3,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "stop_line_estimator.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = stop_line_estimator(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from time import time, sleep\n",
    "\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "    #convert to gray\n",
    "    img = cv.resize(img, (4*SIZE[1], 4*SIZE[0]))\n",
    "\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "        axes_length = (randint(10//4, 50//4), randint(50//4, 300//4))\n",
    "        angle = randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (50,50))\n",
    "    noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = np.clip(light, 0, 51)\n",
    "    light *= 5\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 5), randint(1, 5)))\n",
    "\n",
    "    # cut the top third of the image, \n",
    "    img = img[int(img.shape[0]*(2/5)):,:] ################################# 2/5 frame[int(frame.shape[0]*(2/5)):,:]\n",
    "\n",
    "    #edges\n",
    "    img = cv.resize(img, (2*SIZE[1], 2*SIZE[0]))\n",
    "\n",
    "    # erosion and dilation\n",
    "    r = randint(0, 5)\n",
    "    if r == 0:\n",
    "        #dilate\n",
    "        kernel = np.ones((randint(1, 3), randint(1, 3)), np.uint8) #kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.dilate(img, kernel, iterations=1)\n",
    "    elif r == 1:\n",
    "        #erode\n",
    "        kernel = np.ones((randint(1, 3), randint(1, 3)), np.uint8) #kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.erode(img, kernel, iterations=1)\n",
    "\n",
    "    #edges    \n",
    "    img = cv.Canny(img, 100, 200)\n",
    "\n",
    "    #blur\n",
    "    img = cv.blur(img, (3,3))\n",
    "\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    # #get max brightness\n",
    "    # max_brightness = np.max(img)\n",
    "    # ratio = 255.0/max_brightness\n",
    "    # #normalize\n",
    "    # img = (img*ratio).astype(np.uint8)\n",
    "\n",
    "    # #add random tilt\n",
    "    # max_offset = 1\n",
    "    # offset = randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset, axis=0)\n",
    "    # if offset > 0:\n",
    "    #     img[:offset, :] = 0 #randint(0,255)\n",
    "    # elif offset < 0:\n",
    "    #     img[offset:, :] = 0 # randint(0,255)\n",
    "    \n",
    "    #add noise \n",
    "    std = 60\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(100)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "#TODO add negative examples: inside intersection, in normal road with high curvature\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        # junction_images_indexes = []\n",
    "        # tot_lines = 0\n",
    "        # with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "        #     lines = f.read().split('\\n')\n",
    "        #     lines = lines[0:-1] #remove footer\n",
    "        #     tot_lines = len(lines)\n",
    "        #     for i,line in enumerate(lines):\n",
    "        #         if line == '1,0,0,0,0,0,0,1,0,0,0,0,0,0,0':\n",
    "        #             junction_images_indexes.append(i)\n",
    "        # junction_imgs_mask = np.zeros(tot_lines, dtype=bool)\n",
    "        # junction_imgs_mask[junction_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(200, max_load)): #start from 200 since first imgs have wrong labels\n",
    "            # for i in range(max_load):\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                line = line.split(',')\n",
    "                #x stopline, y stopline, yaw stopline\n",
    "                label = np.array([float(line[4]), float(line[5]), float(line[6])], dtype=np.float32)\n",
    "                dist_label = float(line[4]) #dist is used only to filter images\n",
    "                \n",
    "                MAX_DIST = 9.0\n",
    "                #keep only small distanaces, avoid junctions\n",
    "                if dist_label < MAX_DIST: #and not junction_imgs_mask[i]:  #if  dist_label < 0.6 and not junction_imgs_mask[i]: \n",
    "                    # print(f'Sample {i},  idx = {all_img_idx},  dist = {dist_label}')\n",
    "                    # if dist_label < MIN_DIST:\n",
    "                    #     dist_label = MAX_DIST+MIN_DIST  - dist_label\n",
    "                    #img \n",
    "                    img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "                    img = load_and_augment_img(img)\n",
    "                    if all_img_idx < 1000:\n",
    "                        # cv.putText(img, f'{dist_label[0]:.2f}', (5,10), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                        # cv.putText(img, f'{np.rad2deg(angle_label[0]):.0f}', (5,25), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                        cv.imshow('img', img)\n",
    "                        # print(label)\n",
    "                        cv.waitKey(1)\n",
    "                        if all_img_idx == 999:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(label)\n",
    "                    \n",
    "                    all_img_idx += 1\n",
    "\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "            self.data = np.array(self.data)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249800/249800 [02:20<00:00, 1781.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all imgs: torch.Size([55822, 32, 32, 1])\n",
      "data: (55822, 3)\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 1, 32, 32])\n",
      "torch.Size([8192, 3])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    pos_losses = []\n",
    "    angle_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        pos = output[:, 0:1]\n",
    "        angle = output[:, 2]\n",
    "\n",
    "        pos_label = regr_label[:, 0:1]\n",
    "        angle_label = regr_label[:, 2]\n",
    "\n",
    "        # Compute the losses\n",
    "        pos_loss = 1.0*regr_loss_fn(pos, pos_label)\n",
    "        angle_loss = 0*1.0*regr_loss_fn(angle, angle_label)\n",
    "    \n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = pos_loss + angle_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        pos_losses.append(pos_loss.detach().cpu().numpy())\n",
    "        angle_losses.append(angle_loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(pos_losses), np.mean(angle_losses)\n",
    "\n",
    "# VALIDATION FUNCTION\n",
    "def val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device=device):\n",
    "    stop_line_estimator.eval()\n",
    "    pos_losses = []\n",
    "    angle_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = stop_line_estimator(input)\n",
    "        regr_out = output\n",
    "        pos = regr_out[:, 0:1]\n",
    "        angle = regr_out[:, 2]\n",
    "        dist_label = regr_label[:, 0:1]\n",
    "        angle_label = regr_label[:, 2]\n",
    "        pos_loss = 1.0*regr_loss_fn(pos, dist_label)\n",
    "        angle_loss = 1.0*regr_loss_fn(angle, angle_label)\n",
    "        pos_losses.append(pos_loss.detach().cpu().numpy())\n",
    "        angle_losses.append(angle_loss.detach().cpu().numpy())\n",
    "    return np.mean(pos_losses), np.mean(angle_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  500/500 \n",
      "Pos loss: 0.0510 --- val pos loss: 0.0540\n",
      "angle loss: 0.0000 --- val angle loss: 5.0718\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.003 #0.005\n",
    "epochs = 500\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 0*.1*9e-4 #0.001 \n",
    "L2_lambda = 0*.1*1e-2 #2e-2\n",
    "optimizer = torch.optim.Adam(stop_line_estimator.parameters(), lr=lr, weight_decay=9e-5) #wd = 2e-3# 3e-5\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "best_val_loss = 100.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        pos_loss, angle_loss = train_epoch(stop_line_estimator, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_pos_loss, val_angle_loss = val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Epoch  {epoch+1}/{epochs} \\nPos loss: {pos_loss:.4f} --- val pos loss: {val_pos_loss:.4f}\")\n",
    "        print(f\"angle loss: {np.rad2deg(angle_loss):.4f} --- val angle loss: {np.rad2deg(val_angle_loss):.4f}\")\n",
    "        if val_pos_loss < best_val_loss:\n",
    "            best_val_loss = val_pos_loss\n",
    "            torch.save(stop_line_estimator.state_dict(), model_name)\n",
    "            print(f'Saved model ')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:00<00:00, 207.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val pos_loss: (0.048397195, 0.08850319)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "stop_line_estimator.load_state_dict(torch.load(model_name))\n",
    "print(f\"Val pos_loss: {val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 5, 5)\n",
      "(8, 4, 5, 5)\n",
      "(64, 8, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(stop_line_estimator.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 2, 4, 'conv0', size=(10,5))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopLineEstimator(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout(p=0.2, inplace=False)\n",
       "    (11): Conv2d(8, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "stop_line_estimator.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "stop_line_estimator.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "stop_line_estimator.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(stop_line_estimator, dummy_input, onnx_model_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "stop_line_estimator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 3091.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[ 4.030312e-01 -5.232659e-40 -5.197416e-42]]\n",
      "Predictions shape: (1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_model_path)\n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
