{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/stop_line_estimator.pt'\n",
    "onnx_model_path = \"models/stop_line_estimator.onnx\"\n",
    "max_load = 250_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE\n",
    "\n",
    "# class StopLineEstimator(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             nn.Linear(in_features=4*4*4, out_features=32),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(in_features=32, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# stop_line_estimator = StopLineEstimator(out_dim=2,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class StopLineEstimator(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        prob = 0.15\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 28\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=14\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(8, 8, kernel_size=5, stride=1), #out = 10\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=5\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(8, 128, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            #normalize\n",
    "            # nn.BatchNorm1d(3*3*4),\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=1*1*128, out_features=64),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "stop_line_estimator = StopLineEstimator(out_dim=2,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "stop_line_estimator.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = stop_line_estimator(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from time import time, sleep\n",
    "\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "    #convert to gray\n",
    "    img = cv.resize(img, (4*SIZE[1], 4*SIZE[0]))\n",
    "\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "        axes_length = (randint(10//4, 50//4), randint(50//4, 300//4))\n",
    "        angle = randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (50,50))\n",
    "    noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = np.clip(light, 0, 51)\n",
    "    light *= 5\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 5), randint(1, 5)))\n",
    "\n",
    "    # cut the top third of the image, \n",
    "    img = img[int(img.shape[0]*(2/5)):,:] ################################# 2/5 frame[int(frame.shape[0]*(2/5)):,:]\n",
    "\n",
    "    #edges\n",
    "    img = cv.resize(img, (2*SIZE[1], 2*SIZE[0]))\n",
    "\n",
    "    # erosion and dilation\n",
    "    r = randint(0, 5)\n",
    "    if r == 0:\n",
    "        #dilate\n",
    "        kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.dilate(img, kernel, iterations=1)\n",
    "    elif r == 1:\n",
    "        #erode\n",
    "        kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.erode(img, kernel, iterations=1)\n",
    "\n",
    "    #edges    \n",
    "    img = cv.Canny(img, 100, 200)\n",
    "\n",
    "    #blur\n",
    "    img = cv.blur(img, (3,3))\n",
    "\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    # #get max brightness\n",
    "    # max_brightness = np.max(img)\n",
    "    # ratio = 255.0/max_brightness\n",
    "    # #normalize\n",
    "    # img = (img*ratio).astype(np.uint8)\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = 1\n",
    "    offset = randint(-max_offset, max_offset)\n",
    "    img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        img[:offset, :] = 0 #randint(0,255)\n",
    "    elif offset < 0:\n",
    "        img[offset:, :] = 0 # randint(0,255)\n",
    "    \n",
    "    #add noise \n",
    "    std = 60\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(100)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "#TODO add negative examples: inside intersection, in normal road with high curvature\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        junction_images_indexes = []\n",
    "        tot_lines = 0\n",
    "        with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            tot_lines = len(lines)\n",
    "            for i,line in enumerate(lines):\n",
    "                if line == '1,0,0,0,0,0,0,1,0,0,0,0,0,0,0':\n",
    "                    junction_images_indexes.append(i)\n",
    "        junction_imgs_mask = np.zeros(tot_lines, dtype=bool)\n",
    "        junction_imgs_mask[junction_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(200, max_load)): #start from 200 since first imgs have wrong labels\n",
    "            # for i in range(max_load):\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                dist = line.split(',')\n",
    "                #distance is the 4th element of the label data\n",
    "                dist_label = np.array([float(dist[3])], dtype=np.float32)\n",
    "                angle_label = np.array([float(dist[4])], dtype=np.float32)\n",
    "\n",
    "                #keep only small distanaces, avoid junctions\n",
    "                if -0.01 < dist_label < 0.6 and not junction_imgs_mask[i]:  #if  dist_label < 0.6 and not junction_imgs_mask[i]: \n",
    "                    # print(f'Sample {i},  idx = {all_img_idx},  dist = {dist_label}')\n",
    "                    if dist_label < -0.01:\n",
    "                        dist_label = 0.6 - dist_label\n",
    "                    #img \n",
    "                    img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "                    img = load_and_augment_img(img)\n",
    "                    if all_img_idx < 1000:\n",
    "                        # cv.putText(img, f'{dist_label[0]:.2f}', (5,10), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                        # cv.putText(img, f'{np.rad2deg(angle_label[0]):.0f}', (5,25), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                        cv.imshow('img', img)\n",
    "                        cv.waitKey(1)\n",
    "                        if all_img_idx == 999:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(np.array([dist_label, angle_label]))\n",
    "                    all_img_idx += 1\n",
    "\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "            self.data = np.array(self.data)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97596/97596 [00:31<00:00, 3104.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all imgs: torch.Size([10673, 32, 32, 1])\n",
      "data: (10673, 2, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 1, 32, 32])\n",
      "torch.Size([8192, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    dist_losses = []\n",
    "    angle_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        dist = output[:, 0]\n",
    "        angle = output[:, 1]\n",
    "\n",
    "        dist_label = regr_label[:, 0]\n",
    "        angle_label = regr_label[:, 1]\n",
    "\n",
    "        # Compute the losses\n",
    "        dist_loss = 0.0*1.0*regr_loss_fn(dist, dist_label)\n",
    "        angle_loss = 1.0*regr_loss_fn(angle, angle_label)\n",
    "    \n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = dist_loss + angle_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        dist_losses.append(dist_loss.detach().cpu().numpy())\n",
    "        angle_losses.append(angle_loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(dist_losses), np.mean(angle_losses)\n",
    "\n",
    "# VALIDATION FUNCTION\n",
    "def val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device=device):\n",
    "    stop_line_estimator.eval()\n",
    "    dist_losses = []\n",
    "    angle_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = stop_line_estimator(input)\n",
    "        regr_out = output\n",
    "        dist = regr_out[:, 0]\n",
    "        angle = regr_out[:, 1]\n",
    "        dist_label = regr_label[:, 0]\n",
    "        angle_label = regr_label[:, 1]\n",
    "        dist_loss = 1.0*regr_loss_fn(dist, dist_label)\n",
    "        angle_loss = 1.0*regr_loss_fn(angle, angle_label)\n",
    "        dist_losses.append(dist_loss.detach().cpu().numpy())\n",
    "        angle_losses.append(angle_loss.detach().cpu().numpy())\n",
    "    return np.mean(dist_losses), np.mean(angle_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50/50 \n",
      "dist loss: 0.0000 --- val dist loss: 0.1517\n",
      "angle loss: 2.1296 --- val angle loss: 1.9013\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.003 #0.005\n",
    "epochs = 50\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 0*9e-4 #0.001 \n",
    "L2_lambda = 0*1e-2 #2e-2\n",
    "optimizer = torch.optim.Adam(stop_line_estimator.parameters(), lr=lr, weight_decay=0*9e-5) #wd = 2e-3# 3e-5\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "best_val_loss = np.inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        dist_loss, angle_loss = train_epoch(stop_line_estimator, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_dist_loss, val_angle_loss = val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    if val_dist_loss < best_val_loss:\n",
    "        best_val_loss = val_dist_loss\n",
    "        torch.save(stop_line_estimator.state_dict(), model_name)\n",
    "        print(f'Saved model ')\n",
    "    \n",
    "\n",
    "    print(f\"Epoch  {epoch+1}/{epochs} \\ndist loss: {dist_loss:.4f} --- val dist loss: {val_dist_loss:.4f}\")\n",
    "    print(f\"angle loss: {np.rad2deg(angle_loss):.4f} --- val angle loss: {np.rad2deg(val_angle_loss):.4f}\")\n",
    "\n",
    "\n",
    "#Note: sweet spot for training is around 0.016 -> 0.020, also note that training can get stuck, and loss can start improve randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 164.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val dist_loss: (0.13619822, 0.0344802)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "stop_line_estimator.load_state_dict(torch.load(model_name))\n",
    "print(f\"Val dist_loss: {val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 5, 5)\n",
      "(8, 8, 5, 5)\n",
      "(128, 8, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAAFECAYAAADIoV+oAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQV0lEQVR4nO3abayeBX3H8d8Fpz3Y0q60Jc0ETlsto0RwEOLIGANWJQYfULeFMXBsL4iBJQsSnQFGl5QEnW9GYhbojEFlmUCy4WTzBfAOJlRMSLRBcQksPJSHPkjpKIU+cO1Fe0dCDptH+Yvy/3wSErjP1d913Tk39/n2us8wjmMAALo47K2+AACAXybxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AC/MoZhuGgYhseHYdg9DMO/DcOw9K2+JuDtR/wAvxKGYXhPkn9M8mdJViR5KcmNb+lFAW9L4gd4Q8MwHDcMwx3DMGwbhmHHMAz/MAzDYcMwXHvoDs3WYRhuGYbhNw4dv2oYhnEYhj8fhuGJYRi2D8PwN4e+9s5hGPa89m7OMAynHjpmXpKLk/z7OI73juP4YpL1Sf5wGIZFb8VzB96+xA8wq2EYDk/yH0keT7IqyTFJbkvyF4f++YMk70pyZJJ/eN0fPzPJCUnen+Rvh2E4cRzHp5M8kOSPXnPcRUn+ZRzHfUnek+T7ky+M4/hokr1JfuvNfWZAd+IHeCO/k+SdSf56HMfd4zi+PI7jf+bgHZq/H8fxsUN3aK5OcuEwDFOv+bMbxnHcM47j93MwaH770OPfSPKnSTIMw5DkwkOPJQcj6oXXXcMLSdz5Ad5U4gd4I8cleXwcx/2ve/ydOXg3aOLxJFM5+Hs6E8++5t9fysGwSZJ/TfK7wzD8ZpKzkrya5L5DX3sxyeLXnWtxkv/5eZ8AwGym/v9DgKaeTDIzDMPU6wLo6SQrX/PfM0n2J3kuybH/1+A4js8Pw3B3kj9JcmKS28ZxHA99+eH89A5RhmF4V5LpJP/1iz4RgNdy5wd4Iw8meSbJ3w3DsHAYhiOGYfi9JLcmuXIYhtXDMByZ5PNJbp/lDtEb+UaSS5L8cX76kVeS/HOSjw7D8PvDMCxMcl2SO8ZxdOcHeFOJH2BW4zgeSPLRJGuSPJHkqRy8Y3Nzkn9Kcm+S/07ycpK/msP0nUmOT/Lsod8Jmpzv4SSX5WAEbc3B3/X5y1/4iQC8zvDTO84AAG9/7vwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaEX8AACtiB8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtCJ+AIBWxA8A0Ir4AQBaET8AQCviBwBoRfwAAK2IHwCglam5HLxw4cJx6dKlVdeSI488smx7YseOHaX7ixcvLt1PknEcy7a3bduWXbt2DVX709PT44IFC6rmMzU1p5f0z6X6e7x3797S/STZsmVL6f44jtvHcTy6an/58uXjzMxM1Xx27dpVtj3x0ksvle5Xv9clyapVq8q2n3322bzwwgtl70VHHHHEWPkz58UXXyzbnli9enXp/rx580r3k+SHP/xh6f6BAwdmfS+a00+KpUuX5oorrnjzrup1zj777LLtia9//eul++vWrSvdT5IDBw6UbV911VVl20myYMGCnHPOOWX7y5YtK9ueOPfcc0v3n3nmmdL9JLn66qtL919++eXHK/dnZmbyne98p2z/7rvvLtueeOihh0r3b7nlltL9JLnpppvKti+//PKy7eTgX7bPP//8sv3777+/bHui+ufZihUrSveT5LTTTivd37Fjx6zvRT72AgBaET8AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaEX8AACtiB8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKCVqbkc/Morr+TRRx+tupacccYZZdsTl112Wen+Aw88ULqfJJ/61KfKz1Fl6dKlufjii8v2d+7cWbY98dxzz5Xu/zKewyOPPFK6v2rVqtL9PXv25Ac/+EHZ/ne/+92y7Ym1a9eW7i9fvrx0P0ne//73l5+jyp49e7J58+ay/Q9+8INl2xOV/w8kKf15P3HMMceU7u/YsWPWx935AQBaET8AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaEX8AACtiB8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtCJ+AIBWxA8A0MrUXA6enp7OmjVrqq4l99xzT9n2xPz580v3zzzzzNL9JHnyySfLtj/0oQ+VbSfJli1bcvXVV5ftV39/k+SEE04o3d+3b1/pfpLS78Evw+7du7Np06ay/fPOO69se2LZsmWl+8uXLy/dT5KjjjqqbHvXrl1l20kyb968rFixomz/5ptvLtue+OQnP1m6/8t4Pz366KPLzzEbd34AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtCJ+AIBWxA8A0Ir4AQBaET8AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaEX8AACtiB8AoJWpuRx81FFH5YILLqi6lhx//PFl2xO33XZb6f5PfvKT0v0kmZqa07dtTvbt21e2nSQLFizIKaecUra/evXqsu2Jd7zjHaX7GzZsKN1Pkm9+85vl56i0ffv23HzzzWX71113Xdn2xKWXXlq6v3jx4tL9JLn22mvLtm+44Yay7SR5+eWX8+Mf/7hs/8CBA2XbExs3bizdH8exdD9JHnzwwdL9008/fdbH3fkBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtCJ+AIBWxA8A0Ir4AQBaET8AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaEX8AACtiB8AoBXxAwC0In4AgFbEDwDQytRcDn744Ydz0kknVV1LVqxYUbY9cdppp5XuH3fccaX7SbJ06dKy7V27dpVtJ8n09HTe/e53l+1/8YtfLNue+PznP1+6/9nPfrZ0P0mWLFlSfo5Ke/fuzVNPPVW2/4EPfKBse6L6tXr88ceX7ifJVVddVbb9la98pWw7ScZxzL59+8r2169fX7Y9cc0115Tuz8zMlO4nyVe/+tXyc8zGnR8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtCJ+AIBWxA8A0Ir4AQBaET8AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaGUYx/FnP3gYtiV5vO5y+BWwchzHo6vGvYba8DriF+U1xJth1tfRnOIHAODXnY+9AIBWxA8A0Ir4AQBaET8AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaEX8AACtiB8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtCJ+AIBWpuZy8Lx588bp6emqa8natWvLtif27t1buv/II4+U7ifJypUry7a3bt2aF154YajaX7Ro0bhs2bKq+Wzbtq1se+LEE08s3R/HsXQ/SXbu3Fm6/9hjj20fx/Hoqv3q96KFCxeWbU8sX768dH///v2l+0ny6quvlm1XvxctXLhwXLJkSdV85s+fX7Y9sXv37tL9pUuXlu4nyWGH1d6D+dGPfjTre9Gc4md6ejqnnHLKm3ZRr3ffffeVbU889dRTpftnnHFG6X6S3HDDDWXbV155Zdl2kixbtizr168v29+4cWPZ9sT3vve90v1XXnmldD9J7rzzztL9Cy644PHK/enp6Zx88sll+6effnrZ9sSll15aur99+/bS/SR56aWXyravuOKKsu0kWbJkSS6//PKy/ZmZmbLtiQcffLB0/8ILLyzdT5IjjzyydP/UU0+d9b3Ix14AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaEX8AACtiB8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtDI1l4MXLVqUs846q+pacuKJJ5ZtT1x//fWl+4cdVt+TX/va18q2t2/fXradJNu2bcvGjRvL9qem5vSS/rlcc801pftf/vKXS/eT5NOf/nT5OSrt3r07mzZtKtu/5557yrYnTj755NL9/fv3l+4nya233lq2ffjhh5dtJ8nTTz+d9evXl+1v2LChbHvi29/+dun+xz/+8dL9JFm5cmX5OWbjzg8A0Ir4AQBaET8AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaEX8AACtiB8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtCJ+AIBWpuZy8LPPPpsvfOELVdeS888/v2x7YvHixaX79957b+l+kqxZs6Zse//+/WXbSbJw4cK8733vK9u/5ZZbyrYnPvzhD5fuv/e97y3dT5LzzjuvdH/9+vWl+6tWrcqGDRvK9u+6666y7Yljjz22dH/fvn2l+0myYMGCsu3DDqv9u/lJJ52UO+64o2x/5cqVZdsTDz30UOn+1q1bS/eT5Nprry0/x2zc+QEAWhE/AEAr4gcAaEX8AACtiB8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtCJ+AIBWxA8A0Ir4AQBaET8AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVqbmcvCaNWvypS99qepacv/995dtT5x77rml+7fffnvpfpI88MADZduXXHJJ2XaSzMzM5MYbbyzbv+OOO8q2Jz73uc+V7n/sYx8r3U+SdevWlZ+j0vz583PssceW7d99991l2xPXX3996f7atWtL95PkrrvuKtves2dP2XaS7Ny5M9/61rfK9s8555yy7Ymzzz67dH/Lli2l+0myadOm0v1hGGZ93J0fAKAV8QMAtCJ+AIBWxA8A0Ir4AQBaET8AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaEX8AACtiB8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArUzN5eDnn38+t99+e9W15CMf+UjZ9sQnPvGJ0v3rrruudD9Jtm7dWrZ94MCBsu0k2bx5c1avXl22//DDD5dtT0xPT5fuf+YznyndT5IdO3aUn6PSokWLsm7durL9Sy65pGx74tZbby3dv+mmm0r3k9r3ov3795dtJ8mePXuyefPmsv21a9eWbU+sXLmydP+JJ54o3U+Siy66qPwcs3HnBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaEX8AACtiB8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtCJ+AIBWxA8A0Ir4AQBaGcZx/NkPHoZtSR6vuxx+Bawcx/HoqnGvoTa8jvhFeQ3xZpj1dTSn+AEA+HXnYy8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKCV/wXjGc5nFioaXwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 720x360 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAThCAYAAAA1YTsWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAA5C0lEQVR4nO3ae7SfBX3n+88v2bmH3CCQQLgWJIgiIh5lBMXLgNrRUqVFoHgcILYVL1hKlyCyRrS21RanHS9Lq63FS7kckKqloEBRcaxyWWDQQQFDuARCQggksJMQ/J0/6sxyXJNs9pJvcvye12st1iLZD5/fk72f/ex3nh+D4XAYAICOJmzvEwAAqCJ0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHWC7GQwGCweDwVcGg8GKwWAwHAwGe23vcwJ6ETrA9vSzJFcmeeP2PhGgJ6ED/G8Gg8Hug8HgssFgsGowGDw8GAw+NhgMJgwGg3MGg8HywWDw0GAwuGAwGMz++fF7/fxpzP89GAzuGQwGqweDwXt//rFdB4PB6GAwmPcL+8//+TGThsPhyuFw+IkkN2ynPy7QnNAB/pfBYDAxydeSLE+yV5LdklyY5C0//+flSfZJMjPJx37pPz88yf5JXpnk3MFgcMBwOFyR5Lv535/YnJDk/xkOh09W/TkA/iehA/yi/yvJrknOHA6Hjw+Hww3D4fD6JCcmOX84HP50OByuT3JWkjcNBoORX/hv3z8cDkeHw+GtSW5N8ryf//6XkhyfJIPBYJDkTT//PYByQgf4RbsnWT4cDjf/0u/vmn9/yvM/LU8ykmSXX/i9B3/h35/Ivz/1SZJLkxw2GAwWJnlp/v3/y/n2M3nSAFsyMvYhwP+P3Jtkj8FgMPJLsbMiyZ6/8Os9kmxOsjLJoq0NDofDRwaDwdeTHJfkgCQXDofD4TN72gD/Z57oAL/o+0keSPLng8FgxmAwmDoYDF6S5B+TvHswGOw9GAxmJvlQkov+D09+tuRLSd6c5Nj80ttWg8FgapIpP//llJ//GuAZIXSA/2U4HD6V5HVJ9k1yT5L78u9PYv4uyeeTfCvJsiQbkrxjHNNfSbJfkgd//v/w/KLRJOt//u+3//zXAM+IgSfIAEBXnugAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQ1sh4Dp4xY8Zw3rx5JScyc+bMkt0kuffee8u2k2TXXXct277//vtLdjdu3JjNmzcPSsa3YmRkZDh58uSS7ardJKm67v+n0dHRsu0FCxaUbd9yyy2rh8Ph/LIX2IJZs2YNd95555Ltqu+5JNmwYUPZdpLMmTOnbHvixIklu+vXr8+GDRu2y71o0qRJ2/plf2UTJtQ+nxgM6r4UTz75ZNn2pk2btngvGlfozJs3L6effvozclK/7PDDDy/ZTZJ3vetdZdtJ8sEPfrBs+6yzzirZ/dGPflSyO5bJkydnv/32K9neY489SnaT5IQTTijbTpKlS5eWbf/Jn/xJ2fbcuXOXl41vxc4775yPfOQjJdtnn312yW6S3H777WXbSXLkkUeWbc+dO7dk9ytf+UrJ7lgmTZqUfffdt2R78+bNJbtJ7UOBpPYvjCtWrCjbvvvuu7d4L/LWFQDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtjYzn4EcffTT//M//XHIiN910U8luktx6661l20myevXqsu0XvvCFJbvLly8v2R3L6OhofvCDH5Rs77XXXiW7SfLhD3+4bDtJPvWpT5Vtz5kzp2x7e9m4cWOWLVtWsv2f//N/LtlNkpGRcd1yx+3II48s2z7qqKNKdteuXVuyO5Z99903l19+ecn2jTfeWLKbJO985zvLtpNk9913L9veddddy7bvvvvuLX7MEx0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbI+M5eP369bn++utLTuRnP/tZyW6SLF26tGw7Sb7//e+XbS9evLhkd+rUqSW7Y5k9e3aOOOKIku2DDz64ZDdJlixZUradJOedd17Z9owZM8q2t5eJEydm1qxZJdunnnpqyW6S3HbbbWXbSXL55ZeXbf/whz8s2T3qqKNKdseybNmynHjiiSXbF110UclukqxcubJsO0n+8i//smz77W9/e9n2tGnTtvgxT3QAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoK2R8Rx80EEH5etf/3rJiTz11FMlu0nyqU99qmw7Sd7znveUbd9www0lu9OnTy/ZHcvee++dCy64oGT7a1/7WslukrzwhS8s206SffbZp2z77rvvLtu++OKLy7a35oknnsitt95asr3bbruV7CbJAw88ULadJH/8x39ctv25z32uZPfhhx8u2R3L3nvvnc9//vMl23vttVfJbpLssMMOZdtJcvbZZ5dtX3755WXbW+OJDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoK2RcR08MpKdd9656lzKfOpTnyrdf//731+2vccee5TsPvjggyW7T8eECTV9/bu/+7slu0ly+OGHl20nybvf/e6y7Re84AVl29vLwoULc/bZZ5dsv+hFLyrZTZI3vvGNZdtJ8qUvfalsu+rzvWbNmpLdsSxfvjynnXZayfYhhxxSspsk06dPL9tOkuc85zll22eddVbZ9tZ4ogMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhrMBwOn/7Bg8GqJMvrTodtaM/hcDh/W7+oa6gd1xG/KtcQz4QtXkfjCh0AgF8n3roCANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoK2R8Rw8ffr04Zw5c0pOZObMmSW7SbJ8+fKy7SQ54IADyrbXrl1bsvvwww9n3bp1g5LxrZg6deqw6mu9yy67lOwmyWOPPVa2nSQrVqwo295pp53Kth966KHVw+FwftkLbMGMGTOGc+fOLdkeHR0t2U2S2bNnl20nySOPPFK6X+GJJ57Ixo0bt/m9aGRkZDhp0qSS7R122KFkN6n9fq62cePGsu2f/vSnW7wXjSt05syZk1NOOeWZOatf8rKXvaxkN0mWLFlStp0k1157bdn25ZdfXrJ73nnnleyOZebMmXn9619fsv1Hf/RHJbtJcuWVV5ZtJ7Vfj+OPP75s+6//+q9r/xaxBXPnzs273vWuku1bb721ZDdJXv3qV5dtJ8lll11Wtj0cDkt2r7vuupLdsUyaNCn77rtvyfYRRxxRspskJ598ctl2kkyYUPdGz7Jly8q2jz322C3ei7x1BQC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbI+M5ePPmzVm7dm3JiSxevLhkN0muueaasu0kOfroo8u2n//855fsPv744yW7YxkZGcn8+fNLts8555yS3SR57LHHyraTZN26dWXbZ511Vtn2X//1X5dtb8306dPzvOc9r2R7OByW7CbJwQcfXLadJMuWLSvbrvq+vfnmm0t2xzJ9+vQ897nPLdn+8Ic/XLKbJDNnzizbTpK3ve1tZdvvec97yra3xhMdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAWyPjOXj+/PlZsmRJyYlcfvnlJbtJctBBB5VtJ8mNN95Ytv32t7+9ZPcb3/hGye5YFi5cmLPOOqtk+7rrrivZTZJNmzaVbSfJnXfeWbb98Y9/vGx7e5k1a1aOOuqoku3f/d3fLdlNkh/+8Idl20kyadKk0v0Kmzdv3i6vu+eee+azn/1syfayZctKdpNk6dKlZdtJcuihh5Ztv+Md7yjb3hpPdACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgrZHxHHz77bfniCOOKDmRO++8s2Q3SV7+8peXbSfJj370o7LtU089tWR31apVJbtjWbNmTS666KKS7auuuqpkN0le/OIXl20nyWc+85my7X/7t38r295ebrrppkyaNKlk+4knnijZTZJXvepVZdvV++9+97tLdr/85S+X7I7lvvvuy5lnnlmy/fGPf7xkN0n+7M/+rGw7SY455piy7UsuuaRse2s80QEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQ1Mp6DDzzwwFxxxRUlJ3LSSSeV7CbJJz7xibLtJNl7773Ltr/zne+U7B566KElu2OZNm1aDjzwwJLtl770pSW7SXLAAQeUbSfJySefXLZ9yCGHlG1vLwceeGAuvvjiku3JkyeX7CbJkiVLyraT5CMf+UjZ9vve976S3QkTts/ft2fPnp1Xv/rVJdtnnHFGyW6SnHbaaWXbSXLmmWeWbR922GFl21deeeUWP+aJDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoK3BcDh8+gcPBquSLK87HbahPYfD4fxt/aKuoXZcR/yqXEM8E7Z4HY0rdAAAfp146woAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtkbGc/C8efOGixYtKjmRpUuXluwmyf7771+2nSR33HFH2fbixYtLdlesWJFHHnlkUDK+FTNmzBjOmTOnZHvixIklu0kyf/78su0kGQzqvhTLli0r216zZs3q4XBY+8n5P5gzZ85wwYIFJdurVq0q2U2SzZs3l20nyU477VS2PXv27JLde+65J6tXr97m96LJkycPp02bVrI9OjpaspskM2bMKNtOkqr7c5Js2LChbPvBBx/c4r1oXKGzaNGiXHHFFc/MWf2S3XffvWQ3ST7zmc+UbSfJa1/72rLtf/zHfyzZPf7440t2xzJnzpz84R/+Ycl21Y04Sd761reWbSfJlClTyrZ/7/d+r2z7i1/84vKy8a1YsGBB2ff1pz/96ZLdJFm9enXZdpKccsopZduvec1rSnYPP/zwkt2xTJs2LS95yUtKtm+55ZaS3SR58YtfXLadJK9//evLtn/yk5+Ubf/Zn/3ZFu9F3roCANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtjYzn4NHR0dxyyy0lJ/KhD32oZDdJDj/88LLtJHnTm95Utv2nf/qnJbsPPPBAye5Y1q1bl+uuu65ke/HixSW7SXLDDTeUbSfJueeeW7b96U9/umz7i1/8Ytn21vz4xz/Oy1/+8pLtF7/4xSW7SXLccceVbSfJm9/85rLtD3zgAyW7q1atKtkdy3777ZcrrriiZHvWrFklu0ly5JFHlm0nyWmnnVa2vccee5Rtb40nOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLZGxnPwww8/nC984QslJ3L//feX7CbJm970prLtJHnzm99ctn311VeX7H77298u2R3LtGnT8tznPrdke+HChSW728IxxxxTtv3Od76zbHt72Xnnncu+r88555yS3SR54IEHyraT5LHHHivbvuqqq0p2p06dWrI7lnXr1uWb3/xmyfbtt99espskH/nIR8q2k+SJJ54o2z7ppJPKtt/73vdu8WOe6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAWyPjOXjSpElZuHBhyYm8/vWvL9lNkjVr1pRtJ8kLXvCCsu2/+qu/KtkdHR0t2R3LLrvskne9610l23/0R39UspskJ5xwQtl2kjz11FNl2zfddFPZ9vayfv36fOc73ynZrvxaLF68uGw7Sc4///yy7de97nUlu1OmTCnZHcuyZcty0kknlWwfcsghJbtJcuihh5ZtJ8nxxx9ftv0bv/EbZdtb44kOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgrZHxHLxw4cK8733vKzmRBx98sGQ3SX74wx+WbSfJscceW7Z99913l21vD/fff3/OPvvsku0vf/nLJbtJsn79+rLtJJk8eXLZ9mc/+9my7QsuuKBse2t22WWXnH766SXbCxYsKNlNkgsvvLBsO0nWrVtXtn3AAQeUbW8PixYtynnnnVeyvWTJkpLdJLniiivKtpPk/PPPL9u+7777yra3xhMdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW4PhcPj0Dx4MViVZXnc6bEN7DofD+dv6RV1D7biO+FW5hngmbPE6GlfoAAD8OvHWFQDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtjYzn4BkzZgznzJlTciITJ04s2U2SVatWlW0nyebNm8u2Z82aVbK7fv36bNy4cVAyvhVTp04d7rDDDiXbO++8c8lukvzoRz8q206Sfffdt2x7+fLlZdtPPvnk6uFwOL/sBbZg7ty5w912261ke+rUqSW7SXLfffeVbVer+rw8/PDDWbdu3Ta/F02aNGlY9WeaNGlSyW6S7LPPPmXbSbJu3bqy7SeeeKJs+7777tvivWhcoTNnzpz8/u///jNzVr9k7ty5JbtJ8ulPf7psO0keeuihsu1XvvKVJbtXXXVVye5YdthhhxxzzDEl2+94xztKdpPkec97Xtl2kvzN3/xN2faSJUvKtu+///66itqK3XbbLZdddlnJ9rOe9ayS3ST5kz/5k7LtJBkM6nphv/32K9n94Ac/WLI7lqlTp+bggw8u2d51111LdpPkoosuKttOkuuuu65s++abby7bPuOMM7Z4L/LWFQDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtjYzn4IULF+bcc88tOZELL7ywZDdJNm/eXLadJN///vfLtn/yk5+U7N54440lu2NZvXp1Pve5z5Vs/+3f/m3JbpJMmTKlbDtJ3vnOd5Ztz5w5s2x7e9m0aVOWLVtWsv13f/d3JbtJ/ddi1qxZZdt33HFHye6GDRtKdseyadOmrFixomT7tttuK9lNkuc+97ll20ly1VVXlW1fe+21Zdtb44kOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgrZHxHPzYY4/lG9/4RsmJ3HzzzSW7SXL77beXbSfJpz71qbLtW265pWR35cqVJbtjmTdvXo4++uiS7aVLl5bsJsnIyLi+Vcbt6quvLts+99xzy7Z//OMfl21vzdq1a3P55ZeXbC9evLhkN0lOP/30su2k9jr6yle+UrL7xBNPlOyOZdasWXnFK15Rsv2DH/ygZDdJNm3aVLadJLvvvnvZ9je/+c2y7Q984ANb/JgnOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQ1sh4Dt68eXNWrlxZciLTpk0r2U2Siy66qGw7SZ71rGeVbT/22GMluzfeeGPJ7lg2bdqUe++9t2T7wQcfLNlNkhe84AVl20ly3HHHlW2/5S1vKdu+4IILyra3ZuPGjbn77rtLti+55JKS3SSZO3du2XaS/PSnPy3brrrPLV26tGR3LBs2bMidd95Zsv3973+/ZDdJTjzxxLLtJBkdHS3bnjJlStn21niiAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaGtkPAevXr06n/vc50pO5Pd///dLdpPkZz/7Wdl2kvzLv/xL2fZb3vKWkt1rr722ZHcsEyZMyPTp00u2v/nNb5bsJskZZ5xRtp0kJ554Ytn22WefXba9vey6664599xzS7Y/+tGPluwmyVe/+tWy7ST51re+Vba9YMGCkt2JEyeW7I5lwoQJmTZtWsn2UUcdVbKbJLvttlvZdpJs2LChbPsVr3hF2fbWeKIDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoazAcDp/+wYPBqiTL606HbWjP4XA4f1u/qGuoHdcRvyrXEM+ELV5H4wodAIBfJ966AgDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKCtkfEcvNNOOw333HPPkhNZv359yW6SrF27tmw7SaZNm1a2vXLlypLdJ598Mps3bx6UjG/FyMjIcPLkySXbCxcuLNlNkgkTav9OMHv27LLtVatWlW3fc889q4fD4fyyF9iC6dOnD+fMmVOy/dRTT5XsJsnq1avLtpNkxowZZdvr1q0r2x4Oh9v8XrTjjjsOd99995LtkZFx/Wgdl7vuuqtsO0meeOKJsu3K762nnnpqi/eicX019txzz3z3u999Zs7ql1x//fUlu0nyT//0T2XbSXLwwQeXbf/VX/1VyW71N8uWTJ48Ofvvv3/J9tlnn12ymyQzZ84s206S17zmNWXbn/zkJ8u23/a2ty0vG9+KOXPm5JRTTinZfvzxx0t2k+Qzn/lM2XaSvOhFLyrbvvrqq8u2t4fdd9891157bcn2vHnzSnaT5Ld/+7fLtpPk5ptvLtt+9NFHK7e3eC/y1hUA0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbY2M5+A77rgjr371q0tOZNKkSSW7SXLppZeWbSfJ4sWLy7bvu+++kt1DDz20ZHcs8+fPzx/8wR+UbM+bN69kN0nOPffcsu0kee1rX1u2feKJJ5Ztby+PPfZYrr766pLt73znOyW7SbJixYqy7SR5wxveULZddR992cteVrI7ljvuuCNHH310yXbl/XWXXXYp206S0047rWz71FNPLdvecccdt/gxT3QAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtjYzn4ClTpmSfffYpOZEbb7yxZDdJfvu3f7tsO0luuummsu3NmzeX7A6Hw5LdsTz00EP5m7/5m5LtD33oQyW7SXLJJZeUbSfJs5/97LLtiRMnlm1vLwsXLsx73/veku3Kr/XUqVPLtpNk+vTpZdtLliwp2V2+fHnJ7lg2bNiQ//E//kfJduXPs3/9138t206Sf/iHfyjbPv/888u2t8YTHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoa2S8/8FgMKg4j/z93/99yW6STJs2rWw7SS6++OKy7Ze97GUluxs2bCjZHctOO+2Uk08+uWT7wAMPLNlNkksuuaRsO0nWrl1btn399deXbV9wwQVl21vzyCOP5LLLLivZftvb3laymySf+9znyraTZP/99y/bPumkk0p2K6/PrVm0aFHOOuusku1bbrmlZDdJjjvuuLLtJPn0pz9dtv3e9763bHtrPNEBANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0NTKeg+fOnZvjjjuu5EQeeeSRkt0kOeSQQ8q2k+SYY44p2/7KV75SsnvvvfeW7I5l7dq1ufzyy0u299tvv5LdpP4aOvbYY8u2X/va15Ztby9z5szJb/3Wb5Vsv/SlLy3ZTZKTTjqpbDtJRkbGdUsfl4985CMlu+vXry/ZHcu8efNy/PHHl2yfcsopJbtJ8v73v79sO0kuuOCCsu0rr7yybHv33Xff4sc80QEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQ1GA6HT//gwWBVkuV1p8M2tOdwOJy/rV/UNdSO64hflWuIZ8IWr6NxhQ4AwK8Tb10BAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0NbIeA6ePXv2cJdddik5kc2bN5fsJsno6GjZdpKsX7++bPs3fuM3SnbvvffePPzww4OS8a2YPXv2cOeddy7ZnjJlSslu8u+fr0qTJk0q254/f37Z9u233756OBzWvcAWzJ07d7hw4cKS7RUrVpTsJsn06dPLtpNk7ty5ZdvD4bBkd8WKFVm7du12uRctWLCgZHvDhg0lu0ny6KOPlm0ntfeLlStXlm2vW7dui/eicYXOLrvsko9//OPPzFn9kocffrhkN0l+8IMflG0nyX//7/+9bPviiy8u2T3qqKNKdsey884756Mf/WjJ9j777FOymyRnnHFG2Xby75+XKqeddlrZ9ote9KLlZeNbsXDhwnzpS18q2T7nnHNKdpPkkEMOKdtOkje+8Y1l208++WTJ7pvf/OaS3bEsWLAgn/zkJ0u277zzzpLdJLniiivKtpPkrW99a9n2+eefX7Z9zTXXbPFe5K0rAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoaGc/Bd911V4499tiSE3nsscdKdpNk6tSpZdtJ8rWvfa1s++KLLy7ZXbNmTcnuWEZHR/PDH/6wZPuyyy4r2U2SF77whWXbSXLzzTeXbc+dO7dse3sZHR3N0qVLS7Zf9apXlewmyWGHHVa2nSSXXHJJ2fakSZNKdteuXVuyO5aNGzfm7rvvLtmeNm1ayW6SfOELXyjbTpIzzzyzbHvy5Mll21vjiQ4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKCtkfEcvGDBgvzBH/xByYnMnj27ZDdJHn/88bLtJDniiCPKtq+55pqS3SeffLJk9+m87sqVK0u2L7jggpLdJLnrrrvKtpPkYx/7WNn2ueeeW7a9vUyfPj3Pf/7zS7ZvuOGGkt0kmTNnTtl2klx11VVl20cffXTJ7nA4LNkdy+TJk7PrrruWbK9YsaJkN0lOP/30su0kue2228q2K3/Ob40nOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQ1sh4Dl64cGHe9773lZzIjjvuWLKbJM973vPKtpPk29/+dtn2xIkTS3ZHR0dLdseyevXqfPazny3ZPvLII0t2k2SvvfYq206S4XBYtn3UUUeVbW8vjz/+eL73ve+VbF966aUlu0nyH/7DfyjbTpIbb7yxbLvq833llVeW7I7l7rvvzimnnFKy/eUvf7lkN0ne8IY3lG0nydy5c8u2K7/WX//617f4MU90AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbY2M5+DNmzdn9erVJSfy4Q9/uGQ3Sb785S+XbSfJrFmzyrZPPvnkkt0f//jHJbtjmT59eg455JCS7Z/85Cclu0ly9dVXl20nye/8zu+UbX/1q18t2546dWrZ9tZMmDAhU6ZMKdm+4YYbSnaTZP/99y/bTpI1a9aUbW/atKlkdzgcluyO5aCDDsqNN95Ysv2FL3yhZDdJPvnJT5ZtJ8miRYvKto888siy7a3xRAcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANDWYDgcPv2DB4NVSZbXnQ7b0J7D4XD+tn5R11A7riN+Va4hnglbvI7GFToAAL9OvHUFALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFsj4zp4ZGQ4efLkkhPZvHlzyW6S7LPPPmXbSfLTn/60bHv//fcv2b3//vuzZs2aQcn4VsybN2+4aNGiku1NmzaV7CbJI488UradJNOmTSvbXrduXdn2mjVrVg+Hw/llL7AFU6ZMGc6YMaNke9asWSW7SfKzn/2sbDupvU532223kt0HH3wwa9eu3eb3op122mm45557lmwPBnV/nJUrV5ZtJ8mjjz5atl35c350dHSL96Jxhc7kyZPLfvCuWrWqZDdJ/vZv/7ZsO0ne9KY3lW1ffvnlJbvHHHNMye5YFi1alCuuuKJk+5577inZTZILL7ywbDtJDjrooLLtb33rW2Xbn//855eXjW/FjBkz8qpXvapk+zWveU3JbpI8/vjjZdtJcumll5Ztn3feeSW7S5YsKdkdy5577pnvfe97JdsjI+P60TouH/3oR8u2k+Rf/uVfyrYfeuihsu1bb711i/cib10BAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0NZgOBw+7YMXLVo0fPvb315yIu95z3tKdpPksMMOK9tOktWrV5dtP/DAAyW7o6OjeeqppwYl41sxa9as4aGHHlqyfe2115bsJskb3vCGsu0kWbBgQdn2ueeeW7a9cOHCm4bDYc0XdCtmzJgxPOCAA0q2b7zxxpLdJNljjz3KtpNkxx13LNu+5ZZbyraHw+E2vxfNmTNneOSRR5Zsv/zlLy/ZTWqvz6T2XlT5efnN3/zNLd6LPNEBANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0NRgOh0/74JkzZw4PPvjgkhM57bTTSnaT5D/+x/9Ytp0k//Zv/1a2feWVV5bsXnzxxXnooYcGJeNb8ZznPGd48cUXl2zffvvtJbtJ8qxnPatsO0keeOCBsu3Xve51ZdsbN268aTgcHlr2Alswc+bM4XOf+9yS7RNOOKFkN0m+973vlW0nyb777lu2XXXv/+M//uPceeed2/xeNBgMnv4Pv3Haddddq6bL70Uf+MAHyrZPP/30su2bbrppi/ciT3QAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoK2R8Rw8a9asvPKVryw5kRNOOKFkN0n+63/9r2XbSfKKV7yibPuNb3xjye43vvGNkt2xrF69On//939fsr106dKS3ST54he/WLadJBdeeGHZ9qmnnlq2/fGPf7xse2smT56c3XffvWR71qxZJbtJ8v73v79sO6n9Wn/4wx8u2d2wYUPJ7limTp2afffdt2T77rvvLtlNkjPOOKNsO0m+9KUvlW3/zu/8Ttn2TTfdtMWPeaIDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoa2Q8B8+ePTuvfvWrS05k48aNJbtJctBBB5VtJ8lhhx1Wtr1kyZKS3bVr15bsjmXq1Kl59rOfXbL9l3/5lyW7SfLoo4+WbSfJXXfdVbY9Ojpatr09DYfDkt2PfexjJbtJsnjx4rLtJHnJS15Stn3bbbeV7D755JMlu2PZeeed84d/+Icl2+95z3tKdpPkRz/6Udl2kjz++ONl21/72tfKtrfGEx0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbg+Fw+PQPHgxWJVledzpsQ3sOh8P52/pFXUPtuI74VbmGeCZs8ToaV+gAAPw68dYVANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG2NjOfgwWAwrDqRwWBQNZ2RkXH9Mf8/5cknnyzbHg6HdZ/0LZg+ffpwzpw5Jdu77rpryW6S3HHHHWXbSbJu3bqy7QMPPLBs+7bbbls9HA7nl73AFoyMjAwnT55csj1hQt3f//bdd9+y7STZsGFD6X6FBx98MGvXrt3m96IJEyYMJ06cWLK9xx57lOwmycMPP1y2nSSjo6Nl21OmTCnbXrdu3RbvRf+fKYDKT8COO+5Ytp3URtr9999fsjscljXrVs2ZMydvfetbS7b/y3/5LyW7SXL00UeXbSfJv/7rv5ZtX3bZZWXbz3rWs5aXjW/F5MmTs//++5dsV96LvvrVr5ZtJ8mPf/zj0v0Kp5566nZ53YkTJ2bevHkl23/+539espskn//858u2k+TWW28t295vv/3Ktq+55pot3ou8dQUAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoaGc/BBx10UK688sqSEznmmGNKdpPkBS94Qdl2kvy3//bfyrYnTpxYsnvooYeW7I5l6tSpWbx4ccn29ddfX7Kb1F6fSfKSl7ykbPvkk08u295eNm7cmDvvvLNk+4477ijZTZLZs2eXbSfJqaeeWrZ9++23l+zOnDmzZHcsO+64Y37v936vZPvEE08s2U2SRx99tGw7SR5++OGy7WuvvbZs+5prrtnixzzRAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtDUynoMHg0EmTpxYciJnnnlmyW6S/MM//EPZdpL88z//c9n2pEmTSnYfffTRkt2xLFu2LMcff3zJ9nXXXVeymyTPfvazy7aT5D/9p/9Utr3XXnuVbW8vO+ywQ1760peWbP/mb/5myW6S3HHHHWXbSbJy5cqy7aVLl5bsjo6OluyO5ZFHHskll1xSsn3uueeW7CbJX/zFX5RtJ8lznvOcsu2FCxeWbW+NJzoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0NbIeA7euHFj7rrrrpIT2W233Up2k+S8884r206SV73qVWXba9asKdveHg455JB897vfLdn++te/XrKbJCeccELZdpL80z/9U9n2wQcfXLa94447lm1vzfTp0/P85z+/ZPurX/1qyW6SHH/88WXbSbJ+/fqy7Q9+8IMluw888EDJ7lg2bdqUe+65p2T7nHPOKdlNkiuvvLJsO0m++c1vlm1fdtllZdtb44kOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgrZHxHLxs2bKcdNJJJScybdq0kt0kWb16ddl2kvzWb/1W2fa1115bsvvAAw+U7I7lkUceyaWXXlqy/bGPfaxkN0k+8YlPlG0nycjIuL4Vx+XEE08s295eZs6cmSOOOKJke/HixSW7SfLa1762bDtJrrrqqrLtVatWlexu3ry5ZHcse++9d/70T/+0ZPsv/uIvSnaT5NRTTy3bTpI99tijbPvRRx8t2/7JT36yxY95ogMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhrMBwOn/7Bg8GqJMvrTodtaM/hcDh/W7+oa6gd1xG/KtcQz4QtXkfjCh0AgF8n3roCANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDa+n8BRjtUPOborzcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(stop_line_estimator.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 2, 4, 'conv0', size=(10,5))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopLineEstimator(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.15, inplace=False)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Dropout(p=0.15, inplace=False)\n",
       "    (6): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.15, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout(p=0.15, inplace=False)\n",
       "    (11): Conv2d(8, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "stop_line_estimator.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "stop_line_estimator.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "stop_line_estimator.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(stop_line_estimator, dummy_input, onnx_model_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "stop_line_estimator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 4920.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[-0.11877473  0.03234711]]\n",
      "Predictions shape: (1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_model_path)\n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
