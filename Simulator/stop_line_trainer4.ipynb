{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/stop_line_estimator_advanced.pt'\n",
    "onnx_model_path = \"models/stop_line_estimator_advanced.onnx\"\n",
    "max_load = 250_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE\n",
    "\n",
    "# class StopLineEstimator(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             nn.Linear(in_features=4*4*4, out_features=32),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(in_features=32, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# stop_line_estimator = StopLineEstimator(out_dim=3,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class StopLineEstimator(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        prob = 0.2\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 28\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=14\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(8, 8, kernel_size=5, stride=1), #out = 10\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=5\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(8, 128, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            #normalize\n",
    "            # nn.BatchNorm1d(3*3*4),\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=1*1*128, out_features=32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=32, out_features=16),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(in_features=16, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "stop_line_estimator = StopLineEstimator(out_dim=3,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "stop_line_estimator.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = stop_line_estimator(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from time import time, sleep\n",
    "\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "    #convert to gray\n",
    "    img = cv.resize(img, (4*SIZE[1], 4*SIZE[0]))\n",
    "\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "        axes_length = (randint(10//4, 50//4), randint(50//4, 300//4))\n",
    "        angle = randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (50,50))\n",
    "    noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = np.clip(light, 0, 51)\n",
    "    light *= 5\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 5), randint(1, 5)))\n",
    "\n",
    "    # cut the top third of the image, \n",
    "    img = img[int(img.shape[0]*(2/5)):,:] ################################# 2/5 frame[int(frame.shape[0]*(2/5)):,:]\n",
    "\n",
    "    #edges\n",
    "    img = cv.resize(img, (2*SIZE[1], 2*SIZE[0]))\n",
    "\n",
    "    # erosion and dilation\n",
    "    r = randint(0, 5)\n",
    "    if r == 0:\n",
    "        #dilate\n",
    "        kernel = np.ones((randint(1, 3), randint(1, 3)), np.uint8) #kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.dilate(img, kernel, iterations=1)\n",
    "    elif r == 1:\n",
    "        #erode\n",
    "        kernel = np.ones((randint(1, 3), randint(1, 3)), np.uint8) #kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.erode(img, kernel, iterations=1)\n",
    "\n",
    "    #edges    \n",
    "    img = cv.Canny(img, 100, 200)\n",
    "\n",
    "    #blur\n",
    "    img = cv.blur(img, (3,3))\n",
    "\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    # #get max brightness\n",
    "    # max_brightness = np.max(img)\n",
    "    # ratio = 255.0/max_brightness\n",
    "    # #normalize\n",
    "    # img = (img*ratio).astype(np.uint8)\n",
    "\n",
    "    # #add random tilt\n",
    "    # max_offset = 1\n",
    "    # offset = randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset, axis=0)\n",
    "    # if offset > 0:\n",
    "    #     img[:offset, :] = 0 #randint(0,255)\n",
    "    # elif offset < 0:\n",
    "    #     img[offset:, :] = 0 # randint(0,255)\n",
    "    \n",
    "    #add noise \n",
    "    std = 60\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(100)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "#TODO add negative examples: inside intersection, in normal road with high curvature\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        # junction_images_indexes = []\n",
    "        # tot_lines = 0\n",
    "        # with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "        #     lines = f.read().split('\\n')\n",
    "        #     lines = lines[0:-1] #remove footer\n",
    "        #     tot_lines = len(lines)\n",
    "        #     for i,line in enumerate(lines):\n",
    "        #         if line == '1,0,0,0,0,0,0,1,0,0,0,0,0,0,0':\n",
    "        #             junction_images_indexes.append(i)\n",
    "        # junction_imgs_mask = np.zeros(tot_lines, dtype=bool)\n",
    "        # junction_imgs_mask[junction_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(200, max_load)): #start from 200 since first imgs have wrong labels\n",
    "            # for i in range(max_load):\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                line = line.split(',')\n",
    "                #x stopline, y stopline, yaw stopline\n",
    "                label = np.array([float(line[4]), float(line[5]), float(line[6])], dtype=np.float32)\n",
    "                dist_label = float(line[4]) #dist is used only to filter images\n",
    "                \n",
    "                MAX_DIST = 9.0\n",
    "                #keep only small distanaces, avoid junctions\n",
    "                if dist_label < MAX_DIST: #and not junction_imgs_mask[i]:  #if  dist_label < 0.6 and not junction_imgs_mask[i]: \n",
    "                    # print(f'Sample {i},  idx = {all_img_idx},  dist = {dist_label}')\n",
    "                    # if dist_label < MIN_DIST:\n",
    "                    #     dist_label = MAX_DIST+MIN_DIST  - dist_label\n",
    "                    #img \n",
    "                    img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "                    img = load_and_augment_img(img)\n",
    "                    if all_img_idx < 1000:\n",
    "                        # cv.putText(img, f'{dist_label[0]:.2f}', (5,10), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                        # cv.putText(img, f'{np.rad2deg(angle_label[0]):.0f}', (5,25), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                        cv.imshow('img', img)\n",
    "                        # print(label)\n",
    "                        cv.waitKey(1)\n",
    "                        if all_img_idx == 999:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(label)\n",
    "                    \n",
    "                    all_img_idx += 1\n",
    "\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "            self.data = np.array(self.data)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91941/91941 [00:48<00:00, 1914.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all imgs: torch.Size([20237, 32, 32, 1])\n",
      "data: (20237, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 1, 32, 32])\n",
      "torch.Size([8192, 3])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    pos_losses = []\n",
    "    angle_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        pos = output[:, 0:1]\n",
    "        angle = output[:, 2]\n",
    "\n",
    "        pos_label = regr_label[:, 0:1]\n",
    "        angle_label = regr_label[:, 2]\n",
    "\n",
    "        # Compute the losses\n",
    "        pos_loss = 1.0*regr_loss_fn(pos, pos_label)\n",
    "        angle_loss = 1.0*regr_loss_fn(angle, angle_label)\n",
    "    \n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = pos_loss + angle_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        pos_losses.append(pos_loss.detach().cpu().numpy())\n",
    "        angle_losses.append(angle_loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(pos_losses), np.mean(angle_losses)\n",
    "\n",
    "# VALIDATION FUNCTION\n",
    "def val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device=device):\n",
    "    stop_line_estimator.eval()\n",
    "    pos_losses = []\n",
    "    angle_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = stop_line_estimator(input)\n",
    "        regr_out = output\n",
    "        pos = regr_out[:, 0:1]\n",
    "        angle = regr_out[:, 2]\n",
    "        dist_label = regr_label[:, 0:1]\n",
    "        angle_label = regr_label[:, 2]\n",
    "        pos_loss = 1.0*regr_loss_fn(pos, dist_label)\n",
    "        angle_loss = 1.0*regr_loss_fn(angle, angle_label)\n",
    "        pos_losses.append(pos_loss.detach().cpu().numpy())\n",
    "        angle_losses.append(angle_loss.detach().cpu().numpy())\n",
    "    return np.mean(pos_losses), np.mean(angle_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  500/500 \n",
      "Pos loss: 0.0526 --- val pos loss: 0.0612\n",
      "angle loss: 2.5590 --- val angle loss: 3.2903\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.003 #0.005\n",
    "epochs = 500\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = .1*9e-4 #0.001 \n",
    "L2_lambda = .1*1e-2 #2e-2\n",
    "optimizer = torch.optim.Adam(stop_line_estimator.parameters(), lr=lr, weight_decay=9e-5) #wd = 2e-3# 3e-5\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "best_val_loss = 100.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        pos_loss, angle_loss = train_epoch(stop_line_estimator, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_pos_loss, val_angle_loss = val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Epoch  {epoch+1}/{epochs} \\nPos loss: {pos_loss:.4f} --- val pos loss: {val_pos_loss:.4f}\")\n",
    "        print(f\"angle loss: {np.rad2deg(angle_loss):.4f} --- val angle loss: {np.rad2deg(val_angle_loss):.4f}\")\n",
    "        if val_pos_loss < best_val_loss:\n",
    "            best_val_loss = val_pos_loss\n",
    "            torch.save(stop_line_estimator.state_dict(), model_name)\n",
    "            print(f'Saved model ')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 204.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val pos_loss: (0.05591245, 0.05950179)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "stop_line_estimator.load_state_dict(torch.load(model_name))\n",
    "print(f\"Val pos_loss: {val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 5, 5)\n",
      "(8, 8, 5, 5)\n",
      "(128, 8, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(stop_line_estimator.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 2, 4, 'conv0', size=(10,5))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopLineEstimator(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout(p=0.2, inplace=False)\n",
       "    (11): Conv2d(8, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=16, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "stop_line_estimator.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "stop_line_estimator.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "stop_line_estimator.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(stop_line_estimator, dummy_input, onnx_model_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "stop_line_estimator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 6746.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[ 1.0142937e-01 -1.4409826e-24  1.6099657e-01]]\n",
      "Predictions shape: (1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_model_path)\n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
