{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/stop_line_estimator.pt'\n",
    "onnx_model_path = \"models/stop_line_estimator.onnx\"\n",
    "max_load = 250_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE\n",
    "\n",
    "# class StopLineEstimator(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             nn.Linear(in_features=4*4*4, out_features=32),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(in_features=32, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# stop_line_estimator = StopLineEstimator(out_dim=1,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class StopLineEstimator(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        prob = 0.25\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 4, kernel_size=5, stride=1), #out = 28\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=14\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(4, 4, kernel_size=5, stride=1), #out = 10\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=5\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(4, 64, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            #normalize\n",
    "            # nn.BatchNorm1d(3*3*4),\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=1*1*64, out_features=32),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(in_features=32, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "stop_line_estimator = StopLineEstimator(out_dim=2,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "stop_line_estimator.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = stop_line_estimator(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from time import time, sleep\n",
    "\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "    #convert to gray\n",
    "    img = cv.resize(img, (4*SIZE[1], 4*SIZE[0]))\n",
    "\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "        axes_length = (randint(10//4, 50//4), randint(50//4, 300//4))\n",
    "        angle = randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (50,50))\n",
    "    noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = np.clip(light, 0, 51)\n",
    "    light *= 5\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 5), randint(1, 5)))\n",
    "\n",
    "    # cut the top third of the image, \n",
    "    img = img[int(img.shape[0]*(2/5)):,:] ################################# 2/5 frame[int(frame.shape[0]*(2/5)):,:]\n",
    "\n",
    "    #edges\n",
    "    img = cv.resize(img, (2*SIZE[1], 2*SIZE[0]))\n",
    "\n",
    "    # erosion and dilation\n",
    "    r = randint(0, 5)\n",
    "    if r == 0:\n",
    "        #dilate\n",
    "        kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.dilate(img, kernel, iterations=1)\n",
    "    elif r == 1:\n",
    "        #erode\n",
    "        kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.erode(img, kernel, iterations=1)\n",
    "\n",
    "    #edges    \n",
    "    img = cv.Canny(img, 100, 200)\n",
    "\n",
    "    #blur\n",
    "    img = cv.blur(img, (3,3))\n",
    "\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    # #get max brightness\n",
    "    # max_brightness = np.max(img)\n",
    "    # ratio = 255.0/max_brightness\n",
    "    # #normalize\n",
    "    # img = (img*ratio).astype(np.uint8)\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = 1\n",
    "    offset = randint(-max_offset, max_offset)\n",
    "    img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        img[:offset, :] = 0 #randint(0,255)\n",
    "    elif offset < 0:\n",
    "        img[offset:, :] = 0 # randint(0,255)\n",
    "    \n",
    "    #add noise \n",
    "    std = 60\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(100)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "#TODO add negative examples: inside intersection, in normal road with high curvature\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        junction_images_indexes = []\n",
    "        tot_lines = 0\n",
    "        with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            tot_lines = len(lines)\n",
    "            for i,line in enumerate(lines):\n",
    "                if line == '1,0,0,0,0,0,0,1,0,0,0,0,0,0,0':\n",
    "                    junction_images_indexes.append(i)\n",
    "        junction_imgs_mask = np.zeros(tot_lines, dtype=np.bool)\n",
    "        junction_imgs_mask[junction_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(200, max_load)): #start from 200 since first imgs have wrong labels\n",
    "            # for i in range(max_load):\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                dist = line.split(',')\n",
    "                #distance is the third element of the label data\n",
    "                dist_label = np.array([float(dist[2])], dtype=np.float32)\n",
    "\n",
    "                #keep only small distanaces, avoid junctions\n",
    "                if dist_label < 0.6 and not junction_imgs_mask[i]: \n",
    "                    # print(f'Sample {i},  idx = {all_img_idx},  dist = {dist_label}')\n",
    "                    if dist_label < -0.01:\n",
    "                        dist_label = 0.6 - dist_label\n",
    "                    #img \n",
    "                    img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "                    img = load_and_augment_img(img)\n",
    "                    if i < 100:\n",
    "                        cv.imshow('img', img)\n",
    "                        cv.waitKey(1)\n",
    "                        if i == 99:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(dist_label)\n",
    "                    all_img_idx += 1\n",
    "\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "            self.data = np.array(self.data)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244982/244982 [01:52<00:00, 2179.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all imgs: torch.Size([41806, 32, 32, 1])\n",
      "data: (41806, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 1, 32, 32])\n",
      "torch.Size([8192, 1])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    dist_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        dist = output[:, 0]\n",
    "\n",
    "        dist_label = regr_label[:, 0]\n",
    "\n",
    "        # Compute the losses\n",
    "        dist_loss = 1.0*regr_loss_fn(dist, dist_label)\n",
    "    \n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = dist_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        dist_losses.append(dist_loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(dist_losses)\n",
    "\n",
    "# VALIDATION FUNCTION\n",
    "def val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device=device):\n",
    "    stop_line_estimator.eval()\n",
    "    dist_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = stop_line_estimator(input)\n",
    "        regr_out = output\n",
    "        dist = regr_out[:, 0]\n",
    "        dist_label = regr_label[:, 0]\n",
    "        dist_loss = 1.0*regr_loss_fn(dist, dist_label)\n",
    "        dist_losses.append(dist_loss.detach().cpu().numpy())\n",
    "    return np.mean(dist_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  47/50 -> dist loss: 0.0274\n",
      "Validation dist loss: 0.0282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:01<00:02,  1.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17810/2195907601.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mdist_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_line_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mval_dist_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_line_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_17810/447215019.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Loop over the training batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Move the input and target data to the selected device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0melem_size\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.001 #0.005\n",
    "epochs = 50\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 9e-4 #0.001 \n",
    "L2_lambda = 1e-2 #2e-2\n",
    "optimizer = torch.optim.Adam(stop_line_estimator.parameters(), lr=lr, weight_decay=9e-5) #wd = 2e-3# 3e-5\n",
    "\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "for epoch in range(epochs):\n",
    "    # try:\n",
    "    if True:\n",
    "        dist_loss = train_epoch(stop_line_estimator, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_dist_loss = val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs} -> dist loss: {dist_loss:.4f}\\nValidation dist loss: {val_dist_loss:.4f}\")\n",
    "    # print(f\"lateral_dist_loss: {dist_loss}\")\n",
    "    # print(f\"curv_loss: {curv_loss}\")\n",
    "    torch.save(stop_line_estimator.state_dict(), model_name)\n",
    "\n",
    "#Note: sweet spot for training is around 0.016 -> 0.020, also note that training can get stuck, and loss can start improve randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 145.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val dist_loss: 0.02816889062523842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "stop_line_estimator.load_state_dict(torch.load(model_name))\n",
    "print(f\"Val dist_loss: {val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 5, 5)\n",
      "(4, 8, 5, 5)\n",
      "(4, 4, 6, 6)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAJ5CAYAAACqmupFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAf7UlEQVR4nO3dfaxnBX3n8c+5c+cZh+nwoDjKgw9hBKtSMYCuG3ct1MRuW92iCbYiG62ksVFHTRvYmAaNYrTbP4q2zcoa3NTFLSgC1lZaUKSxlJEUI0wgY3CEgWGZGWaYB+bh3nv2j2GzhJ1h5m6/XIavr1dCIpczn/O79577+71/507iMI5jAAA6m3iuHwAAwLNN8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzzAc24YhrcPw3DbMAxbh2HYOAzDl4dheMFz/biAPgQPcCQ4Osmnk7w4yauSrEzy+ef0EQGtCB7ggIZheOkwDN8YhuHRYRg2D8NwxTAME8Mw/OdhGNYPw/C/hmH46jAMRz95/MnDMIzDMFw4DMPPh2HYNAzDpU/+txcPw/DEMAwrnrJ/xpPHzB/H8WvjOP7tOI67xnF8LMl/TfKm5+YzBzoSPMD/YxiGeUluTLI+ycnZf8fl6iTve/Kff5fkZUmOSnLF0/74v0lyapK3JvnkMAyvGsfxoSQ/TPIfn3LcBUmuGcdx3wEewr9NcnfNZwOQDP6/tICnG4bhnCTXJzlhHMepp3z8H5JcO47jl57891OT/CTJ4iQvSXJ/kpeO4/jgk//9n5P8l3Ecrx6G4f1JLhjH8d8PwzAk+XmS94zjeOvTzn1ukv+Z5KxxHO97tj9X4BeDOzzAgbw0yfqnxs6TXpz9d33+j/VJJpO88Ckf2/iU/70r++8CJcm1Sc4ZhuGE7L+DM5PkB08dH4bh7CRfS/LbYgeoNPlcPwDgiPRAkhOHYZh8WvQ8lOSkp/z7iUmmkjyS/Xd4Dmocx8eGYfhukndn/19Mvnp8yi3mYRjOyP67Sv9pHMd/qPk0APZzhwc4kH9O8nCSy4dhWDoMw6JhGN6U5H8k+egwDKcMw3BUks8k+foB7gQdzNeSvDfJbz/5v5MkwzC8OsnfJvmDcRxvqPxEABLBAxzAOI7TSf5Dkldk/9+1eTD778z8tyT/Pcmt2f/3dXYn+YNZTF+f5JVJNo7jeNdTPv6xJMcluXIYhh1P/uMvLQNl/KVlAKA9d3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBob3I2By9dunRcsWJF2cl37txZtpUkJ554YtnW1q1by7aSZOPGjaV7k5Oz+tY9oz179mTfvn1D2eAzqL6Gtm/fXraVJPPmzSvbeuELX1i2lSQPPfRQ6d7ixYvLtrZt25Zdu3bNyTWUJL/0S780rly5smxv/fr1ZVtJcvzxx5dtVT93HHPMMaV7lZ/rz372s2zatGlOrqMVK1aUXkObN28u20pqv0/btm0r20qS+fPnl+499thj1XubxnE87ukfn9Wr5ooVK/KRj3yk7EHdfvvtZVtJ8sUvfrFs68YbbyzbSpLLL7+8dO/YY48t27rrrrvKtg6l+hr6/ve/X7aVJMuXLy/bWr16ddlWknzyk58s3Xv1q19dtvWVr3ylbOtwrFy5Mtdcc03Z3sUXX1y2lSQf+tCHyrY++9nPlm0lyUUXXVS6V/m5nnnmmWVbh7Jy5cpcd911ZXtXXXVV2VaSvO997yvbqn49e9GLXlS6981vfrN07+qrrz7gOxi/0gIA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDam5zNwYsWLcqrXvWqspNff/31ZVtJ8q1vfats67LLLivbejb84z/+Y9nWmWeeWbZ1KNu3b8+tt95atveud72rbCtJduzYUbZ1zTXXlG0lyTiOpXsve9nLyrYWLlxYtnU4Hn300fzFX/xF2d6v/MqvlG0lyfnnn1+2dfbZZ5dtPRsqn8e3bt1atnUomzdvzle/+tWyvS1btpRtJckVV1xRtrVu3bqyraT+uejv//7vS/cOxh0eAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYmZ3PwC17wgrzlLW8pO/kVV1xRtpUkp556atnW0UcfXbaVJJ/85CdL984777yyrfvuu69s61BOOOGEXHLJJWV7Z599dtlWkrznPe8p2/rUpz5VtpUkF1xwQenePffcU7b1xBNPlG0djkWLFuW0004r2/vxj39ctpXU/ry/7GUvK9tK6p93P/jBD5ZtTU1NlW0dyszMTOl1++53v7tsK0luu+22sq2lS5eWbSXJpz/96dK9L3zhC6V7H//4xw/4cXd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQ3uRsDt66dWu+8Y1vlJ38wgsvLNtKkrvuuqts66Mf/WjZVpK8613vKt179NFHy7Z+9Vd/tWzrUPbt25eHHnqobO+iiy4q20qSV77ylWVbp5xyStlWkvzhH/5h6d7ERN37neuvv75s63DMmzcvS5YsKds7/fTTy7aS5I477ijb+p3f+Z2yrSR54IEHSveuvfbasq3HHnusbOtQ5s2bl2XLlpXtvfnNby7bSpI777yzbGvXrl1lW0nypS99qXSv+po8GHd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANqbnM3BwzBk0aJFZSe/6aabyraS5M1vfnPZ1pVXXlm2lSR//ud/Xrr3ta99rWxry5YtZVuHMjU1lc2bN5ftXXjhhWVbSfLNb36zbOv9739/2VaSvPa1ry3dO/XUU8u2Jibm9r3T448/nptvvrlsb9OmTWVbSfKhD32obOuqq64q20qSN73pTaV7n/jEJ8q23vjGN5ZtHcr8+fPz4he/uGzvgx/8YNlWktx1111lW9dcc03ZVpLcf//9pXvV1/jBuMMDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0N7kbA7etWtXfvSjH5Wd/EUvelHZVpJcccUVZVtnn3122VaS7Nmzp3Tv3e9+d9nWlVdeWbZ1KHv27MlPf/rTsr2//Mu/LNtKkksvvbRs66yzzirbSpK9e/eW7n3ve98r29q3b1/Z1uE45phjcuGFF5bt3XLLLWVbSXL33XeXba1atapsK0luv/320r3JyVm9jDyj7du3l20dytTUVB555JGyvbe97W1lW0nyzne+s2zrhz/8YdlWkmzZsqV07wMf+EDp3sGucXd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANobxnE8/IOH4dEk65+9h8Nz5KRxHI+bixO5htqas2socR015rmICge8jmYVPAAAz0d+pQUAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhvcjYHH3vssePJJ59cdvLdu3eXbSXJMAxlW9WPbWZmpnRvyZIlZVsbNmzIli1b6r54z2DBggXj4sWLy/Ze8YpXlG0lyaZNm8q2xnEs20qSrVu3lu4tWLCgbGvHjh3ZvXv3nFxDSTIMwzgxUfd+rfK5I6n/3leqfi6q/NqN45hxHOfkOqp+PXvooYfKtpJk27ZtZVu7du0q23qe2DSO43FP/+Csgufkk0/OmjVryh7R2rVry7aSZNGiRWVbd999d9lWUh9Qr3vd68q23vGOd5RtHcrixYvzxje+sWzvuuuuK9tKkiuvvLJsa2pqqmwrSW644YbSvZe+9KVlW9dff33Z1uGYmJjIUUcdVbZXHTz79u0r26oMu6T+xW/evHllW9U/M8+k+vXsj//4j8u2kuTb3/522dadd95ZtpXUB/2z8AZh/YE+6FdaAEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuTszl448aN+dznPld28s2bN5dtJcm+ffvKthYsWFC2lSSrVq0q3XvsscfKtqampsq2DmXJkiU544wzyvY++9nPlm0lyfT0dNnWpz71qbKtJDnvvPNK96644oqyrYmJuX3vNAxD6Tkff/zxsq0kOeGEE8q2Nm7cWLaVJIsWLSrdO+qoo8q2tmzZUrZ1KLt3787atWvL9u69996yraT+NehIVv38MTMzc+DzlJ4FAOAIJHgAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANDe5GwO3rlzZ26//fayk2/YsKFsK0kmJur6bWpqqmwrSf7lX/6ldO8DH/hA2dbevXvLtg5l6dKlOeuss8r2brjhhrKtJPn+979ftnX//feXbSXJLbfcUrr3J3/yJ2Vbf/M3f1O2dTjmzZuXo48+umxv+fLlZVtJ7c/U9PR02VaSLFu2rHTv+Wr79u2lP1PVz/H33Xdf6V6lytfaJBnHsXTvYNzhAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe5OzOXjv3r154IEHyk4+MzNTtnWk27RpU+ne5ZdfXra1cePGsq1D2bNnT+6///6yve9+97tlW0mycuXKsq2PfOQjZVtJ8ta3vrV076tf/WrZ1ubNm8u2Dscv//IvZ82aNWV7p556atlWkjz++ONlW0cffXTZVpJMT0+X7h1zzDFlW1u3bi3bOpSZmZns2rWrbG/dunVlW9XGcSzdq37tHoahdO9g3OEBANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaG9ytn9gHMeykw/DULaVJI899ljZ1vLly8u2kuTxxx8v3Vu2bFnp3lyZP39+jj/++LK9yy67rGwrSVatWlW29YY3vKFsK0muv/760r3t27eXbc3MzJRtHY77778/v/u7v1u29/DDD5dtJcmCBQvKtpYsWVK2ldR/r7Zt21a2NT09XbZ1KHv37s0DDzxQtlf9enYkm5iovVcyV88f7vAAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtDf5XJ58HMfSvWXLlpVtbdiwoWwrSR555JHSvdNOO61sa2ZmpmzrULZt25bvfOc7ZXtr1qwp20qS1atXl2194hOfKNtKkh07dpTuvfzlLy/bWrt2bdnW4dixY0d+8IMflO3NmzevbCtJFixYULa1e/fusq2k/nOtfB6vfk14Jtu3b8+tt95atjc9PV22lSTDMJRtzeVz/P+PiYnaey8H+3zd4QEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBobxjH8fAPHoZHk6x/9h4Oz5GTxnE8bi5O5Bpqa86uocR11JjnIioc8DqaVfAAADwf+ZUWANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO1NzubgYRjGZ+uB8Nwax3GYi/MsWrRoPOqooyr3yraSZPfu3aV7lU4++eTSvZ07d5ZtPfzww9m6deucXENJcuyxx46VX49xPHKf2qanp5/rh/CMhqHu2/7zn/88mzdvnpPr6Bfp9azye5Qc2T8vT9o0juNxT//grIIH/rWOOuqo/Pqv/3rZ3qmnnlq2lST33ntv2Vb1k8xXvvKV0r077rijbOu9731v2dbhOPnkk3P77beX7c3MzJRtVdu+fXvpXnVALViwoGzrLW95S9nW4aj8GZ2YqP2FSeU1uXDhwrKtJNm7d2/p3rPw87f+QB/0Ky0AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgvcnZ/oFhGMpOPo5j2RbPD1NTU9m8eXPZ3tVXX122lSRvf/vby7be9ra3lW0lyerVq0v3du/eXbZV+T09HOM4Znp6umzvkUceKdtKkieeeKJsa8GCBWVbSbJixYrSvYmJuvfNla8vc63y61BtZmbmiN6bK0fudwgAoIjgAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe5OzOXj+/Pk57rjjyk6+Y8eOsq1k/+OrUv3Y9u7dW7o3jmPp3lxZvnx5fvM3f7Nsb8GCBWVbSXLnnXeWbX3nO98p23o2HH/88WVbk5Ozeir5V9u2bVtuvPHGsr2pqamyrSR5/etfX7Z1wgknlG0lybx580r3Kg3DMKfnm5ioe88/PT1dtpXUfi2qv66VX7ek/vXsYHvu8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoL3J2Rw8f/78nHDCCWUnf/DBB8u2qq1atap0b3p6unRv0aJFZVv33HNP2dah7NmzJ+vWrSvbW7t2bdlWkvzkJz8p23rDG95QtpUkF198cene0qVLy7a+/vWvl20djgcffDCXXHJJ2d7q1avLtpLkxBNPLNuamPjFeV86DMOcnevlL395Pv/5z5ftXXXVVWVbSXLDDTeUbe3du7dsK6n/Pk1OzipFDmnfvn0H/Pgvzk8SAPALS/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKC9ydkcvHDhwrziFa8oO/mPf/zjsq0kmZqaKtt6zWteU7aVJG9/+9tL90455ZSyrdWrV5dtHcrExESWLFlStnffffeVbSXJ7//+75dt3X333WVbSfLTn/60dO+Vr3xl2dY4jmVbh2PPnj259957y/ZOO+20sq1k/3V+pDqSH9tcWr58ed7xjneU7VU/jw7DULpXaWZmpnRvrp4/XPkAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7U3O5uDFixfn9NNPLzv5X//1X5dtVbvllltK9yq/bkny4Q9/uGzrsssuK9s6lKmpqWzatKls7+Mf/3jZVpKsWbOmbOuss84q20qS6667rnTvxhtvLN2bS6997Wtz0003le2tWLGibCtJZmZmyrbmz59ftpUk4ziW7k1PT5dtVT+2Z7J27dqceeaZZXsPPvhg2VYyt1+L2aq+Jit/XpKDX5Pu8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0N4zjePgHD8OjSdY/ew+H58hJ4zgeNxcncg21NWfXUOI6asxzERUOeB3NKngAAJ6P/EoLAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQ3uRsDh6GYaw8+TAMlXNZunRp2daOHTvKtp4PxnGs/WYcxJIlS8bly5eX7U1M1Db7ONZd4pVbSbJnz57Svcqfly1btmTHjh1zcg0lyYoVK8aVK1fO1emYIxs2bMiWLVvm5Dqqfj3jiLJpHMfjnv7BWQVPUhspCxcuLNtKkte97nVlW7fddlvZFv/X8uXL83u/93tle9XX0PT0dNnW3r17y7aSZN26daV755xzTtnW5z//+bKtw7Fy5cp861vfmtNzzkZl7M7MzJRtJfVvEir91m/91pyer/JrUf0GvvL7Xv3mq/pzrTaO4/oDffzIvfIBAIoIHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7U3O9g+M41h28mOOOaZsK0luu+220r0j2cUXX1y2de2115ZtHcq+ffvy8MMPl+1deumlZVtJsm7durKtf/qnfyrbSpIPf/jDpXsf+9jHyra2bNlStnW4Kp+LZmZmyraOdJVft6T2a1f92J7JMAyZP39+2d7evXvLtpLk9NNPL9t65JFHyraS5NFHHy3dG4ahdO9g3OEBANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaG9yNgcPw5CFCxeWnXzDhg1lW0kyb968sq1xHMu2kuTcc88t3bvjjjvKtnbu3Fm2dSjbt2/P9773vbK9JUuWlG0lyapVq8q2br311rKtJHnNa15Turd06dKyrcqfvefCxETte7/K548j+bElyeLFi8u2qj/XZ7J48eLSn/fdu3eXbSXJ3XffXba1YMGCsq1kfwscyXsHu8bd4QEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuTszl4HMfs3bu37OS/9mu/VraVJDfddFPZ1pIlS8q2kuTv/u7vSveer1796ldnzZo1ZXvnn39+2VaSnHPOOWVbd955Z9lWsv9rV+m8884r27rnnnvKtg7Hjh078oMf/KBs7/Wvf33ZVpIsXry4bGtiovZ96bJly0r3FixYULY1OTmrl6R/lZUrV+byyy8v2/urv/qrsq0kWb9+fdnWrl27yraeDeM4zsl53OEBANoTPABAe4IHAGhP8AAA7QkeAKA9wQMAtCd4AID2BA8A0J7gAQDaEzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaG9yNgefcsop+cxnPlN28gsuuKBsK0le8pKXlG3t27evbCtJPve5z5XurVixomzrkksuKds6lO3bt+fmm28u2zvuuOPKtpLk29/+dtnWb/zGb5RtJcn5559furdmzZrSvbn0s5/9LBdddFHZ3r333lu2VW0cx9K9yueOJHniiSdK9+bKsmXLcu6555bt/dmf/VnZVpLMzMyUbQ3DULb1bKj8XJ+JOzwAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7U3O5uAdO3bk1ltvLTv5OI5lW0myYsWKsq277rqrbCtJzjjjjNK9c845p2zrC1/4QtnWoezcuTM/+tGPyvb+6I/+qGwrSf70T/+0bGvXrl1lW0ny/ve/v3Tvne98Z9nWF7/4xbKtw3HSSSfl0ksvLdubN29e2VaSTEzUvZecnJzV0/QhPfDAA6V7U1NTZVv79u0r2zqUTZs25ctf/nLZ3s0331y2lSS7d+8u3TuSDcNQunewtnCHBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7ggcAaE/wAADtCR4AoD3BAwC0J3gAgPYEDwDQnuABANoTPABAe4IHAGhP8AAA7QkeAKC9YRzHwz94GB5Nsv7Zezg8R04ax/G4uTiRa6itObuGEtdRY56LqHDA62hWwQMA8HzkV1oAQHuCBwBoT/AAAO0JHgCgPcEDALQneACA9gQPANCe4AEA2hM8AEB7/xshIRQbO2n5yQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(stop_line_estimator.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 2, 4, 'conv0', size=(10,5))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopLineEstimator(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Conv2d(8, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(4, 4, kernel_size=(6, 6), stride=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "stop_line_estimator.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "stop_line_estimator.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "stop_line_estimator.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(stop_line_estimator, dummy_input, onnx_model_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "stop_line_estimator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2681.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[0.05708778]]\n",
      "Predictions shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_model_path)\n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
