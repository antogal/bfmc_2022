{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/stop_line_estimator_advanced.pt'\n",
    "onnx_model_path = \"models/stop_line_estimator_advanced.onnx\"\n",
    "max_load = 250_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE\n",
    "\n",
    "# class StopLineEstimator(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             nn.Linear(in_features=4*4*4, out_features=32),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(in_features=32, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# stop_line_estimator = StopLineEstimator(out_dim=3,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class StopLineEstimator(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        prob = 0.2\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 4, kernel_size=5, stride=1), #out = 28\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=14\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(4, 8, kernel_size=5, stride=1), #out = 10\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=5\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(8, 64, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            #normalize\n",
    "            # nn.BatchNorm1d(3*3*4),\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=1*1*64, out_features=32),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Linear(in_features=32, out_features=16),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(in_features=32, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "stop_line_estimator = StopLineEstimator(out_dim=3,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "stop_line_estimator.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = stop_line_estimator(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from time import time, sleep\n",
    "\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "    #convert to gray\n",
    "    img = cv.resize(img, (4*SIZE[1], 4*SIZE[0]))\n",
    "\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "        axes_length = (randint(10//4, 50//4), randint(50//4, 300//4))\n",
    "        angle = randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (50,50))\n",
    "    noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = np.clip(light, 0, 51)\n",
    "    light *= 5\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 5), randint(1, 5)))\n",
    "\n",
    "    # cut the top third of the image, \n",
    "    img = img[int(img.shape[0]*(2/5)):,:] ################################# 2/5 frame[int(frame.shape[0]*(2/5)):,:]\n",
    "\n",
    "    #edges\n",
    "    img = cv.resize(img, (2*SIZE[1], 2*SIZE[0]))\n",
    "\n",
    "    # erosion and dilation\n",
    "    r = randint(0, 5)\n",
    "    if r == 0:\n",
    "        #dilate\n",
    "        kernel = np.ones((randint(1, 3), randint(1, 3)), np.uint8) #kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.dilate(img, kernel, iterations=1)\n",
    "    elif r == 1:\n",
    "        #erode\n",
    "        kernel = np.ones((randint(1, 3), randint(1, 3)), np.uint8) #kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.erode(img, kernel, iterations=1)\n",
    "\n",
    "    #edges    \n",
    "    img = cv.Canny(img, 100, 200)\n",
    "\n",
    "    #blur\n",
    "    img = cv.blur(img, (3,3))\n",
    "\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    # #get max brightness\n",
    "    # max_brightness = np.max(img)\n",
    "    # ratio = 255.0/max_brightness\n",
    "    # #normalize\n",
    "    # img = (img*ratio).astype(np.uint8)\n",
    "\n",
    "    # #add random tilt\n",
    "    # max_offset = 1\n",
    "    # offset = randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset, axis=0)\n",
    "    # if offset > 0:\n",
    "    #     img[:offset, :] = 0 #randint(0,255)\n",
    "    # elif offset < 0:\n",
    "    #     img[offset:, :] = 0 # randint(0,255)\n",
    "    \n",
    "    #add noise \n",
    "    std = 60\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(100)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "#TODO add negative examples: inside intersection, in normal road with high curvature\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            all_img_idx = 0\n",
    "            stopline_idx = 0\n",
    "            non_stopline_idx = 0\n",
    "            for i in tqdm(range(200, max_load)): #start from 200 since first imgs have wrong labels\n",
    "            # for i in range(max_load):\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                line = line.split(',')\n",
    "                #x stopline, y stopline, yaw stopline\n",
    "                label = np.array([float(line[4]), float(line[5]), float(line[6])], dtype=np.float32)\n",
    "                dist_label = float(line[4]) #dist is used only to filter images\n",
    "                \n",
    "                MAX_DIST = 9.0\n",
    "                #keep only small distanaces, avoid junctions\n",
    "                if dist_label < MAX_DIST: #and not junction_imgs_mask[i]:  #if  dist_label < 0.6 and not junction_imgs_mask[i]: \n",
    "                    # print(f'Sample {i},  idx = {all_img_idx},  dist = {dist_label}')\n",
    "                    # if dist_label < MIN_DIST:\n",
    "                    #     dist_label = MAX_DIST+MIN_DIST  - dist_label\n",
    "                    #img \n",
    "                    img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "                    img = load_and_augment_img(img)\n",
    "                    if all_img_idx < 1000:\n",
    "                        # cv.putText(img, f'{dist_label[0]:.2f}', (5,10), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                        # cv.putText(img, f'{np.rad2deg(angle_label[0]):.0f}', (5,25), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                        cv.imshow('img', img)\n",
    "                        # print(label)\n",
    "                        cv.waitKey(1)\n",
    "                        if all_img_idx == 999:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(label)\n",
    "                    \n",
    "                    all_img_idx += 1\n",
    "                    stopline_idx += 1\n",
    "                else:\n",
    "                    if non_stopline_idx < 0.5*stopline_idx and np.random.uniform() > 0.9:\n",
    "                        MAX_VISIBLE_DIST = 0.65\n",
    "                        label[0] = MAX_VISIBLE_DIST\n",
    "                        img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "                        img = load_and_augment_img(img)\n",
    "                        if all_img_idx < 1000:\n",
    "                            # cv.putText(img, f'{dist_label[0]:.2f}', (5,10), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                            # cv.putText(img, f'{np.rad2deg(angle_label[0]):.0f}', (5,25), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                            cv.imshow('img', img)\n",
    "                            # print(label)\n",
    "                            cv.waitKey(1)\n",
    "                            if all_img_idx == 999:\n",
    "                                cv.destroyAllWindows()\n",
    "                        #add a dimension to the image\n",
    "                        img = img[:, :,np.newaxis]\n",
    "                        self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                        self.data.append(label)\n",
    "                        \n",
    "                        all_img_idx += 1\n",
    "                        non_stopline_idx += 1\n",
    "\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "            self.data = np.array(self.data)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'stopline imgs: {stopline_idx}')\n",
    "            print(f'non stopline imgs: {non_stopline_idx}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249800/249800 [03:00<00:00, 1384.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all imgs: torch.Size([75118, 32, 32, 1])\n",
      "stopline imgs: 55822\n",
      "non stopline imgs: 19296\n",
      "data: (75118, 3)\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 1, 32, 32])\n",
      "torch.Size([8192, 3])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    pos_losses = []\n",
    "    angle_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        pos = output[:, 0:1]\n",
    "        angle = output[:, 2]\n",
    "\n",
    "        pos_label = regr_label[:, 0:1]\n",
    "        angle_label = regr_label[:, 2]\n",
    "\n",
    "        # Compute the losses\n",
    "        pos_loss = 1.0*regr_loss_fn(pos, pos_label)\n",
    "        angle_loss = 1.0*regr_loss_fn(angle, angle_label)\n",
    "    \n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = pos_loss + angle_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        pos_losses.append(pos_loss.detach().cpu().numpy())\n",
    "        angle_losses.append(angle_loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(pos_losses), np.mean(angle_losses)\n",
    "\n",
    "# VALIDATION FUNCTION\n",
    "def val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device=device):\n",
    "    stop_line_estimator.eval()\n",
    "    pos_losses = []\n",
    "    angle_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = stop_line_estimator(input)\n",
    "        regr_out = output\n",
    "        pos = regr_out[:, 0:1]\n",
    "        angle = regr_out[:, 2]\n",
    "        dist_label = regr_label[:, 0:1]\n",
    "        angle_label = regr_label[:, 2]\n",
    "        pos_loss = 1.0*regr_loss_fn(pos, dist_label)\n",
    "        angle_loss = 1.0*regr_loss_fn(angle, angle_label)\n",
    "        pos_losses.append(pos_loss.detach().cpu().numpy())\n",
    "        angle_losses.append(angle_loss.detach().cpu().numpy())\n",
    "    return np.mean(pos_losses), np.mean(angle_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  500/500 \n",
      "Pos loss: 0.0453 --- val pos loss: 0.0454\n",
      "angle loss: 2.5589 --- val angle loss: 2.6032\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.003 #0.005\n",
    "epochs = 500\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 0.1*9e-4 #0.001 \n",
    "L2_lambda = 0.1*1e-2 #2e-2\n",
    "optimizer = torch.optim.Adam(stop_line_estimator.parameters(), lr=lr, weight_decay=9e-5) #wd = 2e-3# 3e-5\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "best_val_loss = 100.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        pos_loss, angle_loss = train_epoch(stop_line_estimator, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_pos_loss, val_angle_loss = val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Epoch  {epoch+1}/{epochs} \\nPos loss: {pos_loss:.4f} --- val pos loss: {val_pos_loss:.4f}\")\n",
    "        print(f\"angle loss: {np.rad2deg(angle_loss):.4f} --- val angle loss: {np.rad2deg(val_angle_loss):.4f}\")\n",
    "        if val_pos_loss < best_val_loss:\n",
    "            best_val_loss = val_pos_loss\n",
    "            torch.save(stop_line_estimator.state_dict(), model_name)\n",
    "            print(f'Saved model ')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:00<00:00, 202.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val pos_loss: (0.042343784, 0.0493757)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "stop_line_estimator.load_state_dict(torch.load(model_name))\n",
    "print(f\"Val pos_loss: {val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 5, 5)\n",
      "(8, 4, 5, 5)\n",
      "(64, 8, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(stop_line_estimator.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 2, 4, 'conv0', size=(10,5))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopLineEstimator(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout(p=0.2, inplace=False)\n",
       "    (11): Conv2d(8, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "stop_line_estimator.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "stop_line_estimator.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "stop_line_estimator.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(stop_line_estimator, dummy_input, onnx_model_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "stop_line_estimator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 3873.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[4.8364908e-01 5.0435955e-40 8.0319792e-02]]\n",
      "Predictions shape: (1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_model_path)\n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
