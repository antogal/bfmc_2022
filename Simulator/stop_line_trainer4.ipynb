{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/stop_line_estimator.pt'\n",
    "onnx_model_path = \"models/stop_line_estimator.onnx\"\n",
    "max_load = 250_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE\n",
    "\n",
    "# class StopLineEstimator(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             nn.Linear(in_features=4*4*4, out_features=32),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(in_features=32, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# stop_line_estimator = StopLineEstimator(out_dim=3,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class StopLineEstimator(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        prob = 0.2\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 28\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=14\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(8, 8, kernel_size=5, stride=1), #out = 10\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=5\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(8, 256, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            #normalize\n",
    "            # nn.BatchNorm1d(3*3*4),\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=1*1*256, out_features=64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=64, out_features=32),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(in_features=32, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "stop_line_estimator = StopLineEstimator(out_dim=3,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "stop_line_estimator.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = stop_line_estimator(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from time import time, sleep\n",
    "\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "    #convert to gray\n",
    "    img = cv.resize(img, (4*SIZE[1], 4*SIZE[0]))\n",
    "\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # #create random ellipses to simulate light from the sun\n",
    "    # light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    # #add ellipses\n",
    "    # for j in range(2):\n",
    "    #     cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "    #     axes_length = (randint(10//4, 50//4), randint(50//4, 300//4))\n",
    "    #     angle = randint(0, 360)\n",
    "    #     light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    # #create an image of random white and black pixels\n",
    "    # light = cv.blur(light, (50,50))\n",
    "    # noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    # light = cv.subtract(light, noise)\n",
    "    # light = np.clip(light, 0, 51)\n",
    "    # light *= 5\n",
    "    # #add light to the image\n",
    "    # img = cv.add(img, light)\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 5), randint(1, 5)))\n",
    "\n",
    "    # cut the top third of the image, \n",
    "    img = img[int(img.shape[0]*(2/5)):,:] ################################# 2/5 frame[int(frame.shape[0]*(2/5)):,:]\n",
    "\n",
    "    #edges\n",
    "    img = cv.resize(img, (2*SIZE[1], 2*SIZE[0]))\n",
    "\n",
    "    # erosion and dilation\n",
    "    r = randint(0, 5)\n",
    "    if r == 0:\n",
    "        #dilate\n",
    "        kernel = np.ones((randint(1, 3), randint(1, 3)), np.uint8) #kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.dilate(img, kernel, iterations=1)\n",
    "    elif r == 1:\n",
    "        #erode\n",
    "        kernel = np.ones((randint(1, 3), randint(1, 3)), np.uint8) #kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.erode(img, kernel, iterations=1)\n",
    "\n",
    "    #edges    \n",
    "    img = cv.Canny(img, 100, 200)\n",
    "\n",
    "    #blur\n",
    "    img = cv.blur(img, (3,3))\n",
    "\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    # #get max brightness\n",
    "    # max_brightness = np.max(img)\n",
    "    # ratio = 255.0/max_brightness\n",
    "    # #normalize\n",
    "    # img = (img*ratio).astype(np.uint8)\n",
    "\n",
    "    # #add random tilt\n",
    "    # max_offset = 1\n",
    "    # offset = randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset, axis=0)\n",
    "    # if offset > 0:\n",
    "    #     img[:offset, :] = 0 #randint(0,255)\n",
    "    # elif offset < 0:\n",
    "    #     img[offset:, :] = 0 # randint(0,255)\n",
    "    \n",
    "    #add noise \n",
    "    std = 60\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(100)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "#TODO add negative examples: inside intersection, in normal road with high curvature\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        junction_images_indexes = []\n",
    "        tot_lines = 0\n",
    "        with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            tot_lines = len(lines)\n",
    "            for i,line in enumerate(lines):\n",
    "                if line == '1,0,0,0,0,0,0,1,0,0,0,0,0,0,0':\n",
    "                    junction_images_indexes.append(i)\n",
    "        junction_imgs_mask = np.zeros(tot_lines, dtype=bool)\n",
    "        junction_imgs_mask[junction_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(200, max_load)): #start from 200 since first imgs have wrong labels\n",
    "            # for i in range(max_load):\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                line = line.split(',')\n",
    "                #x stopline, y stopline, yaw stopline\n",
    "                label = np.array([float(line[4]), float(line[5]), float(line[6])], dtype=np.float32)\n",
    "                dist_label = float(line[3]) #dist is used only to filter images\n",
    "                \n",
    "                MAX_DIST = 0.85\n",
    "                MIN_DIST = 0.25\n",
    "                #keep only small distanaces, avoid junctions\n",
    "                if dist_label < MAX_DIST and not junction_imgs_mask[i]:  #if  dist_label < 0.6 and not junction_imgs_mask[i]: \n",
    "                    # print(f'Sample {i},  idx = {all_img_idx},  dist = {dist_label}')\n",
    "                    if dist_label < MIN_DIST:\n",
    "                        dist_label = MAX_DIST+MIN_DIST  - dist_label\n",
    "                    #img \n",
    "                    img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "                    img = load_and_augment_img(img)\n",
    "                    if all_img_idx < 1000:\n",
    "                        # cv.putText(img, f'{dist_label[0]:.2f}', (5,10), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                        # cv.putText(img, f'{np.rad2deg(angle_label[0]):.0f}', (5,25), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                        cv.imshow('img', img)\n",
    "                        # print(label)\n",
    "                        cv.waitKey(1)\n",
    "                        if all_img_idx == 999:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(label)\n",
    "                    \n",
    "                    all_img_idx += 1\n",
    "\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "            self.data = np.array(self.data)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59220/59220 [00:33<00:00, 1743.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all imgs: torch.Size([14670, 32, 32, 1])\n",
      "data: (14670, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 1, 32, 32])\n",
      "torch.Size([8192, 3])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    pos_losses = []\n",
    "    angle_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        pos = output[:, 0:1]\n",
    "        angle = output[:, 2]\n",
    "\n",
    "        pos_label = regr_label[:, 0:1]\n",
    "        angle_label = regr_label[:, 2]\n",
    "\n",
    "        # Compute the losses\n",
    "        pos_loss = 1.0*regr_loss_fn(pos, pos_label)\n",
    "        angle_loss = 1.0*regr_loss_fn(angle, angle_label)\n",
    "    \n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = pos_loss + angle_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        pos_losses.append(pos_loss.detach().cpu().numpy())\n",
    "        angle_losses.append(angle_loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(pos_losses), np.mean(angle_losses)\n",
    "\n",
    "# VALIDATION FUNCTION\n",
    "def val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device=device):\n",
    "    stop_line_estimator.eval()\n",
    "    pos_losses = []\n",
    "    angle_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = stop_line_estimator(input)\n",
    "        regr_out = output\n",
    "        pos = regr_out[:, 0:1]\n",
    "        angle = regr_out[:, 2]\n",
    "        dist_label = regr_label[:, 0:1]\n",
    "        angle_label = regr_label[:, 2]\n",
    "        pos_loss = 1.0*regr_loss_fn(pos, dist_label)\n",
    "        angle_loss = 1.0*regr_loss_fn(angle, angle_label)\n",
    "        pos_losses.append(pos_loss.detach().cpu().numpy())\n",
    "        angle_losses.append(angle_loss.detach().cpu().numpy())\n",
    "    return np.mean(pos_losses), np.mean(angle_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  250/250 \n",
      "Pos loss: 0.1569 --- val pos loss: 0.2143\n",
      "angle loss: 0.9301 --- val angle loss: 1.1298\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.003 #0.005\n",
    "epochs = 250\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 1*9e-4 #0.001 \n",
    "L2_lambda = 1*1e-2 #2e-2\n",
    "optimizer = torch.optim.Adam(stop_line_estimator.parameters(), lr=lr, weight_decay=9e-5) #wd = 2e-3# 3e-5\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "best_val_loss = 100.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        pos_loss, angle_loss = train_epoch(stop_line_estimator, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_pos_loss, val_angle_loss = val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Epoch  {epoch+1}/{epochs} \\nPos loss: {pos_loss:.4f} --- val pos loss: {val_pos_loss:.4f}\")\n",
    "        print(f\"angle loss: {np.rad2deg(angle_loss):.4f} --- val angle loss: {np.rad2deg(val_angle_loss):.4f}\")\n",
    "        if val_pos_loss < best_val_loss:\n",
    "            best_val_loss = val_pos_loss\n",
    "            torch.save(stop_line_estimator.state_dict(), model_name)\n",
    "            print(f'Saved model ')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 185.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val pos_loss: (0.1893854, 0.019533886)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "stop_line_estimator.load_state_dict(torch.load(model_name))\n",
    "print(f\"Val pos_loss: {val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 5, 5)\n",
      "(8, 8, 5, 5)\n",
      "(256, 8, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(stop_line_estimator.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 2, 4, 'conv0', size=(10,5))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopLineEstimator(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout(p=0.2, inplace=False)\n",
       "    (11): Conv2d(8, 256, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "stop_line_estimator.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "stop_line_estimator.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "stop_line_estimator.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(stop_line_estimator, dummy_input, onnx_model_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "stop_line_estimator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 5692.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[ 7.7156699e-01 -2.3583112e-12  6.6901577e-01]]\n",
      "Predictions shape: (1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_model_path)\n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
