{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/lane_keeper_small.pt'\n",
    "onnx_lane_keeper_path = \"models/lane_keeper_small.onnx\"\n",
    "max_load = 150_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE\n",
    "\n",
    "# class LaneKeeper(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 16, kernel_size=3, stride=1), #out = 30\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "#             nn.BatchNorm2d(16),\n",
    "#             nn.Conv2d(16, 8, kernel_size=5, stride=2), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             # nn.MaxPool2d(kernel_size=2, stride=2), #out=6\n",
    "#             # nn.BatchNorm2d(8),\n",
    "#             nn.Conv2d(8, 16, kernel_size=6, stride=1), #out = 1\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             nn.Linear(in_features=1*1*16, out_features=64),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(in_features=64, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# lane_keeper = LaneKeeper(out_dim=3,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class LaneKeeper(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "            # nn.BatchNorm2d(4),\n",
    "            nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=4*4*4, out_features=16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=16, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "lane_keeper = LaneKeeper(out_dim=3,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "lane_keeper.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = lane_keeper(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "    # img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "\n",
    "    #convert to gray\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (np.random.randint(0, img.shape[0]), np.random.randint(0, img.shape[1]))\n",
    "        axes_length = (np.random.randint(10, 50), np.random.randint(50, 300))\n",
    "        angle = np.random.randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (100,100))\n",
    "    noise = np.random.randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = 5 * light\n",
    "\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    # cv.imshow('light', light)\n",
    "    # if cv.waitKey(0) == ord('q'):\n",
    "    #     break\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (9,9))\n",
    "\n",
    "    # cut the top third of the image, let it 640x320\n",
    "    img = img[int(img.shape[0]/3):,:]\n",
    "    assert img.shape == (320,640), f'img shape cut = {img.shape}'\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = 5\n",
    "    offset = np.random.randint(-max_offset, max_offset)\n",
    "    img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        img[:offset, :] = np.random.randint(0,255)\n",
    "    elif offset < 0:\n",
    "        img[offset:, :] = np.random.randint(0,255)\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.1,1.2)\n",
    "    if np.random.uniform() > 5:\n",
    "        const = const*0.2\n",
    "    img = 127*(1-const) + img*const\n",
    "    img = img.astype(np.uint8)\n",
    "\n",
    "    #add noise \n",
    "    std = 150\n",
    "    std = np.random.randint(1, std)\n",
    "    noisem = np.random.randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = np.random.randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "    #blur \n",
    "    img = cv.blur(img, (np.random.randint(1,3),np.random.randint(1,3)))\n",
    "\n",
    "    #add random brightness\n",
    "    max_brightness = 50\n",
    "    brightness = np.random.randint(-max_brightness, max_brightness)\n",
    "    if brightness > 0:\n",
    "        img = cv.add(img, brightness)\n",
    "    elif brightness < 0:\n",
    "        img = cv.subtract(img, -brightness)\n",
    "    \n",
    "    # # invert color\n",
    "    # if np.random.uniform(0, 1) > 0.6:\n",
    "    #     img = cv.bitwise_not(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(100)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        #classification label for road ahead = 1,0,0,0,1,0,0,0,0,0,0,0,0,0,0\n",
    "        road_images_indexes = []\n",
    "        tot_lines = 0\n",
    "        with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            tot_lines = len(lines)\n",
    "            for i,line in enumerate(lines):\n",
    "                if line == '1,0,0,0,1,0,0,0,0,0,0,0,0,0,0':\n",
    "                    road_images_indexes.append(i)\n",
    "        print(f'total pure road images: {len(road_images_indexes)}')\n",
    "        road_imgs_mask = np.zeros(tot_lines, dtype=np.bool)\n",
    "        road_imgs_mask[road_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            # self.all_imgs = torch.zeros((2*max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8) #adding flipped img\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            # road images specifically are added again along with their flipped image and label\n",
    "            road_imgs = torch.zeros((2*len(road_images_indexes), SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "            road_labels = []\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "            road_idx = 0\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(max_load)):\n",
    "\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                sample = line.split(',')\n",
    "                #keep only info related to the lane, discard distance from stop line \n",
    "                sample = [sample[0], sample[1], sample[3]] #e2=lateral error, e3=yaw error point ahead, curvature\n",
    "                reg_label = np.array([float(s) for s in sample], dtype=np.float32)\n",
    "\n",
    "                #img \n",
    "                img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "\n",
    "                #check if its in the road images\n",
    "                if road_imgs_mask[i]:\n",
    "                    img_r = load_and_augment_img(img.copy())\n",
    "                    # cv.imshow('imgR', img_r)\n",
    "                    img_r = img_r[:,:,np.newaxis]\n",
    "\n",
    "                    img_l = cv.flip(img, 1)\n",
    "                    img_l = load_and_augment_img(img_l)\n",
    "                    # cv.imshow('imgL', img_l)\n",
    "                    img_l = img_l[:,:,np.newaxis]\n",
    "                    # cv.waitKey(1)\n",
    "\n",
    "                    road_imgs[2*road_idx] = torch.from_numpy(img_r)\n",
    "                    road_imgs[2*road_idx+1] = torch.from_numpy(img_l)\n",
    "                    road_labels.append(reg_label)\n",
    "                    road_labels.append(-reg_label)\n",
    "                    road_idx += 1\n",
    "\n",
    "                else:\n",
    "                    img = load_and_augment_img(img)\n",
    "                    if i < 100:\n",
    "                        cv.imshow('img', img)\n",
    "                        cv.waitKey(1)\n",
    "                        if i == 99:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(reg_label)\n",
    "                    all_img_idx += 1\n",
    "\n",
    "            #cut imgs to the right length\n",
    "            road_imgs = road_imgs[:2*road_idx]\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "\n",
    "            #concatenate all_imgs and road_imgs\n",
    "            print(f'road images: {road_imgs.shape}')\n",
    "            print(f'all images: {self.all_imgs.shape}')\n",
    "            self.all_imgs = torch.cat((self.all_imgs, road_imgs), dim=0)\n",
    "            print(f'self.data shape: {len(self.data)}')\n",
    "            print(f'road_labels shape = {len(road_labels)}')\n",
    "            self.data = np.concatenate((np.array(self.data), np.array(road_labels)), axis=0)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "            #free road_imgs from memory\n",
    "            del road_imgs\n",
    "            del road_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pure road images: 79427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142527/142527 [20:36<00:00, 115.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "road images: torch.Size([158854, 32, 32, 1])\n",
      "all images: torch.Size([63100, 32, 32, 1])\n",
      "self.data shape: 63100\n",
      "road_labels shape = 158854\n",
      "\n",
      "all imgs: torch.Size([221954, 32, 32, 1])\n",
      "data: (221954, 3)\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4096, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 1, 32, 32])\n",
      "torch.Size([4096, 3])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    err_losses2 = []\n",
    "    err_losses3 = []\n",
    "    curv_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        err2 = output[:, 0]\n",
    "        err3 = output[:, 1]\n",
    "        curv_out = output[:, 2]\n",
    "\n",
    "        err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 1]\n",
    "        curv_label = regr_label[:, 2]\n",
    "\n",
    "        # Compute the losses\n",
    "        err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = err_loss3 + err_loss2 + curv_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    err_loss2 = np.mean(err_losses2)\n",
    "    err_loss3 = np.mean(err_losses3)\n",
    "    curv_loss = np.mean(curv_losses)\n",
    "    return err_loss2, err_loss3, curv_loss\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device=device):\n",
    "    lane_keeper.eval()\n",
    "    err_losses3 = []\n",
    "    err_losses2 = []\n",
    "    curv_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = lane_keeper(input)\n",
    "\n",
    "        regr_out = output\n",
    "        err2 = regr_out[:, 0]\n",
    "        err3 = regr_out[:, 1]\n",
    "        curv_out = regr_out[:, 2]\n",
    "\n",
    "        err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 1]\n",
    "        curv_label = regr_label[:, 2]\n",
    "\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "        loss = err_loss3 + err_loss2 + curv_loss\n",
    "\n",
    "        err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "    return np.mean(err_losses2), np.mean(err_losses3), np.mean(curv_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4/12 -> yaw_err_loss3: 0.0178\n",
      "Validation loss e3: 0.0176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 17/49 [00:07<00:13,  2.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3266/1387648822.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# if True:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0merr_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_loss3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mval_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_curv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3266/383214972.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Loop over the training batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Move the input and target data to the selected device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3266/1181403174.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# img = img.float()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.001 #0.005\n",
    "epochs = 12\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 9e-4 #0.001 \n",
    "L2_lambda = 1e-2 #2e-2\n",
    "optimizer = torch.optim.Adam(lane_keeper.parameters(), lr=lr, weight_decay=9e-5) #wd = 2e-3# 3e-5\n",
    "\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        err_loss2, err_loss3, curv_loss = train_epoch(lane_keeper, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_loss2, val_loss3, val_curv_loss = val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs} -> yaw_err_loss3: {err_loss3:.4f}\\nValidation loss e3: {val_loss3:.4f}\")\n",
    "    # print(f\"lateral_err_loss2: {err_loss2}\")\n",
    "    # print(f\"curv_loss: {curv_loss}\")\n",
    "    torch.save(lane_keeper.state_dict(), model_name)\n",
    "\n",
    "#Note: sweet spot for training is around 0.016 -> 0.020, also note that training can get stuck, and loss can start improve randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [00:01<00:00, 114.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lateral_err2_loss: 0.0017075847135856748\n",
      "yaw_err3_loss: 0.017601899802684784\n",
      "curv_loss: 0.00017236857092939317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "lane_keeper.load_state_dict(torch.load(model_name))\n",
    "lane_keeper.eval()\n",
    "err_losses3 = []\n",
    "err_losses2 = []\n",
    "curv_losses = []\n",
    "for (input, regr_label) in tqdm(val_dataloader):\n",
    "    input, regr_label =input.to(device), regr_label.to(device)\n",
    "    output = lane_keeper(input)\n",
    "\n",
    "    regr_out = output\n",
    "    err2 = regr_out[:, 0]\n",
    "    err3 = regr_out[:, 1]\n",
    "    curv_out = regr_out[:, 2]\n",
    "\n",
    "    err2_label = regr_label[:, 0]\n",
    "    err3_label = regr_label[:, 1]\n",
    "    curv_label = regr_label[:, 2]\n",
    "\n",
    "    err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "    err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "    curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "    loss = err_loss3 + err_loss2 + curv_loss\n",
    "\n",
    "    err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "    err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "    curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "\n",
    "print(f\"lateral_err2_loss: {np.mean(err_losses2)}\")\n",
    "print(f\"yaw_err3_loss: {np.mean(err_losses3)}\")\n",
    "print(f\"curv_loss: {np.mean(curv_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 5, 5)\n",
      "(4, 8, 5, 5)\n",
      "(4, 4, 6, 6)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAAFECAYAAADIoV+oAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPHUlEQVR4nO3aXYim9X3G8es/M+vLvrgvcWm0mtjQWkIIxQQkja1BCklIyEmVaFNqJSCIsAQFD6SuqBBpElIwLw1CUkml1gY1pvYgFHKSlpQIQXKQBAq2bCxx1tG47q775uzcPdh9mkXGNlP9zbr+Ph8IxJlnr/89M88+8517dkzTFACALuZO9wUAAKwn8QMAtCJ+AIBWxA8A0Ir4AQBaET8AQCviBwBoRfwAbxpjjE+NMfaMMV4eYzw+xthxuq8JeOsRP8CbwhjjPUnuT/JnSX4jyaEkf31aLwp4SxI/wGsaY1w8xnhsjLE0xnhhjPGVMcbcGOOOk3donhtj/O0YY+vJx18yxpjGGH8+xvj5GOP5McZfnHzfhWOMw6fezRljXHbyMRuS/GmSJ6Zp+v40TQeT7E7yx2OMLafjYwfeusQPsKoxxnySf0qyJ8klSX4zycNJbjj5v6uSvCvJ5iRfedUf/4Mkv5vkj5LcOcZ49zRNv0jyb0muPuVxn0ryyDRNryR5T5Ifz94xTdPTSY4lufSN/ciA7sQP8FouT3JhktumaXp5mqYj0zT9a07cofmraZr+4+QdmtuTXDfGWDjlz949TdPhaZp+nBNB83sn3/5Qkj9JkjHGSHLdybclJyLqpVddw0tJ3PkB3lDiB3gtFyfZM03T8qvefmFO3A2a2ZNkISf+nc7M4in//1BOhE2SPJrk98cYFyS5MslKkn85+b6DSc571VnnJTnw//0AAFaz8H8/BGjqmSTvGGMsvCqAfpHknaf89zuSLCfZm+Si/21wmqYXxxj/nOTaJO9O8vA0TdPJd/8kv7pDlDHGu5KcneTfX+8HAnAqd36A1/JkkmeT/OUYY9MY45wxxhVJ/j7JLWOM3xpjbE5yb5J/WOUO0Wt5KMn1Sa7Jr37llSR/l+QTY4w/HGNsSnJPksemaXLnB3hDiR9gVdM0HU/yiSS/neTnSf4rJ+7Y/E2SB5N8P8l/JjmSZNcapv8xye8kWTz5b4Jm5/0kyU05EUHP5cS/9bn5dX8gAK8yfnXHGQDgrc+dHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaEX8AACtiB8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtCJ+AIBWxA8A0Ir4AQBaET8AQCviBwBoRfwAAK2IHwCgFfEDALSysJYHjzGmqgtJkp07d1bOJ0nGGKX701T6KSo/48CBAzly5EjZJ2njxo3Ttm3bquYzN1ff89Vf41/+8pel+0mydevW0v29e/c+P01T2V/oc889d6r8GKpfJ94qVlZWyrb379+fw4cPl30hNm/ePO3YsaNqPs8880zZ9nrZsGFD+Rlve9vbSvcXFxdXfS1aU/xUu+aaa8rPWFio/ZArXwxmjh07Vrb97W9/u2w7SbZt25Ybb7yxbH/z5s1l2zNHjhwp3f/Wt75Vup8kH/nIR0r3v/jFL+6p3N+6dWuuv/76sv31iOjqM9bjtejo0aNl2w899FDZdpLs2LEjt912W9n+rbfeWrY9s7y8XLp//vnnl+4nyac//enS/c9+9rOrvhb5tRcA0Ir4AQBaET8AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaEX8AACtiB8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArSys5cHnn39+rr766qpryZEjR8q2Z7Zt21a6v7y8XLqfJAsLa/qyrckYo2w7Sfbv35/vfe97ZfsPPPBA2fbMWWedVbp/+PDh0v0k2bdvX/kZ1Sqfq8eOHSvbnrniiitK95988snS/SSZpqn8jCqvvPJK9u7dW7Z/2223lW3PzM3V3r9YWVkp3U/qv+e8Fnd+AIBWxA8A0Ir4AQBaET8AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaEX8AACtiB8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtLKwlgcfP348+/btK7qUZG7uzG+xL33pS+VnvPe97y3bPnDgQNl2kmzZsiVXXnll2f6ll15atj1z1113le7fe++9pftJcuutt5afcSY7duxY+Rk//OEPS/e3bNlSup8ke/fuLds+fvx42fbMyspK2fbll19etj3z5JNPlp9RbZqm03LumV8bAABrIH4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtCJ+AIBWxA8A0Ir4AQBaET8AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaEX8AACtLKzpwQsL2bFjR9W1ZG6uvsU+/vGPl+4/+OCDpftJcsUVV5RtLy4ulm2vh1tuuaX8jKNHj5bu33XXXaX7bwVjjMzPz5ftr6yslG3PLC8vl+7v3r27dD9J7r777rLts846q2w7OfH3+Omnny7bX4/vZxs2bDij95Pk0KFD5Wesxp0fAKAV8QMAtCJ+AIBWxA8A0Ir4AQBaET8AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaEX8AACtiB8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArSys5cHTNGVlZaXqWjI3V99ijz/+eOn+yy+/XLqfJJs2bSrbHmOUbSfJxo0bc9lll5XtP/XUU2Xb6+X+++8vP+PGG28sP6PaNE1l2x/60IfKtmeuu+660v1HHnmkdD9J7rnnnrLts88+u2w7SQ4ePJgf/OAHZftLS0tl2zO7d+8u3b/qqqtK95PkjjvuKD9jNe78AACtiB8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtCJ+AIBWxA8A0Ir4AQBaET8AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEArY5qmX//BYywl2VN3ObwJvHOapp1V455DbXge8Xp5DvFGWPV5tKb4AQA40/m1FwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtCJ+AIBWxA8A0Ir4AQBaET8AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaEX8AACtiB8AoBXxAwC0In4AgFbEDwDQysJaHrxp06Zp+/btVdeyLp599tnS/QsuuKB0P0lefPHFsu2jR49meXl5VO2fe+6505YtW6rms7y8XLY9MzdX+zND9f56WFpaen6app1V+xs3bpy2bt1aNZ/jx4+Xba+XCy+8sPyM5557rmx73759OXTo0Bn7WrQez6H5+fnS/THKPv3/4+KLLy7d/9GPfrTqa9Ga4mf79u35zGc+88Zd1ausrKyUbc/ce++9pfu7du0q3U+SRx55pGz7pz/9adl2kmzZsiXXXHNN2f7S0lLZ9kzlC2aSnHPOOaX76+FrX/vansr9rVu35oYbbijbP3DgQNn2TPU3x3vuuad0P0m+/OUvl21//etfL9tO6l+LDh48WLY9c95555XuV8dVktx3332l+2OMVV+LzvwfMQEA1kD8AACtiB8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtCJ+AIBWxA8A0Ir4AQBaET8AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVhbW+gdWVlYqriNJMj8/X7Y989JLL5Xuf+ELXyjdT5JPfvKTZdv33Xdf2fZM5dd5586dZdszy8vL5WdUm5s783/umaapbPuxxx4r257ZtWtX6f6DDz5Yup8kR48eLduu/F4zU/latGnTprLtma9+9aul+9XP0SR56qmnys9YzZn/CggAsAbiBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaEX8AACtiB8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtCJ+AIBWxA8A0Ir4AQBaWTjdF3Cq48ePl5/x+c9/vnT/2muvLd1PkksuuaT8jDPV/Pz86b6EM8L+/ftP9yW8LouLi/nc5z5Xtn/nnXeWbc/cfvvtpfu7d+8u3U+SpaWlsu3l5eWy7Zlpmsq2N2zYULY988EPfrB0f2VlpXQ/SZ544onyM1bjzg8A0Ir4AQBaET8AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaEX8AACtiB8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtLJwui/gVK+88kr5GWeffXbp/ne+853S/SRZXFws2/7whz9ctp0kY4yMMcr2P/rRj5Ztz/zsZz8r3X/ggQdK95PkAx/4QPkZld7+9rfnhhtuKNvfvn172fbM3XffXbr//PPPl+4nyebNm8u25+fny7bXwzRN5We8//3vL91fj4/h0UcfLT9jNe78AACtiB8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtCJ+AIBWxA8A0Ir4AQBaET8AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANCK+AEAWhE/AEAr4gcAaGXhdF/AqR5++OHyMw4ePFi6v2vXrtL9JPnmN79Ztv3CCy+UbSfJNE1ZXl4u2//Yxz5Wtj3z3e9+t3T/fe97X+l+knzjG98oP6PaGKNse26u/ufCQ4cOle4/++yzpftJsm/fvrLt6s9PkqysrJRtLy4ulm3PXHTRRaX76/H34Oabby7dv+mmm1Z9uzs/AEAr4gcAaEX8AACtiB8AoBXxAwC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtCJ+AIBWxA8A0Ir4AQBaET8AQCviBwBoRfwAAK2IHwCgFfEDALQifgCAVsQPANDKmKbp13/wGEtJ9tRdDm8C75ymaWfVuOdQG55HvF6eQ7wRVn0erSl+AADOdH7tBQC0In4AgFbEDwDQivgBAFoRPwBAK+IHAGhF/AAArYgfAKAV8QMAtPLftsqda3ff0HcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 720x360 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(lane_keeper.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 2, 4, 'conv0', size=(10,5))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LaneKeeper(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Conv2d(8, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(4, 4, kernel_size=(6, 6), stride=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=16, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "lane_keeper.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "lane_keeper.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "lane_keeper.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(lane_keeper, dummy_input, onnx_lane_keeper_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lane_keeper.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2358.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[-0.00819606  0.05410811  0.003503  ]]\n",
      "Predictions shape: (1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_lane_keeper_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
