{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/stop_line_estimator.pt'\n",
    "onnx_model_path = \"models/stop_line_estimator.onnx\"\n",
    "max_load = 250_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE\n",
    "\n",
    "# class StopLineEstimator(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             nn.Linear(in_features=4*4*4, out_features=32),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(in_features=32, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# stop_line_estimator = StopLineEstimator(out_dim=1,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class StopLineEstimator(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 4, kernel_size=5, stride=1), #out = 30\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Conv2d(4, 4, kernel_size=5, stride=1), #out = 12\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "            # nn.BatchNorm2d(4),\n",
    "            nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=4*4*4, out_features=16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=16, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "stop_line_estimator = StopLineEstimator(out_dim=1,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "stop_line_estimator.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = stop_line_estimator(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from time import time, sleep\n",
    "\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "    #convert to gray\n",
    "    img = cv.resize(img, (4*SIZE[1], 4*SIZE[0]))\n",
    "\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "        axes_length = (randint(10//4, 50//4), randint(50//4, 300//4))\n",
    "        angle = randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (50,50))\n",
    "    noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = np.clip(light, 0, 51)\n",
    "    light *= 5\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 5), randint(1, 5)))\n",
    "\n",
    "    # cut the top third of the image, \n",
    "    img = img[int(img.shape[0]*(2/5)):,:] ################################# 2/5 frame[int(frame.shape[0]*(2/5)):,:]\n",
    "\n",
    "    #edges\n",
    "    img = cv.resize(img, (2*SIZE[1], 2*SIZE[0]))\n",
    "\n",
    "    # erosion and dilation\n",
    "    r = randint(0, 5)\n",
    "    if r == 0:\n",
    "        #dilate\n",
    "        kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.dilate(img, kernel, iterations=1)\n",
    "    elif r == 1:\n",
    "        #erode\n",
    "        kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.erode(img, kernel, iterations=1)\n",
    "\n",
    "    #edges    \n",
    "    img = cv.Canny(img, 100, 200)\n",
    "\n",
    "    #blur\n",
    "    img = cv.blur(img, (3,3))\n",
    "\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    # #get max brightness\n",
    "    # max_brightness = np.max(img)\n",
    "    # ratio = 255.0/max_brightness\n",
    "    # #normalize\n",
    "    # img = (img*ratio).astype(np.uint8)\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = 1\n",
    "    offset = randint(-max_offset, max_offset)\n",
    "    img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        img[:offset, :] = 0 #randint(0,255)\n",
    "    elif offset < 0:\n",
    "        img[offset:, :] = 0 # randint(0,255)\n",
    "    \n",
    "    #add noise \n",
    "    std = 60\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(100)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "#TODO add negative examples: inside intersection, in normal road with high curvature\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        junction_images_indexes = []\n",
    "        tot_lines = 0\n",
    "        with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            tot_lines = len(lines)\n",
    "            for i,line in enumerate(lines):\n",
    "                if line == '1,0,0,0,0,0,0,1,0,0,0,0,0,0,0':\n",
    "                    junction_images_indexes.append(i)\n",
    "        junction_imgs_mask = np.zeros(tot_lines, dtype=np.bool)\n",
    "        junction_imgs_mask[junction_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(200, max_load)): #start from 200 since first imgs have wrong labels\n",
    "            # for i in range(max_load):\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                dist = line.split(',')\n",
    "                #distance is the third element of the label data\n",
    "                dist_label = np.array([float(dist[2])], dtype=np.float32)\n",
    "\n",
    "                #keep only small distanaces, avoid junctions\n",
    "                if dist_label < 0.6 and not junction_imgs_mask[i]: \n",
    "                    # print(f'Sample {i},  idx = {all_img_idx},  dist = {dist_label}')\n",
    "                    if dist_label < -0.01:\n",
    "                        dist_label = 0.6 - dist_label\n",
    "                    #img \n",
    "                    img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "                    img = load_and_augment_img(img)\n",
    "                    if i < 100:\n",
    "                        cv.imshow('img', img)\n",
    "                        cv.waitKey(1)\n",
    "                        if i == 99:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(dist_label)\n",
    "                    all_img_idx += 1\n",
    "\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "            self.data = np.array(self.data)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irong/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/ipykernel_launcher.py:19: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "100%|██████████| 85307/85307 [01:04<00:00, 1332.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all imgs: torch.Size([13513, 32, 32, 1])\n",
      "data: (13513, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 1, 32, 32])\n",
      "torch.Size([8192, 1])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    dist_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        dist = output[:, 0]\n",
    "\n",
    "        dist_label = regr_label[:, 0]\n",
    "\n",
    "        # Compute the losses\n",
    "        dist_loss = 1.0*regr_loss_fn(dist, dist_label)\n",
    "    \n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = dist_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        dist_losses.append(dist_loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(dist_losses)\n",
    "\n",
    "# VALIDATION FUNCTION\n",
    "def val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device=device):\n",
    "    stop_line_estimator.eval()\n",
    "    dist_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = stop_line_estimator(input)\n",
    "        regr_out = output\n",
    "        dist = regr_out[:, 0]\n",
    "        dist_label = regr_label[:, 0]\n",
    "        dist_loss = 1.0*regr_loss_fn(dist, dist_label)\n",
    "        dist_losses.append(dist_loss.detach().cpu().numpy())\n",
    "    return np.mean(dist_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  143/250 -> dist loss: 0.0282\n",
      "Validation dist loss: 0.0310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_83792/2528543732.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mdist_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_line_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mval_dist_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_line_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_83792/447215019.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m#batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mdist_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.001 #0.005\n",
    "epochs = 250\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 9e-4 #0.001 \n",
    "L2_lambda = 1e-2 #2e-2\n",
    "optimizer = torch.optim.Adam(stop_line_estimator.parameters(), lr=lr, weight_decay=9e-5) #wd = 2e-3# 3e-5\n",
    "\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "best_val = 100.\n",
    "for epoch in range(epochs):\n",
    "    # try:\n",
    "    if True:\n",
    "        dist_loss = train_epoch(stop_line_estimator, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_dist_loss = val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs} -> dist loss: {dist_loss:.4f}\\nValidation dist loss: {val_dist_loss:.4f}\")\n",
    "    # print(f\"lateral_dist_loss: {dist_loss}\")\n",
    "    # print(f\"curv_loss: {curv_loss}\")\n",
    "    if val_dist_loss < best_val:\n",
    "        best_val = val_dist_loss\n",
    "        print(f\"New best model saved\")\n",
    "        torch.save(stop_line_estimator.state_dict(), model_name)\n",
    "\n",
    "#Note: sweet spot for training is around 0.016 -> 0.020, also note that training can get stuck, and loss can start improve randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 210.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val dist_loss: 0.03098592348396778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "stop_line_estimator.load_state_dict(torch.load(model_name))\n",
    "print(f\"Val dist_loss: {val_epoch(stop_line_estimator, val_dataloader, regr_loss_fn, device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 5, 5)\n",
      "(4, 4, 5, 5)\n",
      "(4, 4, 6, 6)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(stop_line_estimator.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 2, 4, 'conv0', size=(10,5))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopLineEstimator(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(4, 4, kernel_size=(6, 6), stride=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "stop_line_estimator.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "stop_line_estimator.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "stop_line_estimator.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(stop_line_estimator, dummy_input, onnx_model_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "stop_line_estimator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 9100.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[0.37375474]]\n",
      "Predictions shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_model_path)\n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
