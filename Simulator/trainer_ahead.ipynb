{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/lane_keeper_ahead.pt'\n",
    "onnx_lane_keeper_path = \"models/lane_keeper_ahead.onnx\"\n",
    "max_load = 500_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE\n",
    "# #very good\n",
    "# class LaneKeeperAhead(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             nn.Linear(in_features=4*4*4, out_features=16),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(in_features=16, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# lane_keeper_ahead = LaneKeeperAhead(out_dim=2,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class LaneKeeperAhead(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        prob = 0.3\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 28\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=14\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(8, 8, kernel_size=5, stride=1), #out = 10\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=5\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(8, 64, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            #normalize\n",
    "            # nn.BatchNorm1d(3*3*4),\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=1*1*64, out_features=32),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(in_features=32, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "lane_keeper_ahead = LaneKeeperAhead(out_dim=1,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "lane_keeper_ahead.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = lane_keeper_ahead(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from time import time, sleep\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "    #convert to gray\n",
    "    img = cv.resize(img, (4*SIZE[1], 4*SIZE[0]))\n",
    "\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "        axes_length = (randint(10//4, 50//4), randint(50//4, 300//4))\n",
    "        angle = randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (50,50))\n",
    "    noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = np.clip(light, 0, 51)\n",
    "    light *= 5\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    # cv.imshow('light', light)\n",
    "    # if cv.waitKey(0) == ord('q'):\n",
    "    #     break\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 5), randint(1, 5)))\n",
    "\n",
    "    # cut the top third of the image, let it 640x320\n",
    "    img = img[int(img.shape[0]/3):,:] ################################# /3\n",
    "    # assert img.shape == (320,640), f'img shape cut = {img.shape}'\n",
    "\n",
    "    #edges\n",
    "    img = cv.resize(img, (2*SIZE[1], 2*SIZE[0]))\n",
    "\n",
    "    r = randint(0, 5)\n",
    "    if r == 0:\n",
    "        #dilate\n",
    "        kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.dilate(img, kernel, iterations=1)\n",
    "    elif r == 1:\n",
    "        #erode\n",
    "        kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.erode(img, kernel, iterations=1)\n",
    "\n",
    "\n",
    "    #edges    \n",
    "    img = cv.Canny(img, 100, 200)\n",
    "\n",
    "    #blur\n",
    "    img = cv.blur(img, (3,3))\n",
    "\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    # #get max brightness\n",
    "    # max_brightness = np.max(img)\n",
    "    # ratio = 255.0/max_brightness\n",
    "    # #normalize\n",
    "    # img = (img*ratio).astype(np.uint8)\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = 3\n",
    "    offset = randint(-max_offset, max_offset)\n",
    "    img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        img[:offset, :] = 0 #randint(0,255)\n",
    "    elif offset < 0:\n",
    "        img[offset:, :] = 0 # randint(0,255)\n",
    "    \n",
    "    # #add salt and pepper noise\n",
    "    # sp_noise = randint(0, 4, size=img.shape, dtype=np.uint8)\n",
    "    # sp_noise = np.where(sp_noise == 0, np.zeros_like(img), 255*np.ones_like(img))\n",
    "    # # img = cv.bitwise_xor(img, sp_noise)\n",
    "\n",
    "\n",
    "    # #reduce contrast\n",
    "    # const = np.random.uniform(0.1,0.8)\n",
    "    # # if np.random.uniform() > .5:\n",
    "    # #     const = const*0.2\n",
    "    # img = 127*(1-const) + img*const\n",
    "    # img = img.astype(np.uint8)\n",
    "\n",
    "    #add noise \n",
    "    std = 80\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "    # #add random brightness\n",
    "    # max_brightness = 60\n",
    "    # brightness = randint(-max_brightness, max_brightness)\n",
    "    # if brightness > 0:\n",
    "    #     img = cv.add(img, brightness)\n",
    "    # elif brightness < 0:\n",
    "    #     img = cv.subtract(img, -brightness)\n",
    "\n",
    "    # #blur \n",
    "    # img = cv.blur(img, (randint(1,3),randint(1,3)))\n",
    "\n",
    "    # # invert color\n",
    "    # if np.random.uniform(0, 1) > 0.6:\n",
    "    #     img = cv.bitwise_not(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(5000):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(100)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        #classification label for road ahead = 1,0,0,0,1,0,0,0,0,0,0,0,0,0,0\n",
    "        road_images_indexes = []\n",
    "        tot_lines = 0\n",
    "        with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            tot_lines = len(lines)\n",
    "            for i,line in enumerate(lines):\n",
    "                if line == '1,0,0,0,1,0,0,0,0,0,0,0,0,0,0':\n",
    "                    road_images_indexes.append(i)\n",
    "        print(f'total pure road images: {len(road_images_indexes)}')\n",
    "        road_imgs_mask = np.zeros(tot_lines, dtype=bool)\n",
    "        road_imgs_mask[road_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            # self.all_imgs = torch.zeros((2*max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8) #adding flipped img\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            # road images specifically are added again along with their flipped image and label\n",
    "            road_imgs = torch.zeros((2*len(road_images_indexes), SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "            road_labels = []\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "            road_idx = 0\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(max_load)):\n",
    "\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                sample = line.split(',')\n",
    "                #keep only info related to the lane, discard distance from stop line \n",
    "                # sample = [sample[0], sample[1], sample[2], sample[3], sample[4]] #e2=lateral error, e3=yaw error point ahead, curvature, dist stopline, angle stopline\n",
    "                sample = [sample[1]]\n",
    "                reg_label = np.array([float(s) for s in sample], dtype=np.float32)\n",
    "\n",
    "                #img \n",
    "                img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "\n",
    "                #check if its in the road images\n",
    "                if road_imgs_mask[i]:\n",
    "                    img_r = load_and_augment_img(img.copy())\n",
    "                    # cv.imshow('imgR', img_r)\n",
    "                    img_r = img_r[:,:,np.newaxis]\n",
    "\n",
    "                    img_l = cv.flip(img, 1)\n",
    "                    img_l = load_and_augment_img(img_l)\n",
    "                    # cv.imshow('imgL', img_l)\n",
    "                    img_l = img_l[:,:,np.newaxis]\n",
    "                    # cv.waitKey(1)\n",
    "\n",
    "                    road_imgs[2*road_idx] = torch.from_numpy(img_r)\n",
    "                    road_imgs[2*road_idx+1] = torch.from_numpy(img_l)\n",
    "                    road_labels.append(reg_label)\n",
    "                    road_labels.append(-reg_label)\n",
    "                    road_idx += 1\n",
    "\n",
    "                else:\n",
    "                    img = load_and_augment_img(img)\n",
    "                    # cv.putText(img, f'{np.rad2deg(reg_label[0]):.1f}', (5,25), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                    MAX_SHOW = 1000\n",
    "                    max_show = MAX_SHOW\n",
    "                    if i < max_show:\n",
    "                        cv.imshow('img', img)\n",
    "                        key = cv.waitKey(1)\n",
    "                        if i == max_show-1:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(reg_label)\n",
    "                    all_img_idx += 1\n",
    "\n",
    "            #cut imgs to the right length\n",
    "            road_imgs = road_imgs[:2*road_idx]\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "\n",
    "            #concatenate all_imgs and road_imgs\n",
    "            print(f'road images: {road_imgs.shape}')\n",
    "            print(f'all images: {self.all_imgs.shape}')\n",
    "            self.all_imgs = torch.cat((self.all_imgs, road_imgs), dim=0)\n",
    "            print(f'self.data shape: {len(self.data)}')\n",
    "            print(f'road_labels shape = {len(road_labels)}')\n",
    "            self.data = np.concatenate((np.array(self.data), np.array(road_labels)), axis=0)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "            #free road_imgs from memory\n",
    "            del road_imgs\n",
    "            del road_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pure road images: 104461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250910/250910 [11:39<00:00, 358.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "road images: torch.Size([208922, 32, 32, 1])\n",
      "all images: torch.Size([146449, 32, 32, 1])\n",
      "self.data shape: 146449\n",
      "road_labels shape = 208922\n",
      "\n",
      "all imgs: torch.Size([355371, 32, 32, 1])\n",
      "data: (355371, 1)\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 1, 32, 32])\n",
      "torch.Size([8192, 1])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    # err_losses2 = []\n",
    "    err_losses3 = []\n",
    "    # curv_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        # err2 = output[:, 0]\n",
    "        err3 = output[:, 0]\n",
    "        # curv_out = output[:, 2]\n",
    "\n",
    "        # err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 0]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        # Compute the losses\n",
    "        # err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        # curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = err_loss3 + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        # err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    # err_loss2 = np.mean(err_losses2)\n",
    "    err_loss3 = np.mean(err_losses3)\n",
    "    # curv_loss = np.mean(curv_losses)\n",
    "    return err_loss3\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(lane_keeper_ahead, val_dataloader, regr_loss_fn, device=device):\n",
    "    lane_keeper_ahead.eval()\n",
    "    err_losses3 = []\n",
    "    # err_losses2 = []\n",
    "    # curv_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = lane_keeper_ahead(input)\n",
    "\n",
    "        regr_out = output\n",
    "        # err2 = regr_out[:, 0]\n",
    "        err3 = regr_out[:, 0]\n",
    "        # curv_out = regr_out[:, 2]\n",
    "\n",
    "        # err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 0]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "\n",
    "        # err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "    return np.mean(err_losses3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  75/500,  loss = MSELoss() \n",
      "yaw_err_loss3: 0.0334,   Val: 0.0344, best_val: 0.0335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [00:02<00:22,  1.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_538615/1041829495.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# if True:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mregr_loss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr_loss_fn1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mregr_loss_fn2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0merr_loss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper_ahead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mval_loss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper_ahead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_538615/42942214.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Loop over the training batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Move the input and target data to the selected device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.003 #0.005\n",
    "epochs = 500\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 0.5*1e-4 #9e-4\n",
    "L2_lambda = 0.5*1e-2 #1e-2\n",
    "optimizer = torch.optim.Adam(lane_keeper_ahead.parameters(), lr=lr, weight_decay=0.5*9e-5) #wd = 2e-3# 3e-5\n",
    "regr_loss_fn1 = nn.MSELoss() #before epochs/2\n",
    "regr_loss_fn2 = nn.MSELoss() #after epochs/2 for finetuning\n",
    "\n",
    "best_val = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        regr_loss_fn = regr_loss_fn1 if epoch < epochs//2 else regr_loss_fn2\n",
    "        err_loss3 = train_epoch(lane_keeper_ahead, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_loss3 = val_epoch(lane_keeper_ahead, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    if val_loss3 < best_val:\n",
    "        best_val = val_loss3\n",
    "        torch.save(lane_keeper_ahead.state_dict(), model_name)\n",
    "        print(\"model saved\")\n",
    "    \n",
    "    print(f\"Epoch  {epoch+1}/{epochs},  loss = {regr_loss_fn} \\nyaw_err_loss3: {err_loss3:.4f},   Val: {val_loss3:.4f}, best_val: {best_val:.4f}\")\n",
    "    # print(f\"lat_err_loss2: {err_loss2:.4f},   Val: {val_loss2:.4f}\")\n",
    "    # print(f\"curv_loss: {curv_loss}\")\n",
    "\n",
    "#Note: sweet spot for training is around 0.016 -> 0.020, also note that training can get stuck, and loss can start improving randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 356/356 [00:01<00:00, 213.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yaw_err3_loss: 0.03357258811593056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "lane_keeper_ahead.load_state_dict(torch.load(model_name))\n",
    "err_loss3 = val_epoch(lane_keeper_ahead, val_dataloader, regr_loss_fn, device)\n",
    "\n",
    "# print(f\"lateral_err2_loss: {err_loss2}\")\n",
    "print(f\"yaw_err3_loss: {err_loss3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 5, 5)\n",
      "(8, 8, 5, 5)\n",
      "(64, 8, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x144 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(lane_keeper_ahead.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 1, 4, 'conv0', size=(8,2))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 4, 4, 'conv1', size=(5,5)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 8, 8, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LaneKeeperAhead(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.3, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout(p=0.3, inplace=False)\n",
       "    (11): Conv2d(8, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "lane_keeper_ahead.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "lane_keeper_ahead.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "lane_keeper_ahead.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(lane_keeper_ahead, dummy_input, onnx_lane_keeper_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lane_keeper_ahead.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 5015.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[-0.08142425]]\n",
      "Predictions shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_lane_keeper_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
