{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/lane_keeper_ahead.pt'\n",
    "onnx_lane_keeper_path = \"models/lane_keeper_ahead.onnx\"\n",
    "max_load = 100_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE\n",
    "# #very good\n",
    "# class LaneKeeperAhead(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             nn.Linear(in_features=4*4*4, out_features=16),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(in_features=16, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# lane_keeper_ahead = LaneKeeperAhead(out_dim=2,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class LaneKeeperAhead(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        prob = 0.3\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 28\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=14\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(8, 8, kernel_size=5, stride=1), #out = 10\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=5\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(8, 64, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            #normalize\n",
    "            # nn.BatchNorm1d(3*3*4),\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=1*1*64, out_features=32),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(in_features=32, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "lane_keeper_ahead = LaneKeeperAhead(out_dim=1,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "lane_keeper_ahead.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = lane_keeper_ahead(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from time import time, sleep\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "    #convert to gray\n",
    "    img = cv.resize(img, (4*SIZE[1], 4*SIZE[0]))\n",
    "\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "        axes_length = (randint(10//4, 50//4), randint(50//4, 300//4))\n",
    "        angle = randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (50,50))\n",
    "    noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = np.clip(light, 0, 51)\n",
    "    light *= 5\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    # cv.imshow('light', light)\n",
    "    # if cv.waitKey(0) == ord('q'):\n",
    "    #     break\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 5), randint(1, 5)))\n",
    "\n",
    "    # cut the top third of the image, let it 640x320\n",
    "    img = img[int(img.shape[0]/3):,:] ################################# /3\n",
    "    # assert img.shape == (320,640), f'img shape cut = {img.shape}'\n",
    "\n",
    "    #edges\n",
    "    img = cv.resize(img, (2*SIZE[1], 2*SIZE[0]))\n",
    "\n",
    "    r = randint(0, 5)\n",
    "    if r == 0:\n",
    "        #dilate\n",
    "        kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.dilate(img, kernel, iterations=1)\n",
    "    elif r == 1:\n",
    "        #erode\n",
    "        kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.erode(img, kernel, iterations=1)\n",
    "\n",
    "\n",
    "    #edges    \n",
    "    img = cv.Canny(img, 100, 200)\n",
    "\n",
    "    #blur\n",
    "    img = cv.blur(img, (3,3))\n",
    "\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    # #get max brightness\n",
    "    # max_brightness = np.max(img)\n",
    "    # ratio = 255.0/max_brightness\n",
    "    # #normalize\n",
    "    # img = (img*ratio).astype(np.uint8)\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = 3\n",
    "    offset = randint(-max_offset, max_offset)\n",
    "    img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        img[:offset, :] = 0 #randint(0,255)\n",
    "    elif offset < 0:\n",
    "        img[offset:, :] = 0 # randint(0,255)\n",
    "    \n",
    "    # #add salt and pepper noise\n",
    "    # sp_noise = randint(0, 4, size=img.shape, dtype=np.uint8)\n",
    "    # sp_noise = np.where(sp_noise == 0, np.zeros_like(img), 255*np.ones_like(img))\n",
    "    # # img = cv.bitwise_xor(img, sp_noise)\n",
    "\n",
    "\n",
    "    # #reduce contrast\n",
    "    # const = np.random.uniform(0.1,0.8)\n",
    "    # # if np.random.uniform() > .5:\n",
    "    # #     const = const*0.2\n",
    "    # img = 127*(1-const) + img*const\n",
    "    # img = img.astype(np.uint8)\n",
    "\n",
    "    #add noise \n",
    "    std = 80\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "    # #add random brightness\n",
    "    # max_brightness = 60\n",
    "    # brightness = randint(-max_brightness, max_brightness)\n",
    "    # if brightness > 0:\n",
    "    #     img = cv.add(img, brightness)\n",
    "    # elif brightness < 0:\n",
    "    #     img = cv.subtract(img, -brightness)\n",
    "\n",
    "    # #blur \n",
    "    # img = cv.blur(img, (randint(1,3),randint(1,3)))\n",
    "\n",
    "    # # invert color\n",
    "    # if np.random.uniform(0, 1) > 0.6:\n",
    "    #     img = cv.bitwise_not(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(5000):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(100)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        #classification label for road ahead = 1,0,0,0,1,0,0,0,0,0,0,0,0,0,0\n",
    "        road_images_indexes = []\n",
    "        tot_lines = 0\n",
    "        with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            tot_lines = len(lines)\n",
    "            for i,line in enumerate(lines):\n",
    "                if line == '1,0,0,0,1,0,0,0,0,0,0,0,0,0,0':\n",
    "                    road_images_indexes.append(i)\n",
    "        print(f'total pure road images: {len(road_images_indexes)}')\n",
    "        road_imgs_mask = np.zeros(tot_lines, dtype=bool)\n",
    "        road_imgs_mask[road_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            # self.all_imgs = torch.zeros((2*max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8) #adding flipped img\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            # road images specifically are added again along with their flipped image and label\n",
    "            road_imgs = torch.zeros((2*len(road_images_indexes), SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "            road_labels = []\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "            road_idx = 0\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(max_load)):\n",
    "\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                sample = line.split(',')\n",
    "                #keep only info related to the lane, discard distance from stop line \n",
    "                # sample = [sample[0], sample[1], sample[2], sample[3], sample[4]] #e2=lateral error, e3=yaw error point ahead, curvature, dist stopline, angle stopline\n",
    "                sample = [sample[1]]\n",
    "                reg_label = np.array([float(s) for s in sample], dtype=np.float32)\n",
    "\n",
    "                #img \n",
    "                img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "\n",
    "                #check if its in the road images\n",
    "                if road_imgs_mask[i]:\n",
    "                    img_r = load_and_augment_img(img.copy())\n",
    "                    # cv.imshow('imgR', img_r)\n",
    "                    img_r = img_r[:,:,np.newaxis]\n",
    "\n",
    "                    img_l = cv.flip(img, 1)\n",
    "                    img_l = load_and_augment_img(img_l)\n",
    "                    # cv.imshow('imgL', img_l)\n",
    "                    img_l = img_l[:,:,np.newaxis]\n",
    "                    # cv.waitKey(1)\n",
    "\n",
    "                    road_imgs[2*road_idx] = torch.from_numpy(img_r)\n",
    "                    road_imgs[2*road_idx+1] = torch.from_numpy(img_l)\n",
    "                    road_labels.append(reg_label)\n",
    "                    road_labels.append(-reg_label)\n",
    "                    road_idx += 1\n",
    "\n",
    "                else:\n",
    "                    img = load_and_augment_img(img)\n",
    "                    # cv.putText(img, f'{np.rad2deg(reg_label[0]):.1f}', (5,25), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                    MAX_SHOW = 1000\n",
    "                    max_show = MAX_SHOW\n",
    "                    if i < max_show:\n",
    "                        cv.imshow('img', img)\n",
    "                        key = cv.waitKey(1)\n",
    "                        if i == max_show-1:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(reg_label)\n",
    "                    all_img_idx += 1\n",
    "\n",
    "            #cut imgs to the right length\n",
    "            road_imgs = road_imgs[:2*road_idx]\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "\n",
    "            #concatenate all_imgs and road_imgs\n",
    "            print(f'road images: {road_imgs.shape}')\n",
    "            print(f'all images: {self.all_imgs.shape}')\n",
    "            self.all_imgs = torch.cat((self.all_imgs, road_imgs), dim=0)\n",
    "            print(f'self.data shape: {len(self.data)}')\n",
    "            print(f'road_labels shape = {len(road_labels)}')\n",
    "            self.data = np.concatenate((np.array(self.data), np.array(road_labels)), axis=0)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "            #free road_imgs from memory\n",
    "            del road_imgs\n",
    "            del road_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pure road images: 51214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97796/97796 [05:15<00:00, 309.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "road images: torch.Size([102428, 32, 32, 1])\n",
      "all images: torch.Size([46582, 32, 32, 1])\n",
      "self.data shape: 46582\n",
      "road_labels shape = 102428\n",
      "\n",
      "all imgs: torch.Size([149010, 32, 32, 1])\n",
      "data: (149010, 1)\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 1, 32, 32])\n",
      "torch.Size([8192, 1])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    # err_losses2 = []\n",
    "    err_losses3 = []\n",
    "    # curv_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        # err2 = output[:, 0]\n",
    "        err3 = output[:, 0]\n",
    "        # curv_out = output[:, 2]\n",
    "\n",
    "        # err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 0]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        # Compute the losses\n",
    "        # err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        # curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = err_loss3 + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        # err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    # err_loss2 = np.mean(err_losses2)\n",
    "    err_loss3 = np.mean(err_losses3)\n",
    "    # curv_loss = np.mean(curv_losses)\n",
    "    return err_loss3\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(lane_keeper_ahead, val_dataloader, regr_loss_fn, device=device):\n",
    "    lane_keeper_ahead.eval()\n",
    "    err_losses3 = []\n",
    "    # err_losses2 = []\n",
    "    # curv_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = lane_keeper_ahead(input)\n",
    "\n",
    "        regr_out = output\n",
    "        # err2 = regr_out[:, 0]\n",
    "        err3 = regr_out[:, 0]\n",
    "        # curv_out = regr_out[:, 2]\n",
    "\n",
    "        # err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 0]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "\n",
    "        # err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "    return np.mean(err_losses3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  79/130,  loss = MSELoss() \n",
      "yaw_err_loss3: 0.0381,   Val: 0.0368, best_val: 0.0361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 4/17 [00:02<00:08,  1.52it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_284062/2014730370.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# if True:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mregr_loss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr_loss_fn1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mregr_loss_fn2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0merr_loss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper_ahead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mval_loss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper_ahead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_284062/42942214.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Loop over the training batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Move the input and target data to the selected device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0melem_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0melem_size\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0melem_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0melem_size\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.003 #0.005\n",
    "epochs = 130\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 0.*1e-4 #9e-4\n",
    "L2_lambda = 0.*1e-2 #1e-2\n",
    "optimizer = torch.optim.Adam(lane_keeper_ahead.parameters(), lr=lr, weight_decay=0*9e-5) #wd = 2e-3# 3e-5\n",
    "regr_loss_fn1 = nn.MSELoss() #before epochs/2\n",
    "regr_loss_fn2 = nn.MSELoss() #after epochs/2 for finetuning\n",
    "\n",
    "best_val = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        regr_loss_fn = regr_loss_fn1 if epoch < epochs//2 else regr_loss_fn2\n",
    "        err_loss3 = train_epoch(lane_keeper_ahead, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_loss3 = val_epoch(lane_keeper_ahead, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    if val_loss3 < best_val:\n",
    "        best_val = val_loss3\n",
    "        torch.save(lane_keeper_ahead.state_dict(), model_name)\n",
    "        print(\"model saved\")\n",
    "    \n",
    "    print(f\"Epoch  {epoch+1}/{epochs},  loss = {regr_loss_fn} \\nyaw_err_loss3: {err_loss3:.4f},   Val: {val_loss3:.4f}, best_val: {best_val:.4f}\")\n",
    "    # print(f\"lat_err_loss2: {err_loss2:.4f},   Val: {val_loss2:.4f}\")\n",
    "    # print(f\"curv_loss: {curv_loss}\")\n",
    "\n",
    "#Note: sweet spot for training is around 0.016 -> 0.020, also note that training can get stuck, and loss can start improving randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 169.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yaw_err3_loss: 0.03607897460460663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "lane_keeper_ahead.load_state_dict(torch.load(model_name))\n",
    "err_loss3 = val_epoch(lane_keeper_ahead, val_dataloader, regr_loss_fn, device)\n",
    "\n",
    "# print(f\"lateral_err2_loss: {err_loss2}\")\n",
    "print(f\"yaw_err3_loss: {err_loss3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 5, 5)\n",
      "(8, 8, 5, 5)\n",
      "(64, 8, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x144 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(lane_keeper_ahead.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 1, 4, 'conv0', size=(8,2))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 4, 4, 'conv1', size=(5,5)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 8, 8, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LaneKeeperAhead(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.3, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout(p=0.3, inplace=False)\n",
       "    (11): Conv2d(8, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "lane_keeper_ahead.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "lane_keeper_ahead.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "lane_keeper_ahead.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(lane_keeper_ahead, dummy_input, onnx_lane_keeper_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lane_keeper_ahead.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1605.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[-0.30020157]]\n",
      "Predictions shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_lane_keeper_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
