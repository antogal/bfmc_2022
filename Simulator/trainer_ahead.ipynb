{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/lane_keeper_ahead.pt'\n",
    "onnx_lane_keeper_path = \"models/lane_keeper_ahead.onnx\"\n",
    "max_load = 500_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE\n",
    "# #very good\n",
    "# class LaneKeeperAhead(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             nn.Linear(in_features=4*4*4, out_features=16),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(in_features=16, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# lane_keeper_ahead = LaneKeeperAhead(out_dim=2,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class LaneKeeperAhead(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        prob = 0.3\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 4, kernel_size=5, stride=1), #out = 28\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=14\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(4, 4, kernel_size=5, stride=1), #out = 10\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=5\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(4, 32, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            #normalize\n",
    "            # nn.BatchNorm1d(3*3*4),\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=1*1*32, out_features=16),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(in_features=16, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "lane_keeper_ahead = LaneKeeperAhead(out_dim=1,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "lane_keeper_ahead.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = lane_keeper_ahead(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from time import time, sleep\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "    #convert to gray\n",
    "    img = cv.resize(img, (4*SIZE[1], 4*SIZE[0]))\n",
    "\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "        axes_length = (randint(10//4, 50//4), randint(50//4, 300//4))\n",
    "        angle = randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (50,50))\n",
    "    noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = np.clip(light, 0, 51)\n",
    "    light *= 5\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    # cv.imshow('light', light)\n",
    "    # if cv.waitKey(0) == ord('q'):\n",
    "    #     break\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 5), randint(1, 5)))\n",
    "\n",
    "    # cut the top third of the image, let it 640x320\n",
    "    img = img[int(img.shape[0]/3):,:] ################################# /3\n",
    "    # assert img.shape == (320,640), f'img shape cut = {img.shape}'\n",
    "\n",
    "    #edges\n",
    "    img = cv.resize(img, (2*SIZE[1], 2*SIZE[0]))\n",
    "\n",
    "    r = randint(0, 5)\n",
    "    if r == 0:\n",
    "        #dilate\n",
    "        kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.dilate(img, kernel, iterations=1)\n",
    "    elif r == 1:\n",
    "        #erode\n",
    "        kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.erode(img, kernel, iterations=1)\n",
    "\n",
    "\n",
    "    #edges    \n",
    "    img = cv.Canny(img, 100, 200)\n",
    "\n",
    "    #blur\n",
    "    img = cv.blur(img, (3,3))\n",
    "\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    # #get max brightness\n",
    "    # max_brightness = np.max(img)\n",
    "    # ratio = 255.0/max_brightness\n",
    "    # #normalize\n",
    "    # img = (img*ratio).astype(np.uint8)\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = 3\n",
    "    offset = randint(-max_offset, max_offset)\n",
    "    img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        img[:offset, :] = 0 #randint(0,255)\n",
    "    elif offset < 0:\n",
    "        img[offset:, :] = 0 # randint(0,255)\n",
    "    \n",
    "    # #add salt and pepper noise\n",
    "    # sp_noise = randint(0, 4, size=img.shape, dtype=np.uint8)\n",
    "    # sp_noise = np.where(sp_noise == 0, np.zeros_like(img), 255*np.ones_like(img))\n",
    "    # # img = cv.bitwise_xor(img, sp_noise)\n",
    "\n",
    "\n",
    "    # #reduce contrast\n",
    "    # const = np.random.uniform(0.1,0.8)\n",
    "    # # if np.random.uniform() > .5:\n",
    "    # #     const = const*0.2\n",
    "    # img = 127*(1-const) + img*const\n",
    "    # img = img.astype(np.uint8)\n",
    "\n",
    "    #add noise \n",
    "    std = 80\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "    # #add random brightness\n",
    "    # max_brightness = 60\n",
    "    # brightness = randint(-max_brightness, max_brightness)\n",
    "    # if brightness > 0:\n",
    "    #     img = cv.add(img, brightness)\n",
    "    # elif brightness < 0:\n",
    "    #     img = cv.subtract(img, -brightness)\n",
    "\n",
    "    # #blur \n",
    "    # img = cv.blur(img, (randint(1,3),randint(1,3)))\n",
    "\n",
    "    # # invert color\n",
    "    # if np.random.uniform(0, 1) > 0.6:\n",
    "    #     img = cv.bitwise_not(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(5000):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(100)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        #classification label for road ahead = 1,0,0,0,1,0,0,0,0,0,0,0,0,0,0\n",
    "        road_images_indexes = []\n",
    "        tot_lines = 0\n",
    "        with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            tot_lines = len(lines)\n",
    "            for i,line in enumerate(lines):\n",
    "                if line == '1,0,0,0,1,0,0,0,0,0,0,0,0,0,0':\n",
    "                    road_images_indexes.append(i)\n",
    "        print(f'total pure road images: {len(road_images_indexes)}')\n",
    "        road_imgs_mask = np.zeros(tot_lines, dtype=bool)\n",
    "        road_imgs_mask[road_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            # self.all_imgs = torch.zeros((2*max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8) #adding flipped img\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            # road images specifically are added again along with their flipped image and label\n",
    "            road_imgs = torch.zeros((2*len(road_images_indexes), SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "            road_labels = []\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "            road_idx = 0\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(max_load)):\n",
    "\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                sample = line.split(',')\n",
    "                #keep only info related to the lane, discard distance from stop line \n",
    "                # sample = [sample[0], sample[1], sample[2], sample[3], sample[4]] #e2=lateral error, e3=yaw error point ahead, curvature, dist stopline, angle stopline\n",
    "                sample = [sample[1]]\n",
    "                reg_label = np.array([float(s) for s in sample], dtype=np.float32)\n",
    "\n",
    "                #img \n",
    "                img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "\n",
    "                #check if its in the road images\n",
    "                if road_imgs_mask[i]:\n",
    "                    img_r = load_and_augment_img(img.copy())\n",
    "                    # cv.imshow('imgR', img_r)\n",
    "                    img_r = img_r[:,:,np.newaxis]\n",
    "\n",
    "                    img_l = cv.flip(img, 1)\n",
    "                    img_l = load_and_augment_img(img_l)\n",
    "                    # cv.imshow('imgL', img_l)\n",
    "                    img_l = img_l[:,:,np.newaxis]\n",
    "                    # cv.waitKey(1)\n",
    "\n",
    "                    road_imgs[2*road_idx] = torch.from_numpy(img_r)\n",
    "                    road_imgs[2*road_idx+1] = torch.from_numpy(img_l)\n",
    "                    road_labels.append(reg_label)\n",
    "                    road_labels.append(-reg_label)\n",
    "                    road_idx += 1\n",
    "\n",
    "                else:\n",
    "                    img = load_and_augment_img(img)\n",
    "                    # cv.putText(img, f'{np.rad2deg(reg_label[0]):.1f}', (5,25), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                    MAX_SHOW = 1000\n",
    "                    max_show = MAX_SHOW\n",
    "                    if i < max_show:\n",
    "                        cv.imshow('img', img)\n",
    "                        key = cv.waitKey(1)\n",
    "                        if i == max_show-1:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(reg_label)\n",
    "                    all_img_idx += 1\n",
    "\n",
    "            #cut imgs to the right length\n",
    "            road_imgs = road_imgs[:2*road_idx]\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "\n",
    "            #concatenate all_imgs and road_imgs\n",
    "            print(f'road images: {road_imgs.shape}')\n",
    "            print(f'all images: {self.all_imgs.shape}')\n",
    "            self.all_imgs = torch.cat((self.all_imgs, road_imgs), dim=0)\n",
    "            print(f'self.data shape: {len(self.data)}')\n",
    "            print(f'road_labels shape = {len(road_labels)}')\n",
    "            self.data = np.concatenate((np.array(self.data), np.array(road_labels)), axis=0)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "            #free road_imgs from memory\n",
    "            del road_imgs\n",
    "            del road_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pure road images: 104461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250910/250910 [10:53<00:00, 383.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "road images: torch.Size([208922, 32, 32, 1])\n",
      "all images: torch.Size([146449, 32, 32, 1])\n",
      "self.data shape: 146449\n",
      "road_labels shape = 208922\n",
      "\n",
      "all imgs: torch.Size([355371, 32, 32, 1])\n",
      "data: (355371, 1)\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 1, 32, 32])\n",
      "torch.Size([8192, 1])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    # err_losses2 = []\n",
    "    err_losses3 = []\n",
    "    # curv_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        # err2 = output[:, 0]\n",
    "        err3 = output[:, 0]\n",
    "        # curv_out = output[:, 2]\n",
    "\n",
    "        # err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 0]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        # Compute the losses\n",
    "        # err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        # curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = err_loss3 + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        # err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    # err_loss2 = np.mean(err_losses2)\n",
    "    err_loss3 = np.mean(err_losses3)\n",
    "    # curv_loss = np.mean(curv_losses)\n",
    "    return err_loss3\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(lane_keeper_ahead, val_dataloader, regr_loss_fn, device=device):\n",
    "    lane_keeper_ahead.eval()\n",
    "    err_losses3 = []\n",
    "    # err_losses2 = []\n",
    "    # curv_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = lane_keeper_ahead(input)\n",
    "\n",
    "        regr_out = output\n",
    "        # err2 = regr_out[:, 0]\n",
    "        err3 = regr_out[:, 0]\n",
    "        # curv_out = regr_out[:, 2]\n",
    "\n",
    "        # err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 0]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "\n",
    "        # err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "    return np.mean(err_losses3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  172/500,  loss = MSELoss() \n",
      "yaw_err_loss3: 0.0365,   Val: 0.0462, best_val: 0.0357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [00:03<00:18,  1.76it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18552/1778378720.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# if True:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mregr_loss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr_loss_fn1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mregr_loss_fn2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0merr_loss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper_ahead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mval_loss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper_ahead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_18552/42942214.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# err_losses2.append(err_loss2.detach().cpu().numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0merr_losses3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_loss3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;31m# curv_losses.append(curv_loss.detach().cpu().numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.003 #0.005\n",
    "epochs = 500\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 1e-4 #9e-4\n",
    "L2_lambda = 1e-2 #1e-2\n",
    "optimizer = torch.optim.Adam(lane_keeper_ahead.parameters(), lr=lr, weight_decay=9e-5) #wd = 2e-3# 3e-5\n",
    "regr_loss_fn1 = nn.MSELoss() #before epochs/2\n",
    "regr_loss_fn2 = nn.MSELoss() #after epochs/2 for finetuning\n",
    "\n",
    "best_val = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        regr_loss_fn = regr_loss_fn1 if epoch < epochs//2 else regr_loss_fn2\n",
    "        err_loss3 = train_epoch(lane_keeper_ahead, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_loss3 = val_epoch(lane_keeper_ahead, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    if val_loss3 < best_val:\n",
    "        best_val = val_loss3\n",
    "        torch.save(lane_keeper_ahead.state_dict(), model_name)\n",
    "        print(\"model saved\")\n",
    "    \n",
    "    print(f\"Epoch  {epoch+1}/{epochs},  loss = {regr_loss_fn} \\nyaw_err_loss3: {err_loss3:.4f},   Val: {val_loss3:.4f}, best_val: {best_val:.4f}\")\n",
    "    # print(f\"lat_err_loss2: {err_loss2:.4f},   Val: {val_loss2:.4f}\")\n",
    "    # print(f\"curv_loss: {curv_loss}\")\n",
    "\n",
    "#Note: sweet spot for training is around 0.016 -> 0.020, also note that training can get stuck, and loss can start improving randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 356/356 [00:01<00:00, 271.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yaw_err3_loss: 0.035721305757761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "lane_keeper_ahead.load_state_dict(torch.load(model_name))\n",
    "err_loss3 = val_epoch(lane_keeper_ahead, val_dataloader, regr_loss_fn, device)\n",
    "\n",
    "# print(f\"lateral_err2_loss: {err_loss2}\")\n",
    "print(f\"yaw_err3_loss: {err_loss3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 5, 5)\n",
      "(4, 4, 5, 5)\n",
      "(32, 4, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x144 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(lane_keeper_ahead.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 1, 4, 'conv0', size=(8,2))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 4, 4, 'conv1', size=(5,5)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 8, 8, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LaneKeeperAhead(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.3, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout(p=0.3, inplace=False)\n",
       "    (11): Conv2d(4, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "lane_keeper_ahead.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "lane_keeper_ahead.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "lane_keeper_ahead.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(lane_keeper_ahead, dummy_input, onnx_lane_keeper_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lane_keeper_ahead.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1920.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[-0.1468579]]\n",
      "Predictions shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_lane_keeper_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
