{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "# NOTE : Sign detecttion uses the folder training_images to load the backgrounds\n",
    "# it uses the folder sign_imgs to load the signs, the rate of examples is important\n",
    "num_channels = 1\n",
    "SIZE = (16, 16)\n",
    "model_name = 'models/sign_classifier.pt'\n",
    "onnx_sign_classifier_path = \"models/sign_classifier.onnx\"\n",
    "max_load = 100_000 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 examples loaded for 10 class_names\n",
      "example labels: [9, 7, 9, 9, 9, 5, 6, 1, 4, 8, 0, 9, 2, 9, 3]\n"
     ]
    }
   ],
   "source": [
    "# LOAD EXAMPLES\n",
    "# imgs of the examples must be in a specific folder (ex: sign_imgs), and must be named as \"class_<number>.png\"\n",
    "# ex: sign_imgs/stop_1.png, note: start from 1\n",
    "\n",
    "#load examples\n",
    "examples_folder = 'sign_imgs'     \n",
    "class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway', 'nosign']\n",
    "# class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway']\n",
    "\n",
    "file_names = [f for f in os.listdir(examples_folder) if f.endswith('.png')]\n",
    "example_labels = [class_names.index(name.split('_')[0]) for name in file_names if name.split('_')[0] in class_names]\n",
    "example_imgs = [cv.resize(cv.imread(os.path.join(examples_folder, name), cv.IMREAD_GRAYSCALE), (64,64)) for name in file_names if name.split('_')[0] in class_names]\n",
    "tot_examples = len(example_imgs)\n",
    "tot_classes = len(class_names) \n",
    "print(f'{tot_examples} examples loaded for {tot_classes} class_names')\n",
    "print(f'example labels: {example_labels}')    \n",
    "\n",
    "#show images\n",
    "cv.namedWindow('example', cv.WINDOW_NORMAL)\n",
    "for i in range(tot_examples):\n",
    "    img = example_imgs[i].copy()\n",
    "    # add text label\n",
    "    cv.putText(img, class_names[example_labels[i]], (10,30), cv.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "    cv.imshow('example', img)\n",
    "    key = cv.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SignClassifier(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         p = 0.3\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 16, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=1), #out=10\n",
    "#             nn.BatchNorm2d(16),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.Conv2d(16, 32, kernel_size=5, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=5\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.Conv2d(32, 256, kernel_size=5, stride=1), #out = 1\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             # nn.Linear(in_features=4*4*4, out_features=128),\n",
    "#             # nn.ReLU(True),\n",
    "#             # nn.Dropout(p),\n",
    "#             # nn.Linear(in_features=128, out_features=out_dim),\n",
    "#             nn.Linear(in_features=256*1*1, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# sign_classifier = SignClassifier(out_dim=tot_classes,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class SignClassifier(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        p = 0.3\n",
    "        ### Convoluational layers\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 12\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1), #out=10\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(p),\n",
    "            nn.Conv2d(8, 16, kernel_size=5, stride=1), #out = 6\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1), #out=5\n",
    "            # nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p),\n",
    "            nn.Conv2d(16, 128, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            # nn.Linear(in_features=4*4*4, out_features=128),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.Dropout(p),\n",
    "            # nn.Linear(in_features=128, out_features=out_dim),\n",
    "            nn.Linear(in_features=128*1*1, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "sign_classifier = SignClassifier(out_dim=tot_classes,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 16, 16])\n",
      "out shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "sign_classifier.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = sign_classifier(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "\n",
    "def draw_box(img, x, y, w):\n",
    "    img = cv.rectangle(img, (int(x-w/2), int(y-w/2)), (int(x+w/2), int(y+w/2)), 255, 2)\n",
    "    return img\n",
    "\n",
    "def load_and_augment_img(img, example_index=0, example_imgs=example_imgs):\n",
    "    # img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "    if img is not None:\n",
    "        #convert to gray\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "        # #crop to the top right quadrant\n",
    "        img = img[0:img.shape[1]//2, img.shape[1]//2:]\n",
    "        canv_dim = randint(50//4, 80//4)\n",
    "        start_x = randint(0, img.shape[1]-canv_dim)\n",
    "        start_y = randint(0, img.shape[0]-canv_dim)\n",
    "        img = img[start_y:start_y+canv_dim, start_x:start_x+canv_dim]\n",
    "        img = cv.resize(img, (64, 64))\n",
    "    else:\n",
    "        img = randint(0,255,(64,64), dtype=np.uint8)\n",
    "\n",
    "\n",
    "    ## EXAMPLE AUGMENTATION ##############################################################\n",
    "    #load example\n",
    "    example = example_imgs[example_index]\n",
    "    resize_ratio = max(img.shape)/max(example.shape[0], example.shape[1])\n",
    "    example = cv.resize(example, (int(example.shape[1]*resize_ratio), int(example.shape[0]*resize_ratio)))\n",
    "    \n",
    "    # #flip\n",
    "    # if np.random.uniform() < 0.5:\n",
    "    #     example = cv.flip(example, 1)\n",
    "\n",
    "    #get example mask\n",
    "    example_mask = np.where(example == 0, np.zeros_like(example), 255*np.ones_like(example))\n",
    "    #blur the example\n",
    "    # example = cv.blur(example, (randint(3,9),randint(3,9)))\n",
    "\n",
    "    #add noise to the example\n",
    "    std = 20\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.subtract(example, noisem)\n",
    "    noisep = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.add(example, noisep)\n",
    "\n",
    "    # #dilate\n",
    "    # kernel = np.ones((randint(3,7),randint(3,7)), np.uint8)\n",
    "    # example = cv.dilate(example, kernel, iterations=1)\n",
    "\n",
    "    #set zero where example mask is zero\n",
    "    example = np.where(example_mask == 0, np.zeros_like(example), example)\n",
    "    # cv.imshow('test', example)\n",
    "\n",
    "    # #random rotation\n",
    "    # angle = randint(-10,10)\n",
    "    # M = cv.getRotationMatrix2D((example.shape[1]/2, example.shape[0]/2), angle, 1)\n",
    "    # example = cv.warpAffine(example, M, (example.shape[1], example.shape[0]))\n",
    "\n",
    "    #perspective transform example\n",
    "    perspective_deformation = 20\n",
    "    pts1 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = pts2 + np.float32(randint(0,perspective_deformation,size=pts2.shape))\n",
    "    # print(f'pts2 = \\n{pts2}')\n",
    "    new_size_x = int(np.max(pts2[:,0]) - np.min(pts2[:,0]))\n",
    "    new_size_y = int(np.max(pts2[:,1]) - np.min(pts2[:,1]))\n",
    "    M = cv.getPerspectiveTransform(pts1,pts2)\n",
    "    example = cv.warpPerspective(example,M,(new_size_x,new_size_y))\n",
    "\n",
    "    #resize example keeping proportions\n",
    "    img_example_ratio = min(img.shape[0]/example.shape[0], img.shape[1]/example.shape[1])\n",
    "    scale_factor = np.random.uniform(.98, .99) ##.15, .35############################ PARAM ##############################\n",
    "    scale_factor = scale_factor * img_example_ratio\n",
    "    example = cv.resize(example, (0,0), fx=scale_factor, fy=scale_factor)\n",
    "    #match img shape\n",
    "    example_canvas = np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8)\n",
    "\n",
    "    #get a random position for the example\n",
    "    example_y = randint(0, img.shape[0] - example.shape[0])\n",
    "    example_x = randint(0, img.shape[1] - example.shape[1])\n",
    "    #paste example on canvas\n",
    "    example_canvas[example_y:example_y+example.shape[0], example_x:example_x+example.shape[1]] = example\n",
    "\n",
    "    #generate label for example\n",
    "    x_lab = example_x + example.shape[1]/2\n",
    "    y_lab = example_y + example.shape[0]/2\n",
    "    w_lab = max(example.shape[0], example.shape[1])\n",
    "    box = [x_lab, y_lab, w_lab]\n",
    "\n",
    "    old_example_canvas = example_canvas.copy()\n",
    "    \n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.3,.99)\n",
    "    example_canvas = 127*(1-const) + example_canvas*const\n",
    "    #clip values\n",
    "    example_canvas = np.clip(example_canvas, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #paste canvas on img\n",
    "    img = np.where(old_example_canvas > 0, example_canvas, img) \n",
    "\n",
    "    #blur example\n",
    "    # example_canvas = cv.blur(example_canvas, (randint(1,7),randint(1,7)))\n",
    "\n",
    "    ##########################################################################################\n",
    "\n",
    "    # #create random ellipses to simulate light from the sun\n",
    "    # light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    # #add ellipses\n",
    "    # for j in range(2):\n",
    "    #     cent = (np.random.randint(0, img.shape[0]), np.random.randint(0, img.shape[1]))\n",
    "    #     axes_length = (np.random.randint(10, 50), np.random.randint(50, 300))\n",
    "    #     angle = np.random.randint(0, 360)\n",
    "    #     light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    # #create an image of random white and black pixels\n",
    "    # light = cv.blur(light, (100,100))\n",
    "    # noise = np.random.randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    # light = cv.subtract(light, noise)\n",
    "    # light = 5 * light\n",
    "    # #add light to the image\n",
    "    # img = cv.add(img, light)\n",
    "\n",
    "    #blur the image\n",
    "    # img = cv.blur(img, (9,9))\n",
    "    # img = cv.blur(img, (randint(1,3), randint(1,3)))\n",
    "\n",
    "    #crop into the img at random position\n",
    "    zoom = randint(0, 16)\n",
    "    img = img[zoom:img.shape[0]-zoom, zoom:img.shape[1]-zoom]\n",
    "\n",
    "\n",
    "    # #add random tilt\n",
    "    # max_offset = 10\n",
    "    # offset = np.random.randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset, axis=0)\n",
    "    # if offset > 0:\n",
    "    #     img[:offset, :] = np.random.randint(0,255)\n",
    "    # elif offset < 0:\n",
    "    #     img[offset:, :] = np.random.randint(0,255)\n",
    "\n",
    "    # offset_y = np.random.randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset_y, axis=1)\n",
    "    # if offset_y > 0:\n",
    "    #     img[:, :offset_y] = np.random.randint(0,255)\n",
    "    # elif offset_y < 0:\n",
    "    #     img[:, offset_y:] = np.random.randint(0,255)\n",
    "\n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.6,.99)\n",
    "    if np.random.uniform() > 5:\n",
    "        const = const*0.2\n",
    "    img = 127*(1-const) + img*const\n",
    "    img = img.astype(np.uint8)\n",
    "\n",
    "    #add noise \n",
    "    std = 20\n",
    "    std = np.random.randint(1, std)\n",
    "    noisem = np.random.randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = np.random.randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "    # #blur \n",
    "    # img = cv.blur(img, (np.random.randint(1,3),np.random.randint(1,3)))\n",
    "\n",
    "    #add random brightness\n",
    "    max_brightness = 50\n",
    "    brightness = np.random.randint(-max_brightness, max_brightness)\n",
    "    if brightness > 0:\n",
    "        img = cv.add(img, brightness)\n",
    "    elif brightness < 0:\n",
    "        img = cv.subtract(img, -brightness)\n",
    "    \n",
    "    # # invert color\n",
    "    # if np.random.uniform(0, 1) > 0.5:\n",
    "    #     img = cv.bitwise_not(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = cv.resize(img, (160,120))\n",
    "    # img = None\n",
    "    img = load_and_augment_img(img, example_index=(i%tot_examples))\n",
    "\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(150)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.class_names = []\n",
    "        self.channels = channels\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            # self.all_imgs = torch.zeros((2*max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8) #adding flipped img\n",
    "            self.all_imgs = torch.zeros((max_load*tot_examples, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN\n",
    "\n",
    "            for i in tqdm(range(max_load)):\n",
    "                img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "                img = cv.resize(img, (160,120))\n",
    "                for j in range(tot_examples):\n",
    "                    img_j = load_and_augment_img(img.copy(), example_index=j)\n",
    "                    if i < 100:\n",
    "                        cv.imshow('img', img_j)\n",
    "                        cv.waitKey(1)\n",
    "                        if i == 99:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img_j = img_j[:, :,np.newaxis]\n",
    "                    #convert to tensor\n",
    "                    img_j = torch.from_numpy(img_j)\n",
    "                    self.all_imgs[i*tot_examples+j] = img_j\n",
    "                    self.class_names.append(example_labels[j])\n",
    "            \n",
    "            self.data = torch.from_numpy(np.array(self.data))\n",
    "            self.class_names = torch.from_numpy(np.array(self.class_names))\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'class_names: {self.class_names.shape}')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.class_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        class_label = self.class_names[idx]\n",
    "        return img, class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [28:55<00:00, 57.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all imgs: torch.Size([1500000, 16, 16, 1])\n",
      "class_names: torch.Size([1500000])\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16384, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16384, 1, 16, 16])\n",
      "torch.Size([16384])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, class_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    class_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, class_label) in tqdm(dataloader):\n",
    "        #one hot encode the class label\n",
    "        class_label = torch.eye(tot_classes)[class_label]\n",
    "        # Move the input and target data to the selected device\n",
    "        input, class_label = input.to(device), class_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        class_out = output[:, 0:]\n",
    "\n",
    "        #classification loss\n",
    "        assert class_label.shape == class_out.shape, f'class_label.shape: {class_label.shape}, output.shape: {output.shape}'\n",
    "\n",
    "        class_loss = 1.0*class_loss_fn(class_out, class_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = class_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        class_losses.append(class_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    class_loss = np.mean(class_losses)\n",
    "    return  np.sqrt(class_loss)\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(sign_classifier, val_dataloader, regr_loss_fn, class_loss_fn, device=device):\n",
    "    sign_classifier.eval()\n",
    "    y_losses = []\n",
    "    x_losses = []\n",
    "    w_losses = []\n",
    "    class_losses = []\n",
    "    for (input, class_label) in tqdm(val_dataloader):\n",
    "        #one hot encode the class label\n",
    "        class_label = torch.eye(tot_classes)[class_label]\n",
    "        input, class_label = input.to(device), class_label.to(device)\n",
    "        output = sign_classifier(input)\n",
    "\n",
    "        class_loss = 1.0*class_loss_fn(output[:, 0:], class_label)\n",
    "    \n",
    "        class_losses.append(class_loss.detach().cpu().numpy())\n",
    "    return  np.mean(class_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  32/50\n",
      "class_loss: 0.2435 --- Val: 0.0454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3/83 [00:04<01:57,  1.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_56191/1476717294.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# if True:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mclass_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msign_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mval_class_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msign_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_56191/3319940368.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, regr_loss_fn, class_loss_fn, optimizer, L1_lambda, L2_lambda, device)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mclass_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Return the average training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.005 #0.005\n",
    "epochs = 50\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 1e-4 #9e-4\n",
    "L2_lambda = 1e-3 #1e-2\n",
    "optimizer = torch.optim.Adam(sign_classifier.parameters(), lr=lr, weight_decay=3e-5) #wd = 2e-3# 9e-5\n",
    "\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "class_loss_fn = nn.CrossEntropyLoss()\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        class_loss = train_epoch(sign_classifier, train_dataloader, regr_loss_fn, class_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_class_loss = val_epoch(sign_classifier, val_dataloader, regr_loss_fn, class_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs}\")\n",
    "    print(f\"class_loss: {class_loss:.4f} --- Val: {val_class_loss:.4f}\")\n",
    "    torch.save(sign_classifier.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "# val_dataloader2 = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "# sign_classifier.load_state_dict(torch.load(model_name))\n",
    "# sign_classifier.eval()\n",
    "# for (input, class_label) in tqdm(val_dataloader2):\n",
    "#     #convert img to numpy array\n",
    "#     img = input[0].detach().cpu().numpy()[0]\n",
    "#     #convert to sigle byte\n",
    "#     img = img.astype(np.uint8)\n",
    "#     input, box_label, class_label =input.to(device), box_label.to(device), class_label.to(device)\n",
    "#     output = sign_classifier(input)\n",
    "#     x = output[:, 0]\n",
    "#     y = output[:, 1]\n",
    "#     w = output[:, 2]\n",
    "#     class_out = output[:, 3:]\n",
    "#     # print(class_out)\n",
    "#     class_out = torch.argmax(class_out, dim=1)\n",
    "#     class_out = class_out.detach().cpu().numpy()\n",
    "#     # print(f\"Predicted class: {class_out[0]}, Actual class: {class_label[0]}\")\n",
    "#     class_out = class_out[0]\n",
    "#     class_out = class_names[class_out]\n",
    "#     true_class = class_names[class_label[0]]\n",
    "#     img = draw_box(img, x, y, w)\n",
    "#     #text for labels\n",
    "#     img = cv.putText(img, f\"True: {true_class}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "#     img = cv.putText(img, f\"Pred: {class_out}\", (10, 60), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "#     cv.imshow('img', img)\n",
    "#     key = cv.waitKey(0)\n",
    "#     if key == 27:\n",
    "#         break\n",
    "    \n",
    "# cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 5, 5)\n",
      "(16, 8, 5, 5)\n",
      "(128, 16, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(sign_classifier.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 4, 4, 'conv0', size=(10,10))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignClassifier(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Dropout(p=0.3, inplace=False)\n",
       "    (5): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout(p=0.3, inplace=False)\n",
       "    (10): Conv2d(16, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "sign_classifier.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "sign_classifier.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "sign_classifier.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(sign_classifier, dummy_input, onnx_sign_classifier_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "sign_classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 5328.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[-6.1170955   0.58457124 -3.2295918  -6.210986    0.51956844 -4.06247\n",
      "   1.671898   -6.629699   -3.1462693   0.8787304 ]]\n",
      "Predictions shape: (1, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_sign_classifier_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
