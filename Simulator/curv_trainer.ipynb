{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/lane_keeper_small.pt'\n",
    "onnx_lane_keeper_path = \"models/lane_keeper_small.onnx\"\n",
    "max_load = 150_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class LaneKeeper(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "            # nn.BatchNorm2d(4),\n",
    "            nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=4*4*4, out_features=16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=16, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "lane_keeper = LaneKeeper(out_dim=3,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "lane_keeper.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = lane_keeper(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "    # img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "\n",
    "    #convert to gray\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (np.random.randint(0, img.shape[0]), np.random.randint(0, img.shape[1]))\n",
    "        axes_length = (np.random.randint(10, 50), np.random.randint(50, 300))\n",
    "        angle = np.random.randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (100,100))\n",
    "    noise = np.random.randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = 5 * light\n",
    "\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    # cv.imshow('light', light)\n",
    "    # if cv.waitKey(0) == ord('q'):\n",
    "    #     break\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (9,9))\n",
    "\n",
    "    # cut the top third of the image, let it 640x320\n",
    "    img = img[int(img.shape[0]/3):,:] ################################# /3\n",
    "    # assert img.shape == (320,640), f'img shape cut = {img.shape}'\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = 5\n",
    "    offset = np.random.randint(-max_offset, max_offset)\n",
    "    img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        img[:offset, :] = np.random.randint(0,255)\n",
    "    elif offset < 0:\n",
    "        img[offset:, :] = np.random.randint(0,255)\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.1,1.2)\n",
    "    if np.random.uniform() > 5:\n",
    "        const = const*0.2\n",
    "    img = 127*(1-const) + img*const\n",
    "    img = img.astype(np.uint8)\n",
    "\n",
    "    #add noise \n",
    "    std = 150\n",
    "    std = np.random.randint(1, std)\n",
    "    noisem = np.random.randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = np.random.randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "    #blur \n",
    "    img = cv.blur(img, (np.random.randint(1,3),np.random.randint(1,3)))\n",
    "\n",
    "    #add random brightness\n",
    "    max_brightness = 50\n",
    "    brightness = np.random.randint(-max_brightness, max_brightness)\n",
    "    if brightness > 0:\n",
    "        img = cv.add(img, brightness)\n",
    "    elif brightness < 0:\n",
    "        img = cv.subtract(img, -brightness)\n",
    "    \n",
    "    # # invert color\n",
    "    # if np.random.uniform(0, 1) > 0.6:\n",
    "    #     img = cv.bitwise_not(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(100)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        #classification label for road ahead = 1,0,0,0,1,0,0,0,0,0,0,0,0,0,0\n",
    "        road_images_indexes = []\n",
    "        tot_lines = 0\n",
    "        with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            tot_lines = len(lines)\n",
    "            for i,line in enumerate(lines):\n",
    "                if line == '1,0,0,0,1,0,0,0,0,0,0,0,0,0,0':\n",
    "                    road_images_indexes.append(i)\n",
    "        print(f'total pure road images: {len(road_images_indexes)}')\n",
    "        road_imgs_mask = np.zeros(tot_lines, dtype=np.bool)\n",
    "        road_imgs_mask[road_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            # self.all_imgs = torch.zeros((2*max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8) #adding flipped img\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            # road images specifically are added again along with their flipped image and label\n",
    "            road_imgs = torch.zeros((2*len(road_images_indexes), SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "            road_labels = []\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "            road_idx = 0\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(max_load)):\n",
    "\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                sample = line.split(',')\n",
    "                #keep only info related to the lane, discard distance from stop line \n",
    "                sample = [sample[0], sample[1], sample[3]] #e2=lateral error, e3=yaw error point ahead, curvature\n",
    "                reg_label = np.array([float(s) for s in sample], dtype=np.float32)\n",
    "\n",
    "                #img \n",
    "                img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "\n",
    "                #check if its in the road images\n",
    "                if road_imgs_mask[i]:\n",
    "                    img_r = load_and_augment_img(img.copy())\n",
    "                    # cv.imshow('imgR', img_r)\n",
    "                    img_r = img_r[:,:,np.newaxis]\n",
    "\n",
    "                    img_l = cv.flip(img, 1)\n",
    "                    img_l = load_and_augment_img(img_l)\n",
    "                    # cv.imshow('imgL', img_l)\n",
    "                    img_l = img_l[:,:,np.newaxis]\n",
    "                    # cv.waitKey(1)\n",
    "\n",
    "                    road_imgs[2*road_idx] = torch.from_numpy(img_r)\n",
    "                    road_imgs[2*road_idx+1] = torch.from_numpy(img_l)\n",
    "                    road_labels.append(reg_label)\n",
    "                    road_labels.append(-reg_label)\n",
    "                    road_idx += 1\n",
    "\n",
    "                else:\n",
    "                    img = load_and_augment_img(img)\n",
    "                    if i < 100:\n",
    "                        cv.imshow('img', img)\n",
    "                        cv.waitKey(1)\n",
    "                        if i == 99:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(reg_label)\n",
    "                    all_img_idx += 1\n",
    "\n",
    "            #cut imgs to the right length\n",
    "            road_imgs = road_imgs[:2*road_idx]\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "\n",
    "            #concatenate all_imgs and road_imgs\n",
    "            print(f'road images: {road_imgs.shape}')\n",
    "            print(f'all images: {self.all_imgs.shape}')\n",
    "            self.all_imgs = torch.cat((self.all_imgs, road_imgs), dim=0)\n",
    "            print(f'self.data shape: {len(self.data)}')\n",
    "            print(f'road_labels shape = {len(road_labels)}')\n",
    "            self.data = np.concatenate((np.array(self.data), np.array(road_labels)), axis=0)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "            #free road_imgs from memory\n",
    "            del road_imgs\n",
    "            del road_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pure road images: 79427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142527/142527 [20:17<00:00, 117.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "road images: torch.Size([158854, 32, 32, 1])\n",
      "all images: torch.Size([63100, 32, 32, 1])\n",
      "self.data shape: 63100\n",
      "road_labels shape = 158854\n",
      "\n",
      "all imgs: torch.Size([221954, 32, 32, 1])\n",
      "data: (221954, 3)\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4096, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 1, 32, 32])\n",
      "torch.Size([4096, 3])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    err_losses2 = []\n",
    "    err_losses3 = []\n",
    "    curv_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        err2 = output[:, 0]\n",
    "        err3 = output[:, 1]\n",
    "        curv_out = output[:, 2]\n",
    "\n",
    "        err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 1]\n",
    "        curv_label = regr_label[:, 2]\n",
    "\n",
    "        # Compute the losses\n",
    "        err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = err_loss3 + err_loss2 + curv_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    err_loss2 = np.mean(err_losses2)\n",
    "    err_loss3 = np.mean(err_losses3)\n",
    "    curv_loss = np.mean(curv_losses)\n",
    "    return err_loss2, err_loss3, curv_loss\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device=device):\n",
    "    lane_keeper.eval()\n",
    "    err_losses3 = []\n",
    "    err_losses2 = []\n",
    "    curv_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = lane_keeper(input)\n",
    "\n",
    "        regr_out = output\n",
    "        err2 = regr_out[:, 0]\n",
    "        err3 = regr_out[:, 1]\n",
    "        curv_out = regr_out[:, 2]\n",
    "\n",
    "        err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 1]\n",
    "        curv_label = regr_label[:, 2]\n",
    "\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "        loss = err_loss3 + err_loss2 + curv_loss\n",
    "\n",
    "        err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "    return np.mean(err_losses2), np.mean(err_losses3), np.mean(curv_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/12 -> yaw_err_loss3: 0.0186\n",
      "Validation loss e3: 0.0187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 20/49 [00:06<00:09,  3.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22116/1387648822.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# if True:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0merr_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_loss3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mval_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_curv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22116/383214972.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Loop over the training batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Move the input and target data to the selected device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22116/1181403174.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# img = img.float()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.001 #0.005\n",
    "epochs = 12\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 9e-4 #0.001 \n",
    "L2_lambda = 1e-2 #2e-2\n",
    "optimizer = torch.optim.Adam(lane_keeper.parameters(), lr=lr, weight_decay=9e-5) #wd = 2e-3# 3e-5\n",
    "\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        err_loss2, err_loss3, curv_loss = train_epoch(lane_keeper, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_loss2, val_loss3, val_curv_loss = val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs} -> yaw_err_loss3: {err_loss3:.4f}\\nValidation loss e3: {val_loss3:.4f}\")\n",
    "    # print(f\"lateral_err_loss2: {err_loss2}\")\n",
    "    # print(f\"curv_loss: {curv_loss}\")\n",
    "    torch.save(lane_keeper.state_dict(), model_name)\n",
    "\n",
    "#Note: sweet spot for training is around 0.016 -> 0.020, also note that training can get stuck, and loss can start improve randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [00:01<00:00, 201.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lateral_err2_loss: 0.0017700579483062029\n",
      "yaw_err3_loss: 0.018654203042387962\n",
      "curv_loss: 0.00017484702402725816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "lane_keeper.load_state_dict(torch.load(model_name))\n",
    "lane_keeper.eval()\n",
    "err_losses3 = []\n",
    "err_losses2 = []\n",
    "curv_losses = []\n",
    "for (input, regr_label) in tqdm(val_dataloader):\n",
    "    input, regr_label =input.to(device), regr_label.to(device)\n",
    "    output = lane_keeper(input)\n",
    "\n",
    "    regr_out = output\n",
    "    err2 = regr_out[:, 0]\n",
    "    err3 = regr_out[:, 1]\n",
    "    curv_out = regr_out[:, 2]\n",
    "\n",
    "    err2_label = regr_label[:, 0]\n",
    "    err3_label = regr_label[:, 1]\n",
    "    curv_label = regr_label[:, 2]\n",
    "\n",
    "    err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "    err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "    curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "    loss = err_loss3 + err_loss2 + curv_loss\n",
    "\n",
    "    err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "    err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "    curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "\n",
    "print(f\"lateral_err2_loss: {np.mean(err_losses2)}\")\n",
    "print(f\"yaw_err3_loss: {np.mean(err_losses3)}\")\n",
    "print(f\"curv_loss: {np.mean(curv_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 5, 5)\n",
      "(4, 8, 5, 5)\n",
      "(4, 4, 6, 6)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAJ5CAYAAACqmupFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAilUlEQVR4nO3dfayfBX3//9fVHkopbS1ySlvKrXMgA3VqFdSvLPMrkDERI7oq4A+jbuoWE/FHDOjugqxuwFh0bmhgqFt06ryb/DLMgCW4zRmEzaEo3gRorVBK72ih9Ib2+v0B34S4wunJ3pT5/j4eCQl8uHhen9Nzneu8ep0mDOM4BgCgsxlP9xsAAHiqGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAzzthmH49WEY/mUYhk3DMKwZhuHqYRjmPd3vC+jD4AH+J3hGkkuSHJrkuCRLk1z2tL4joBWDB9ijYRgOH4bhS8Mw3D8Mw/phGD46DMOMYRh+dxiGlcMwrB2G4a+HYXjGY8cfNQzDOAzDecMwrBqGYd0wDB947N8dOgzDw8MwPPNx/Rc8dsx+4zh+ZhzHr43juHUcx41Jrkry8qfnIwc6MniA/2IYhplJ/r8kK5MclUefuHw2yVse++tXkzwrydwkH/2Z//x/JTk2yf9O8vvDMBw3juM9Sf4tyVmPO+7sJF8Yx3HnHt7CyUlur/loAJLB/0sL+FnDMLw0yVeTLBnH8ZHHvX5jki+O4/iXj/3zsUm+m+SAJIcluSvJ4eM4rn7s39+c5IpxHD87DMPbk5w9juMrh2EYkqxKcs44jl//mXOfkuTzSU4cx/GHT/XHCvzfwRMeYE8OT7Ly8WPnMYfm0ac+/8fKJBNJFj3utTWP+/utefQpUJJ8MclLh2FYkkef4OxO8s+Pjw/DcFKSzyR5vbEDVJp4ut8A8D/ST5IcMQzDxM+MnnuSHPm4fz4iySNJ7sujT3ie0DiOG4dh+Mcky/PoH0z+7Pi4R8zDMLwgjz5Veus4jjfWfBgAj/KEB9iTm5Pcm+SPh2E4cBiG2cMwvDzJ3yY5fxiGo4dhmJtkRZLP7eFJ0BP5TJL/J8nrH/v7JMkwDCck+VqSd4/jeG3lBwKQGDzAHozjuCvJGUmenUf/rM3qPPpk5pokf5Pk63n0z+tsS/LuaaS/muQXk6wZx/E/H/f6/5tkYZK/Gobhwcf+8oeWgTL+0DIA0J4nPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAexPTOXjGjBnjjBl1G2n37t1lrSQZx7GsNTk5WdZKkiOPPLK09+1vf7ustXv37uzevXsoCz6JycnJ8YgjjijrPfjgg2WtJBmGul+G++67r6yVJJVfe0nyyCOPlLW2bduWHTt27JNrKEn233//ce7cuZW9slaS7Nixo7RXqfIaT2rvbXfffXfWrVu3T66jgw46aFy6dGlZb2JiWt9Op1T5ebr//vvLWkkyb9680l71ve173/veunEcF/7s69MdPFmwYEHZm9q8eXNZK6m9gb/uda8rayXJxz/+8dLeM5/5zLJW9efhyRxxxBH5l3/5l7LeN77xjbJWUnuT+fM///OyVpLMmjWrtLdu3bqy1i233FLW2htz587NaaedVtY75phjylpJsmrVqrLWrl27ylpJ/XV01VVXlbWWLVtW1prK0qVL88UvfrGst3Dhf/n++t9SeS+6+uqry1pJcvLJJ5f2Zs+eXdr75V/+5ZV7et2PtACA9gweAKA9gwcAaM/gAQDaM3gAgPYMHgCgPYMHAGjP4AEA2jN4AID2DB4AoD2DBwBoz+ABANozeACA9gweAKA9gwcAaM/gAQDaM3gAgPYmpnPwrl27sn79+rKTf/Ob3yxrJckb3vCGstaiRYvKWklyxBFHlPbWrVtX1nrJS15S1prKvffem4svvrisN2NG7WbfsmVLWevmm28uayXJ29/+9tLe2rVry1rf+c53ylp7Y/fu3dmxY0dZb+vWrWWtJHnWs55V1rr99tvLWklyyimnlPauvfbastYDDzxQ1prKxMREJicny3qVvw5JcvXVV5e1Tj311LJWkhx00EGlvUsuuaS090Q84QEA2jN4AID2DB4AoD2DBwBoz+ABANozeACA9gweAKA9gwcAaM/gAQDaM3gAgPYMHgCgPYMHAGjP4AEA2jN4AID2DB4AoD2DBwBoz+ABANozeACA9gweAKC9iekcfMghh2T58uVlJ//whz9c1kqSbdu2lbW+973vlbWSZObMmaW9ww47rKy1bt26stZU1qxZkz/5kz8p682ZM6eslSSvfvWry1qV12OSXHzxxaW9V73qVWWt7du3l7X2xjiO2blzZ1nvxz/+cVkrSY466qiy1otf/OKyVpL867/+a2nv7LPPLmtV3yefzF133ZVzzjmnrPe1r32trJUkGzduLGtt3ry5rJUkO3bsKO1NTk6W9p6IJzwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0N7EdA6eM2dOXvziF5ed/LzzzitrJckv/dIvlfYqVX+sk5OTZa1LL720rDWVE044IV/+8pfLeq997WvLWknyh3/4h2WtJUuWlLWS5I1vfGNp7/vf/35Za9u2bWWtvTFz5szMmzevrPeDH/ygrJUkhx12WFnrRz/6UVkrSe69997S3vXXX1/W2rx5c1lrKosXL8773ve+st4FF1xQ1qq2e/fu0t7s2bNLe+985ztLe3/2Z3+2x9c94QEA2jN4AID2DB4AoD2DBwBoz+ABANozeACA9gweAKA9gwcAaM/gAQDaM3gAgPYMHgCgPYMHAGjP4AEA2jN4AID2DB4AoD2DBwBoz+ABANozeACA9gweAKC9iekc/NBDD+Xf/u3fyk7+/Oc/v6yVJMccc0xZa8aM2i34oQ99qLR31llnlbUeeuihstZUvv/97+clL3lJWe+rX/1qWStJTj/99LLWHXfcUdZKkre97W2lvcnJybLWxo0by1p7Y8OGDfn0pz9d1nvVq15V1kqS7du3l7W2bNlS1kqSgw46qLT39re/vaz1la98paw1lR07dmT16tVlvQULFpS1kuTBBx8saz3wwANlrST5yEc+UtqbO3duae+JeMIDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAexPTOXjHjh1ZtWpV2ck3bNhQ1kqS448/vqx1++23l7WS5NBDDy3tzZjx87lVh2HIzJkzy3q33nprWStJTj311LLWihUrylpJ/ef8k5/8ZFnrnHPOKWvtjfnz5+flL395We/ggw8uayXJ5s2by1r77bdfWStJVq5cWdp7//vfX9b66U9/Wtaayt13353zzjuvrHfBBReUtZLk4YcfLmsde+yxZa0kufDCC0t7r3/960t7T+Tn87smAMA0GDwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0N4wjuPeHzwM9ydZ+dS9HZ4mR47juHBfnMg11NY+u4YS11Fj7kVU2ON1NK3BAwDw88iPtACA9gweAKA9gwcAaM/gAQDaM3gAgPYMHgCgPYMHAGjP4AEA2jN4AID2DB4AoD2DBwBoz+ABANozeACA9gweAKA9gwcAaM/gAQDaM3gAgPYMHgCgPYMHAGjP4AEA2jN4AID2DB4AoL2J6Rw8DMM4Y0bdRjr66KPLWknywAMPlLW2b99e1kqSQw45pLQ3Z86cstbq1auzYcOGoSz4JGbNmjXOnj27rFf565Akle9t5syZZa2k9vpOkgMPPLCstX79+jz44IP75BpK6q+jajt37vwf2UqSXbt2lfaqjeO4T66jBQsWjIsXLy7rPfLII2WtpPZ70KJFi8paSXLHHXeU9g466KDS3urVq9eN47jwZ1+f1uCZMWNG6U3ysssuK2slyXXXXVfWuvvuu8taSfLbv/3bpb1ly5aVtU4//fSy1lRmz56dl7zkJWW9F73oRWWtJDn22GPLWvPnzy9rJcm1115b2nvZy15W1lqxYkVZa2/Mnj279Gug8jdySfLTn/60rLVmzZqyVlI/nMdxLO3tK4sXL84111xT1lu7dm1ZK0nuuuuustb5559f1kqSk046qbT3hje8obR3wQUXrNzT636kBQC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtDcxnYMnJyfzpje9qezkW7ZsKWslyRvf+May1te//vWyVpK84hWvKO1NTk6W9vaVLVu25MYbbyzrLVmypKyVJN/61rfKWosWLSprJcnExLS+XKf0jne8o6x11VVXlbX2xjAMmTVrVlnvpz/9aVkrSbZu3VrWmj9/flkrSTZv3lzaG4ahrLV79+6y1lTGcczDDz9c1ps5c2ZZK0nOOOOMstbXvva1slaSvOY1ryntXXDBBaW9J+IJDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO1NTOfgTZs25Stf+UrZyU877bSyVpJ8/OMfL2udcMIJZa0kWbVqVWnv29/+dlnr7LPPLmtN5bDDDst73/vest6ZZ55Z1kqS3/qt3yprPeMZzyhrJckrX/nK0t6ll15a1lqzZk1Za2+M45hdu3aV9bZt21bWSpJ58+b9j2wlKf11S/b9577Kzp07s3bt2rLeL/7iL5a1kuSRRx4p7VU69dRTS3vvfve7S3vz58/f4+ue8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAexPTOXj37t156KGHyk7+ute9rqyVJGeddVZZa+XKlWWtJPnYxz5W2vv4xz9e1jrggAPKWlPZtm1b7rjjjrLet771rbJWkhx//PFlrQceeKCslSR//dd/Xdp7z3veU9aaM2dOWWtvzJgxo/Sc8+fPL2slycEHH1zWOuigg8paSTI5OVnaW716dVnrBz/4QVlrb+zatausddNNN5W1kuSQQw4pax111FFlrST5j//4j9Je9X38iXjCAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsT0zn46KOPzl/+5V+WnfyOO+4oayXJmjVrylo333xzWStJnvOc55T2VqxYUda69957y1pTefjhh3PbbbeV9U466aSyVpIceuihZa0rr7yyrJUkH/zgB0t7F1xwQVlr9erVZa29MWvWrNLP1cyZM8taSbJgwYKy1jOf+cyyVlL73pLkta99bVmr+hp/MrNnzy69L2/durWslSRXXHFFWevMM88sayXJ9u3bS3u33357ae+JeMIDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAexPTOfihhx7KzTffXHbyrVu3lrWS5MQTTyxrzZ49u6yVJKeddlpp76KLLiprbdmypaw1lW3btuVHP/pRWe81r3lNWStJrrjiirLW5z//+bLWU+HXfu3Xylr33XdfWWtv7L///jnmmGNKe5VmzZpV1lq0aFFZK0mWLVtW2vuVX/mVstaVV15Z1prK7Nmz85znPKesd8stt5S1kpR+r73hhhvKWkly2WWXlfZe9KIXlfauvfbaPb7uCQ8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtDeM47v3Bw3B/kpVP3dvhaXLkOI4L98WJXENt7bNrKHEdNeZeRIU9XkfTGjwAAD+P/EgLAGjP4AEA2jN4AID2DB4AoD2DBwBoz+ABANozeACA9gweAKA9gwcAaM/gAQDaM3gAgPYMHgCgPYMHAGjP4AEA2jN4AID2DB4AoD2DBwBoz+ABANozeACA9gweAKA9gwcAaG9iOgfPmzdvXLhwYdnJt23bVtZKknXr1pW1jjvuuLJWkuzevbu0V/mxPvDAA9m6detQFnwSk5OT41FHHVXWu+uuu8paSbJ9+/ay1rx588paSTJ//vzS3tatW8taGzZsyEMPPbRPrqEkWbBgwbh48eKy3oYNG8paSf3Xe6X169eX9p7znOeUte69995s2rRpn1xH8+bNGycnJ8t6q1atKmslSeV9chzHslaSbNq0qbS3cePG0l6SdeM4/pexMq3Bs3DhwqxYsaLsHX3ve98rayXJNddcU9a67rrrylpJ8vDDD5f2rrrqqrLWpz71qbLWVI466qjcfPPNZb3zzjuvrJUkP/7xj8tav/qrv1rWSpJTTjmltHfrrbeWtT784Q+XtfbG4sWLc/XVV5f1PvOZz5S1kvrfzFX6xCc+UdqrvO++9a1vLWtNZXJyMn/wB39Q1nv3u99d1kpS+r12165dZa0k+epXv1ra+9znPlfaS7JyTy/6kRYA0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANDexHQO3rVrVzZt2lR28rVr15a1kmT16tVlrd/7vd8rayXJL/zCL5T2li5dWtbab7/9ylpT2bFjR1atWlXW++AHP1jWSpLvfve7Za1vfOMbZa0kWbZsWWnvla98ZWlvX1qzZk0uv/zyst75559f1kqSCy+8sKx16aWXlrWSZPfu3aW9T37yk2WtdevWlbWmsnv37mzbtq2sd9lll5W1kuRLX/pSWevMM88saz0VxnEs7Q3DsMfXPeEBANozeACA9gweAKA9gwcAaM/gAQDaM3gAgPYMHgCgPYMHAGjP4AEA2jN4AID2DB4AoD2DBwBoz+ABANozeACA9gweAKA9gwcAaM/gAQDaM3gAgPYMHgCgvYnpHLxt27b88Ic/fKrey3/be9/73rLWueeeW9ZKkjvvvLO098IXvrCs9bGPfaysNZUNGzbks5/9bFnvsMMOK2slyfXXX1/W2rRpU1krST760Y+W9pYvX17W+sd//Mey1t4YhiETE9O6fT2pb37zm2WtJPnSl75U1nrb295W1kqSRYsWlfbe8pa3lLW+/vWvl7WmsmXLlvzTP/3TPjvfdJ1yyillrXe84x1lrST59V//9dLehRdeWNp7Ip7wAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7E9M5eBiGzJw5s+zkc+fOLWslyRFHHFHW+tCHPlTWSpLJycnS3o033ljWuv/++8taU9m5c2fuueeest7SpUvLWkmya9eusta8efPKWkly4YUXlvZmz55d1tq5c2dZa28sWbIk73//+8t6l1xySVkrSV7wgheUtV796leXtZKU3sOT5IUvfGFZa86cOWWtqRxwwAF53vOeV9arvsdff/31Za3LLrusrJUka9euLe3deeedpb0n4gkPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7U1M5+D7778/H/vYx8pOvmLFirJWktx6661lrYMPPrislaT01y1JbrnllrLWTTfdVNaayuGHH56PfOQjZb3vfve7Za0kWbt2bVnr4osvLmslyTnnnFPa+43f+I2y1j/8wz+UtfbG1q1b8+1vf7usd/LJJ5e1kuQTn/hEWesVr3hFWStJjj766NLe/vvvX9YahqGsNZWJiYlMTk7us/M9na6++urS3rve9a7S3rOf/ezS3qc+9ak9vu4JDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0N4zju9cFLliwZzzvvvLKTH3rooWWtJJmYmChr3XbbbWWtJJkzZ05p72//9m/LWuvWrcvOnTuHsuCTOPzww8f3vOc9Zb0vfOELZa0ked/73lfaq/SWt7yltHfGGWeUta677rqsX79+n1xDSXLCCSeMf/d3f1fWO/vss8taSbJkyZKy1saNG8taSXLiiSeW9oah7tP+mc98Jvfdd98+uY6WLl06vutd7yrrTU5OlrWSR+/LVTZs2FDWSpLVq1eX9q677rrS3oMPPnjrOI7LfvZ1T3gAgPYMHgCgPYMHAGjP4AEA2jN4AID2DB4AoD2DBwBoz+ABANozeACA9gweAKA9gwcAaM/gAQDaM3gAgPYMHgCgPYMHAGjP4AEA2jN4AID2DB4AoD2DBwBobxjHce8PHob7k6x86t4OT5Mjx3FcuC9O5Bpqa59dQ4nrqDH3Iirs8Tqa1uABAPh55EdaAEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQ3sR0Dp4/f/64cOHCspOvX7++rJUkS5YsKWs99NBDZa0kWbNmTWnvec97Xlnr7rvvzrp164ay4JPYf//9xwMPPLCst3Tp0rJWkmzevLmstWrVqrLWU2Hx4sVlrQceeCBbt27dJ9dQksyZM2dcsGBBZa+slSSzZ88ua23atKmslSQbN24s7R133HFlrX15L5o/f/64aNGist7WrVvLWkmyc+fOslb1ffKee+4p7c2dO7e0d+edd64bx/G/jJVpDZ6FCxfm0ksvLXtTn/jEJ8paSfKBD3ygrHXzzTeXtZLk8ssvL+1961vfKmu9+MUvLmtN5cADD8wpp5xS1rvkkkvKWklyww03lLV+53d+p6yVJDNnziztve1tbytr/dVf/VVZa28sWLAgv/mbv1nWe+ELX1jWSpJjjjmmrPX3f//3Za0k+eIXv1jaq7wXLVu2rKw1lUWLFuWKK64o6/3nf/5nWSupHRUf+tCHylpJ8vu///ulvZe97GWlvTe+8Y0r9/S6H2kBAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtTUzn4FWrVuWd73xn2cmf//znl7WS5MYbbyxrrVq1qqyVJF/4whdKe8uXLy9r3XnnnWWtqezevTvbtm0r6/3RH/1RWStJTj/99LLWMAxlrSSZMaP29yeV19CXv/zlstbeGMcxO3bsKOutWLGirJUkBx98cFmr8vOUJGeeeWZpr/LzMI5jWWsqW7duzXe+852y3pIlS8paSTJ37tyy1kUXXVTWeirsq8+7JzwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0NzGdg5/97GfnmmuuKTv5xo0by1pJcvnll5e1nve855W1kuSkk04q7W3fvr2s9dKXvrSsNZUZM2Zk1qxZZb399tuvrJUky5cvL2tNTEzry2tKv/u7v1vau/LKK8ta999/f1lrb+y3335ZunTpPj3ndBx77LFlrYMPPrislSRf/vKXS3vvec97ylo/+clPylpTOfjgg/PmN7+5rPe5z32urJUkF198cVnr3HPPLWslyZFHHlnae+SRR0p7T8QTHgCgPYMHAGjP4AEA2jN4AID2DB4AoD2DBwBoz+ABANozeACA9gweAKA9gwcAaM/gAQDaM3gAgPYMHgCgPYMHAGjP4AEA2jN4AID2DB4AoD2DBwBob2I6B+/YsSP33HNP2ckXLVpU1kqSP/7jPy5rzZs3r6yVJOeee25pbxzH0t6+MmfOnCxbtqysd9ppp5W1ktrP+1/8xV+UtZJkxYoVpb2VK1eWtb7xjW+UtfbG9u3b88Mf/rCs96xnPauslST//u//XtY68cQTy1rJo792lc4444yy1vXXX1/WmsrGjRvz+c9/vqxXfQ2dfvrpZa2TTz65rJUk++23X2lv586dpb0n4gkPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7U1M5+AdO3bkJz/5SdnJDzzwwLJWkhxxxBFlreOOO66slSRvetObSnv//M//XNZau3ZtWWsqCxcuzDve8Y6y3vnnn1/WSpJ169aVtYZhKGslyYwZtb8/ufDCC8taq1evLmvtjS1btuSmm24q6912221lrSQ5+eSTy1qHHHJIWStJdu3aVdr70z/907LWfffdV9aayurVq3PBBReU9S666KKyVpIsX768rHXttdeWtZLkuc99bmnvAx/4QGnviXjCAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsT0zn44Ycfzm233VZ28spWkixYsKCsdeqpp5a1kmT58uWlvVtuuaWsNWPGvtu969evz9/8zd+U9cZxLGslyZvf/Oay1g033FDWSpKzzjqrtHfNNdeUtZYtW1bW2hsHHHBAjj/++LLeW9/61rJWkmzZsqWs9YIXvKCslSSLFy8u7e3cubOsNTExrW9J/y3DMGTWrFllveprqPIeX/m1niTPfe5zS3uf/vSnS3uve93r9vi6JzwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0Z/AAAO0ZPABAewYPANCewQMAtGfwAADtGTwAQHsGDwDQnsEDALRn8AAA7Rk8AEB7Bg8A0J7BAwC0N4zjuPcHD8P9SVY+dW+Hp8mR4zgu3Bcncg21tc+uocR11Jh7ERX2eB1Na/AAAPw88iMtAKA9gwcAaM/gAQDaM3gAgPYMHgCgPYMHAGjP4AEA2jN4AID2DB4AoL3/H7+/VJe3RdtsAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(lane_keeper.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 2, 4, 'conv0', size=(10,5))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LaneKeeper(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Conv2d(8, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(4, 4, kernel_size=(6, 6), stride=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=16, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "lane_keeper.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "lane_keeper.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "lane_keeper.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(lane_keeper, dummy_input, onnx_lane_keeper_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lane_keeper.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 6776.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[-0.00158071  0.01025109  0.00258277]]\n",
      "Predictions shape: (1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_lane_keeper_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
