{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "# NOTE : Sign detecttion uses the folder training_images to load the backgrounds\n",
    "# it uses the folder sign_imgs to load the signs, the rate of examples is important\n",
    "num_channels = 3 # COLOR IMGS\n",
    "SIZE = (16, 16)\n",
    "model_name = 'models/sign_classifier.pt'\n",
    "onnx_sign_classifier_path = \"models/sign_classifier.onnx\"\n",
    "max_load = 5_000 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD EXAMPLES\n",
    "# imgs of the examples must be in a specific folder (ex: sign_imgs), and must be named as \"class_<number>.png\"\n",
    "# ex: sign_imgs/stop_1.png, note: start from 1\n",
    "\n",
    "#load examples\n",
    "examples_folder = 'sign_imgs'     \n",
    "class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway', 'nosign']\n",
    "# class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway']\n",
    "\n",
    "file_names = [f for f in os.listdir(examples_folder) if f.endswith('.png')]\n",
    "example_labels = [class_names.index(name.split('_')[0]) for name in file_names if name.split('_')[0] in class_names]\n",
    "example_imgs = [cv.resize(cv.imread(os.path.join(examples_folder, name)), (128,128)) for name in file_names if name.split('_')[0] in class_names]\n",
    "tot_examples = len(example_imgs)\n",
    "tot_classes = len(class_names) \n",
    "print(f'{tot_examples} examples loaded for {tot_classes} class_names')\n",
    "print(f'example labels: {example_labels}')    \n",
    "\n",
    "#show images\n",
    "cv.namedWindow('example', cv.WINDOW_NORMAL)\n",
    "for i in range(tot_examples):\n",
    "    img = example_imgs[i].copy()\n",
    "    # add text label\n",
    "    cv.putText(img, class_names[example_labels[i]], (10,30), cv.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "    cv.imshow('example', img)\n",
    "    key = cv.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE 32x32\n",
    "\n",
    "# class SignClassifier(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=num_channels): \n",
    "#         super().__init__()\n",
    "#         p = 0.2\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=3, stride=1), #out = 12 - 28  \n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=10 - 14\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.Conv2d(8, 8, kernel_size=4, stride=2), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=5\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.Conv2d(8, 32, kernel_size=5, stride=1), #out = 1\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             # nn.Linear(in_features=4*4*4, out_features=128),\n",
    "#             # nn.ReLU(True),\n",
    "#             # nn.Dropout(p),\n",
    "#             # nn.Linear(in_features=128, out_features=out_dim),\n",
    "#             nn.Linear(in_features=32*1*1, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# sign_classifier = SignClassifier(out_dim=tot_classes,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE 16x16\n",
    "\n",
    "class SignClassifier(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=num_channels): \n",
    "        super().__init__()\n",
    "        p = 0.3\n",
    "        ### Convoluational layers\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 12\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1), #out=10\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(p),\n",
    "            nn.Conv2d(8, 16, kernel_size=5, stride=1), #out = 6\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1), #out=5\n",
    "            # nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p),\n",
    "            nn.Conv2d(16, 128, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            # nn.Linear(in_features=4*4*4, out_features=128),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.Dropout(p),\n",
    "            # nn.Linear(in_features=128, out_features=out_dim),\n",
    "            nn.Linear(in_features=128*1*1, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "sign_classifier = SignClassifier(out_dim=tot_classes,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "sign_classifier.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = sign_classifier(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "\n",
    "def draw_box(img, x, y, w):\n",
    "    img = cv.rectangle(img, (int(x-w/2), int(y-w/2)), (int(x+w/2), int(y+w/2)), 255, 2)\n",
    "    return img\n",
    "\n",
    "def load_and_augment_img(img, example_index=0, example_imgs=example_imgs):\n",
    "    # img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "    if img is not None:\n",
    "        #convert to gray\n",
    "        # img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "        # #crop to the top right quadrant\n",
    "        img = img[0:img.shape[1]//2, img.shape[1]//2:]\n",
    "        canv_dim = randint(64//2, 90//2)  ## RANGE OF DIMENSION OF THE SIGN\n",
    "        start_x = randint(0, img.shape[1]-canv_dim)\n",
    "        start_y = randint(0, img.shape[0]-canv_dim)\n",
    "        img = img[start_y:start_y+canv_dim, start_x:start_x+canv_dim]\n",
    "        # img = cv.resize(img, (2*SIZE[0], 2*SIZE[1]))\n",
    "    else:\n",
    "        img = randint(0,255,(2*SIZE[0], 2*SIZE[1]), dtype=np.uint8)\n",
    "\n",
    "    ## EXAMPLE AUGMENTATION ##############################################################\n",
    "    #load example\n",
    "    example = example_imgs[example_index]\n",
    "    resize_ratio = max(img.shape)/max(example.shape)\n",
    "    example = cv.resize(example, (int(example.shape[1]*resize_ratio), int(example.shape[0]*resize_ratio)))\n",
    "\n",
    "    #get example mask\n",
    "    example_mask = np.where(example == np.array([0,0,0]), np.zeros_like(example), 255*np.ones_like(example))\n",
    "    #blur the example\n",
    "    # example = cv.blur(example, (randint(3,9),randint(3,9)))\n",
    "\n",
    "    # add noise to the example\n",
    "    std = 20\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.subtract(example, noisem)\n",
    "    noisep = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.add(example, noisep)\n",
    "\n",
    "    #set zero where example mask is zero\n",
    "    example = np.where(example_mask == np.array([0,0,0]), np.zeros_like(example), example)\n",
    "    # cv.imshow('test', example)\n",
    "\n",
    "    #perspective transform example\n",
    "    perspective_deformation = img.shape[0]//10\n",
    "    pts1 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = pts2 + np.float32(randint(0,perspective_deformation,size=pts2.shape))\n",
    "    # print(f'pts2 = \\n{pts2}')\n",
    "    new_size_x = int(np.max(pts2[:,0]) - np.min(pts2[:,0]))\n",
    "    new_size_y = int(np.max(pts2[:,1]) - np.min(pts2[:,1]))\n",
    "    M = cv.getPerspectiveTransform(pts1,pts2)\n",
    "    example = cv.warpPerspective(example,M,(new_size_x,new_size_y))\n",
    "\n",
    "    #resize example keeping proportions\n",
    "    img_example_ratio = min(img.shape[0]/example.shape[0], img.shape[1]/example.shape[1])\n",
    "    scale_factor = np.random.uniform(.6, .98) ##.15, .35############################ PARAM ##############################\n",
    "    scale_factor = scale_factor * img_example_ratio\n",
    "    example = cv.resize(example, (0,0), fx=scale_factor, fy=scale_factor)\n",
    "    #match img shape\n",
    "    example_canvas = np.zeros((img.shape[0], img.shape[1], num_channels), dtype=np.uint8)\n",
    "\n",
    "    #get a random position for the example\n",
    "    example_y = randint(0, img.shape[0] - example.shape[0])\n",
    "    example_x = randint(0, img.shape[1] - example.shape[1])\n",
    "    #paste example on canvas\n",
    "    example_canvas[example_y:example_y+example.shape[0], example_x:example_x+example.shape[1]] = example\n",
    "\n",
    "    old_example_canvas = example_canvas.copy()\n",
    "\n",
    "    #convert to hsv\n",
    "    example_canvas = cv.cvtColor(example_canvas, cv.COLOR_BGR2HSV)\n",
    "    #get the hsv channels\n",
    "    example_canvas_h = example_canvas[:,:,0]\n",
    "    example_canvas_s = example_canvas[:,:,1]\n",
    "    example_canvas_v = example_canvas[:,:,2]\n",
    "\n",
    "    # #reduce brightness\n",
    "    brightness_shift = randint(-80,5)\n",
    "    # example_canvas_v = np.clip(example_canvas_v + brightness_shift, 0, 255).astype(np.uint8)\n",
    "    if brightness_shift > 0:\n",
    "        example_canvas_v = cv.add(example_canvas_v, np.ones_like(example_canvas_v)*brightness_shift)\n",
    "    elif brightness_shift < 0:\n",
    "        example_canvas_v = cv.subtract(example_canvas_v, np.ones_like(example_canvas_v)*abs(brightness_shift))\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.25,.9)\n",
    "    example_canvas_v = np.clip(127*(1-const) + example_canvas_v*const, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #augment hue\n",
    "    hue_shift = randint(-7,7)\n",
    "    example_canvas_h = (example_canvas_h + hue_shift) % 180\n",
    "\n",
    "    #augment saturation\n",
    "    sat_shift = randint(-100,0)\n",
    "    example_canvas_s = np.clip(example_canvas_s + sat_shift, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #rebuild the channels\n",
    "    example_canvas[:,:,0] = example_canvas_h\n",
    "    example_canvas[:,:,1] = example_canvas_s\n",
    "    example_canvas[:,:,2] = example_canvas_v\n",
    "    #back to bgr\n",
    "    example_canvas = cv.cvtColor(example_canvas, cv.COLOR_HSV2BGR)\n",
    "\n",
    "    #paste canvas on img\n",
    "    old_example_canvas = cv.cvtColor(old_example_canvas, cv.COLOR_BGR2GRAY)\n",
    "    img_b = img[:,:,0]\n",
    "    img_g = img[:,:,1]\n",
    "    img_r = img[:,:,2]\n",
    "    example_b = example_canvas[:,:,0]\n",
    "    example_g = example_canvas[:,:,1]\n",
    "    example_r = example_canvas[:,:,2]\n",
    "    img_b = np.where(old_example_canvas > 0, example_b, img_b)\n",
    "    img_g = np.where(old_example_canvas > 0, example_g, img_g)\n",
    "    img_r = np.where(old_example_canvas > 0, example_r, img_r)\n",
    "    # img = np.where(old_example_canvas > 0, example_canvas, img) \n",
    "    img[:,:,0] = img_b\n",
    "    img[:,:,1] = img_g\n",
    "    img[:,:,2] = img_r\n",
    "\n",
    "    # # blur example\n",
    "    # b = randint(2,5)\n",
    "    # example_canvas = cv.blur(example_canvas, (b,b))\n",
    "\n",
    "    ##########################################################################################\n",
    "    \n",
    "    # convert whole img to hsv\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "    #get the hsv channels\n",
    "    img_h = img[:,:,0]\n",
    "    img_s = img[:,:,1]\n",
    "    img_v = img[:,:,2]\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.7,.99)\n",
    "    img_v = np.clip(127*(1-const) + img_v*const, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #reduce brightness\n",
    "    brightness_shift = randint(-30,0)\n",
    "    if brightness_shift > 0:\n",
    "        img_v = cv.add(img_v, np.ones_like(img_v)*brightness_shift)\n",
    "    elif brightness_shift < 0:\n",
    "        img_v = cv.subtract(img_v, np.ones_like(img_v)*abs(brightness_shift))\n",
    "\n",
    "    #augment hue\n",
    "    hue_shift = randint(-2,2)\n",
    "    img_h = (img_h + hue_shift) % 180\n",
    "\n",
    "    #augment saturation\n",
    "    sat_shift = randint(-20,0)\n",
    "    img_s = np.clip(img_s + sat_shift, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #rebuild the channels\n",
    "    img[:,:,0] = img_h\n",
    "    img[:,:,1] = img_s\n",
    "    img[:,:,2] = img_v\n",
    "    #back to bgr\n",
    "    img = cv.cvtColor(img, cv.COLOR_HSV2BGR)\n",
    "\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = img.shape[0]//5\n",
    "    offset = np.random.randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        # img[:offset, :] = np.random.randint(0,255)\n",
    "        img = img[offset:, :]\n",
    "    elif offset < 0:\n",
    "        # img[offset:, :] = np.random.randint(0,255)\n",
    "        img = img[:offset, :]\n",
    "\n",
    "    offset_y = np.random.randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset_y, axis=1)\n",
    "    if offset_y > 0:\n",
    "        # img[:, :offset_y] = np.random.randint(0,255)\n",
    "        img = img[:, offset_y:]\n",
    "    elif offset_y < 0:\n",
    "        # img[:, offset_y:] = np.random.randint(0,255)\n",
    "        img = img[:, :offset_y]\n",
    "\n",
    "    min_dim = min(img.shape[0], img.shape[1])\n",
    "    #crop to square\n",
    "    img = img[:min_dim, :min_dim]\n",
    "\n",
    "    #add noise\n",
    "    std = 50\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "\n",
    "    # # crop into the img at random position\n",
    "    # zoom = randint(0, SIZE[0]//4)\n",
    "    # img = img[zoom:img.shape[0]-zoom, zoom:img.shape[1]-zoom]\n",
    "\n",
    "    # #blur \n",
    "    # b = randint(2,5)\n",
    "    # img = cv.blur(img, (b,b))\n",
    "    \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    #convert to hsv\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = cv.resize(img, (160,120))\n",
    "    # img = None\n",
    "    img = load_and_augment_img(img, example_index=(i%tot_examples))\n",
    "\n",
    "    #to bgr\n",
    "    img = cv.cvtColor(img, cv.COLOR_HSV2BGR)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(200)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.class_names = []\n",
    "        self.channels = channels\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            # self.all_imgs = torch.zeros((2*max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8) #adding flipped img\n",
    "            self.all_imgs = torch.zeros((max_load*tot_examples, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN\n",
    "\n",
    "            for i in tqdm(range(max_load)):\n",
    "                img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "                img = cv.resize(img, (160,120))\n",
    "                for j in range(tot_examples):\n",
    "                    img_j = load_and_augment_img(img.copy(), example_index=j)\n",
    "                    if i < 100:\n",
    "                        cv.imshow('img', img_j)\n",
    "                        cv.waitKey(1)\n",
    "                        if i == 99:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img_j = img_j[:, :,np.newaxis] if self.channels == 1 else img_j\n",
    "                    #convert to tensor\n",
    "                    img_j = torch.from_numpy(img_j)\n",
    "                    self.all_imgs[i*tot_examples+j] = img_j\n",
    "                    self.class_names.append(example_labels[j])\n",
    "            \n",
    "            self.data = torch.from_numpy(np.array(self.data))\n",
    "            self.class_names = torch.from_numpy(np.array(self.class_names))\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'class_names: {self.class_names.shape}')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.class_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        class_label = self.class_names[idx]\n",
    "        return img, class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [02:34<00:00, 32.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all imgs: torch.Size([75000, 16, 16, 3])\n",
      "class_names: torch.Size([75000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4*8192//3, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8192//3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10922, 3, 16, 16])\n",
      "torch.Size([10922])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, class_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    class_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, class_label) in tqdm(dataloader):\n",
    "        #one hot encode the class label\n",
    "        class_label = torch.eye(tot_classes)[class_label]\n",
    "        # Move the input and target data to the selected device\n",
    "        input, class_label = input.to(device), class_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        class_out = output[:, 0:]\n",
    "\n",
    "        #classification loss\n",
    "        assert class_label.shape == class_out.shape, f'class_label.shape: {class_label.shape}, output.shape: {output.shape}'\n",
    "\n",
    "        class_loss = 1.0*class_loss_fn(class_out, class_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = class_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        class_losses.append(class_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    class_loss = np.mean(class_losses)\n",
    "    return  np.sqrt(class_loss)\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(sign_classifier, val_dataloader, regr_loss_fn, class_loss_fn, device=device):\n",
    "    sign_classifier.eval()\n",
    "    y_losses = []\n",
    "    x_losses = []\n",
    "    w_losses = []\n",
    "    class_losses = []\n",
    "    for (input, class_label) in tqdm(val_dataloader):\n",
    "        #one hot encode the class label\n",
    "        class_label = torch.eye(tot_classes)[class_label]\n",
    "        input, class_label = input.to(device), class_label.to(device)\n",
    "        output = sign_classifier(input)\n",
    "\n",
    "        class_loss = 1.0*class_loss_fn(output[:, 0:], class_label)\n",
    "    \n",
    "        class_losses.append(class_loss.detach().cpu().numpy())\n",
    "    return  np.mean(class_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  150/150\n",
      "class_loss: 0.1133 --- Val: 0.0198\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.005 #0.005\n",
    "epochs = 150 #50+\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 0*1e-4 #9e-4\n",
    "L2_lambda = 0*1e-3 #1e-2\n",
    "optimizer = torch.optim.Adam(sign_classifier.parameters(), lr=lr, weight_decay=3e-5) #wd = 2e-3# 9e-5\n",
    "\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "class_loss_fn = nn.CrossEntropyLoss()\n",
    "best_val = 100\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        class_loss = train_epoch(sign_classifier, train_dataloader, regr_loss_fn, class_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_class_loss = val_epoch(sign_classifier, val_dataloader, regr_loss_fn, class_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs}\")\n",
    "    print(f\"class_loss: {class_loss:.4f} --- Val: {val_class_loss:.4f}\")\n",
    "    if val_class_loss < best_val:\n",
    "        torch.save(sign_classifier.state_dict(), model_name)\n",
    "        print(f'Model saved as {model_name}')\n",
    "        best_val = val_class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "# val_dataloader2 = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "# sign_classifier.load_state_dict(torch.load(model_name))\n",
    "# sign_classifier.eval()\n",
    "# for (input, class_label) in tqdm(val_dataloader2):\n",
    "#     #convert img to numpy array\n",
    "#     img = input[0].detach().cpu().numpy()[0]\n",
    "#     #convert to sigle byte\n",
    "#     img = img.astype(np.uint8)\n",
    "#     input, box_label, class_label =input.to(device), box_label.to(device), class_label.to(device)\n",
    "#     output = sign_classifier(input)\n",
    "#     x = output[:, 0]\n",
    "#     y = output[:, 1]\n",
    "#     w = output[:, 2]\n",
    "#     class_out = output[:, 3:]\n",
    "#     # print(class_out)\n",
    "#     class_out = torch.argmax(class_out, dim=1)\n",
    "#     class_out = class_out.detach().cpu().numpy()\n",
    "#     # print(f\"Predicted class: {class_out[0]}, Actual class: {class_label[0]}\")\n",
    "#     class_out = class_out[0]\n",
    "#     class_out = class_names[class_out]\n",
    "#     true_class = class_names[class_label[0]]\n",
    "#     img = draw_box(img, x, y, w)\n",
    "#     #text for labels\n",
    "#     img = cv.putText(img, f\"True: {true_class}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "#     img = cv.putText(img, f\"Pred: {class_out}\", (10, 60), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "#     cv.imshow('img', img)\n",
    "#     key = cv.waitKey(0)\n",
    "#     if key == 27:\n",
    "#         break\n",
    "    \n",
    "# cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 3, 5, 5)\n",
      "(16, 8, 5, 5)\n",
      "(128, 16, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAThCAYAAAA1YTsWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAA22klEQVR4nO3ae6ynB33f+c8z5zJ3z92e8cx4bIPxRQZscCDIyYJlCKKFSgmhZBdps6lQ0qjdRGmbVCvopptWq22bpiUKm4RECSRtlw0mUQw1JG4Am7iLr9QU369zsRnbZzzjuZy5nTPP/oFTUdSZ47P4OyO++3pJSIzn8ef3zO885znveX4exnEMAEBHS871CQAAVBE6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDnDODMOwZRiGm4dheHYYhnEYhovP9TkBvQgd4Fw6leSLSd5/rk8E6EnoAP+VYRi2D8Pwx8MwvDAMw75hGH5jGIYlwzB8dBiGncMwPD8Mwx8Mw7Dm5eMvfvlpzE8Ow7BrGIaZYRg+8vLvXTgMw9FhGNZ/x/61Lx8zNY7jc+M4/p9J7j5Hf1ygOaED/BfDMEwk+XySnUkuTrI1yaeT/E8v/++GJJcmWZXkN77rX/+hJJcnuTHJ/zoMw5XjOD6b5P/Jf/3E5n9IctM4jier/hwAf0XoAN/pLUkuTPKL4zgeGcfx2DiOf5nkQ0l+bRzHJ8dxPJzkf0nyE8MwTH7Hv/u/jeN4dBzH+5Pcn+SNL//zf5fkv0+SYRiGJD/x8j8DKCd0gO+0PcnOcRznvuufX5hvP+X5KzuTTCa54Dv+2d7v+P+z+fZTnyT5bJK3DcOwJcl/l2//dzlffTVPGuB0Jhc+BPj/kd1JLhqGYfK7YufZJDu+49cXJZlL8lySbWcaHMdx/zAMf57kg0muTPLpcRzHV/e0Af7bPNEBvtNdSb6V5P8YhmHlMAzLhmG4Psn/leQXhmG4ZBiGVUn+9yT/93/jyc/p/Lsk/2OSH893fWw1DMOyJEtf/uXSl38N8KoQOsB/MY7jfJL3JXltkl1J9uTbT2J+L8kfJrk9yVNJjiX5nxcxfXOSy5Lsffm/4flOR5Mcfvn/P/zyrwFeFYMnyABAV57oAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0NbkYg4ehmGsOpHp6emq6SxbtqxsO0nGsextydGjR0t25+fnc+rUqaFk/AyWLl06rly58my/7PdsyZLavxNs2rSpbPvw4cNl23v27JkZx7Hu5E9j1apV47p16872y37Pjh07VrpfeR2tWLGiZPfpp5/OzMzMWb8XrV+/fty2bVvJ9qlTp0p2q7eTZHJyUVmwKJU/5++9997T3ovq/kSLVHXBJcnrXve6su0kOX78eNn2Qw89VLI7MzNTsruQlStX5sYbbyzZ/n79Bk2Sn/3Zny3bvuOOO8q2/8E/+Ac7y8bPYN26dfn7f//vn4uX/p5UfT//lZ/5mZ8p237jG99YsvvWt761ZHch27Zty+c+97mS7SNHjpTsJt/fsbx9+/ay7WEYTnsv8tEVANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG1NLubg9evX593vfnfJiVxzzTUlu9XbSbJ79+6y7X//7/99ye6Xv/zlkt2FnDx5MjMzMyXbV155Zclukpx33nll20nK3pMkefOb31y2fa5MT09n+/btJduXXXZZyW6SXHHFFWXbSfKmN72pbHtubq5kdxzHkt2F7Nu3L3/4h39Ysn3ixImS3SSZmpoq206SHTt2lG0vWXJunq14ogMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhrcjEHr1y5Mm9961tLTuTtb397yW6SLF26tGw7SU6dOlW2/Za3vKVk96677irZXcjU1FQ2btxYsr19+/aS3SR585vfXLadJMMwlG2/4x3vKNs+V8ZxzPHjx0u2H3nkkZLdJGXn/Fc+//nPl23fdNNNJbs7d+4s2V3I0aNH88ADD5RsX3nllSW7SXLttdeWbSfJ7bffXrZd9fNsIZ7oAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbk4s5eGpqKhdeeGHJiWzZsqVkN0kmJibKtpNkbm6ubHsYhpLdFStWlOwuZBiGLF26tGT7yJEjJbtJctNNN5VtJ8nll19etv0nf/InZdvnyvz8fA4fPlyyvW/fvpLdJLnlllvKtpPk6NGjZdszMzMlu4cOHSrZXcjJkyfz7LPPlmxv3bq1ZPdseP3rX1+2PTm5qOR41XiiAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaGtyMQdPTExk3bp1NScyuahTWZQNGzaUbSfJOI5l2xs3bizZXb58ecnuQk6cOJE9e/aUbD/++OMlu0myfv36su0kuemmm8q29+/fX7Z9rjz//PP5V//qX5VsV36t9+7dW7adJE8++WTZdtW9f35+vmR3IYcPH87tt99esn3eeeeV7CbJoUOHyraTZN++fWXbc3NzZdtn4okOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgrWEcx1d+8DC8kGRn3elwFu0Yx3HT2X5R11A7riO+V64hXg2nvY4WFToAAN9PfHQFALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFuTizl4GIax6kSWLVtWNZ3ly5eXbSfJihUryranpqZKdmdmZnLo0KGhZPwMlixZMk5OLuqye8VOnjxZsns2LF26tGy76hpKksOHD8+M47ip7AVOY+PGjeOOHTtKtofhrH9bvGqOHDlStn3q1KmS3b179+all14662/6hg0byq6hStX3ucp70ezsbNn2Qw89dNp7Uc1PnP8PXvva15ZtX3XVVWXbSXLdddeVbZ9//vklu//4H//jkt2FTE5OZuPGjSXbe/fuLdlNknEsa/wkyfbt28u2t23bVrb9la98ZWfZ+Bns2LEjd9xxR8l25V+6qmLhr9x1111l28ePHy/Z/emf/umS3YXs2LEjt912W8l2ZSw/99xzZdtJ8prXvKZs+5577inb/oEf+IHT3ot8dAUAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW5OLOXjp0qXZsWNHyYmsXbu2ZDdJTp48Wbad1J77Bz7wgZLdj33sYyW7CxmGIdPT0yXbl156aclukqxevbpsO0nZ91WSXH311WXbX/nKV8q2z2Qcx8zNzZVsf+tb3yrZTZJDhw6VbSfJkSNHyrYfffTRkt1jx46V7C5kdnY2X//610u23/CGN5TsJsmJEyfKtpPkwQcfLNuu/ll8Op7oAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2ppczMEnTpzI7t27S05kyZK65nr7299etp0ku3btKts+ePBgye78/HzJ7kKmpqaybdu2ku0DBw6U7CbJ3Nxc2Xb1/po1a8q2z5UjR47kzjvvLNm+6667SnaT5MUXXyzbTpI9e/aUbW/YsKFk9+jRoyW7Czl48GC+/OUvl2w/9thjJbtJcvHFF5dtJ8n27dvLth9++OGy7TPxRAcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2ppc1MGTk1m7dm3JiWzcuLFkN0kee+yxsu0k+Ymf+Imy7c2bN5fsTk1NlewuZMWKFbn22mtLti+++OKS3SR59NFHy7aTZPny5WXb69atK9s+V44ePZoHH3ywZPvZZ58t2U2SW265pWw7SYZhKNtevXp1ye7hw4dLdhcyOzube+65p2T7ueeeK9lNkj179pRtJ8n09HTZ9vz8fNn2mXiiAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaGtyMQcvW7YsV1xxRcmJvO997yvZTZLXvOY1ZdtJMo5j2fbNN99csnvgwIGS3YWsW7cuP/ZjP1ayfeWVV5bsJsnx48fLtpNkx44dZdvPPvts2fa5dOrUqZLdl156qWQ3Saampsq2k+SRRx4p256cXNSPi1dsbm6uZHchBw8ezH/4D/+hZPvYsWMlu0nygz/4g2Xb1R5++OFz8rqe6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoaxnF85QcPwwtJdtadDmfRjnEcN53tF3UNteM64nvlGuLVcNrraFGhAwDw/cRHVwBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0NbmYg4dhGKtOZMWKFVXTOe+888q2k2T58uVl2ydPnizZffHFF3PkyJGhZPwMJicnx+np6artkt0kWbZsWdl2kuzfv79se82aNWXb+/btmxnHcVPZC5zGihUrxqo/18TERMlukgxD7bfc7Oxs2XbV+3Lo0KEcPXr0rN+L1q1bN27durVk+9ixYyW71dtJ7X208mflww8/fNp7Ud2faJGuuOKKsu13v/vdZdtJ8vrXv75s+9lnny3Z/df/+l+X7C5keno6l19+ecn2hg0bSnaT2uszSf7oj/6obPu9731v2fbv//7v7ywbP4M1a9bkJ3/yJ8u2q1QH83333Ve2vWrVqpLdz372syW7C9m6dWs+85nPlGw//PDDJbtJ8vjjj5dtJ8natWvLtit/Vr7tbW877b3IR1cAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKCtycUcvG3btvz8z/98yYl8+MMfLtlNkp07d5ZtJ8m6devKtv/oj/6obPtcmJqaypYtW0q2165dW7KbJP/5P//nsu0kmZ6eLttesqTf32f27t2bf/7P/3nJ9oc+9KGS3ST5N//m35RtV7v88stLdg8ePFiyu5AlS5Zk9erVJduPPPJIyW6S3HHHHWXbSbJq1aqy7bm5ubLtM+l3BwQAeJnQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKCtycUcvGLFilx77bUlJzI3N1eymyRTU1Nl20nyl3/5l2XbVec+DEPJ7kKOHj2ab37zmyXb11xzTcluktx+++1l20ly3XXXlW1PTEyUbZ8rExMTWbNmTcn2rbfeWrKbJD/7sz9btp0k999/f9n2D//wD5fsfupTnyrZXcj09HS2bdtWsv3ggw+W7CbJ4cOHy7aTZBzHsu3HHnusbPtMPNEBANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLYmF3Pw1NRULrzwwpITmZiYKNlNkscee6xsO0kuuuiisu2vfvWrJbvHjh0r2V3IeeedlxtuuKFk+xvf+EbJbpJMTi7qW2XRfumXfqls++TJk2Xbn/jEJ8q2z2Qcx7I/1/XXX1+ymySzs7Nl20ny1FNPlW2vWrWqZLf6PTmd559/Pr/xG79Rsl31XiXJ3r17y7arnatz90QHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQ1uRiDl62bFmuvPLKkhP52te+VrKbJLOzs2XbSXLbbbeVbQ/DULZ9Llx88cX55Cc/WbJ98803l+wmyY/8yI+UbSfJ7bffXrb97ne/u2z7Qx/6UNn2maxZsybvete7SrZ/8Ad/sGQ3SX7hF36hbDtJbr311rLtX/mVXynZPXXqVMnuQlasWJE3velNJdtf+tKXSnaT5OGHHy7bTpLVq1eXbV911VVl22fiiQ4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKCtYRzHV37wMLyQZGfd6XAW7RjHcdPZflHXUDuuI75XriFeDae9jhYVOgAA3098dAUAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW5OLOXj9+vXj1q1bS05kenq6ZDdJhmEo206S2dnZsu2pqamS3d27d+fFF1+sfWP+G9auXTtu3ry5ZPvkyZMlu0ly4sSJsu2k9hqt3N61a9fMOI6byl7gNFavXj1u3LixZLvyXrR69eqy7SR55plnyraPHj1asjs7O5sTJ06c9XvRqlWrxg0bNpRsj+NYspskc3NzZdvVjhw5UrZ98ODB096LFhU6W7duzZ/+6Z++Omf1XbZv316ym9TFwl+57777yrarouA973lPye5CNm/enN/93d8t2X722WdLdpNk165dZdtJsmzZsrLtyuv/b//tv72zbPwMNm7cmF/5lV8p2b7wwgtLdpPkxhtvLNtOko9+9KNl29/4xjdKdm+//faS3YVs2LAhH/nIR0q2jx8/XrKbJPv27SvbTpJTp06Vbd99991l21/84hdPey/y0RUA0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbU0u5uBjx47lgQceKDmRZ555pmQ3SU6cOFG2nST33HNP2fZb3vKWkt1jx46V7C5kyZIlWbVqVcn2iy++WLKbJE8++WTZdpKsXLmybPuHf/iHy7bPlcnJyWzYsKFke9euXSW7SXL77beXbSfJO97xjrLtkydPluzeddddJbsLWblyZd70pjeVbFddm0mydOnSsu0kmZmZKdtevnx52fYXv/jF0/6eJzoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2Jhdz8OzsbO6///6SE9m6dWvJbpLcd999ZdtJ8swzz5Rt33bbbSW7e/fuLdldyLFjx/Lggw+WbN9yyy0lu0nyn/7TfyrbTpIbbrihbHvfvn1l2+fK8ePH8+STT5Zs79q1q2Q3SR577LGy7SQ5ePBg2fYjjzxSsjs7O1uyu5CDBw/m1ltvLdn+u3/375bsJsnq1avLtpNkyZK65x9r1qwp2z4TT3QAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoK3JxRw8NzeXmZmZkhOp2k2S//gf/2PZdpLMz8+XbV944YVl2+fC4cOH89WvfrVk+/bbby/ZTZKXXnqpbDtJdu/eXba9cePGsu1zZW5uLs8//3zJ9sTERMlukixfvrxsO0nuuuuusu2/+Iu/KNs+Fw4dOlR2L7rmmmtKdpPkzW9+c9l2kkxNTZVtv+ENbyjbPhNPdACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG1NLubg2dnZ3H///SUn8t73vrdkN0mmp6fLtpPka1/7Wtn2M888U7I7OztbsruQTZs25e/8nb9Tsr1nz56S3SSZmZkp206Sv/W3/lbZ9vve976y7XPlhRdeyG/+5m+WbF977bUlu0ly0UUXlW0nycMPP1y2PQxDye44jiW7Czl48GC+8IUvlGyfOHGiZDdJfuzHfqxsO0m++c1vlm3fe++9Zdtn4okOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgrWEcx1d+8DC8kGRn3elwFu0Yx3HT2X5R11A7riO+V64hXg2nvY4WFToAAN9PfHQFALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFuTizl4w4YN444dO0pOZO/evSW7SXLw4MGy7SSZmJgo277ssstKdp9++unMzMwMJeNnsHbt2nHLli0l2ytXrizZTZJDhw6VbSfJ8ePHy7b37dtXtn348OGZcRw3lb3Aaaxdu3bcvHlzyfbk5KJui4ty4sSJsu0k2b9/f9n2zMxM2fY4jmf9XjQ1NTUuXbq0ZHt+fr5kN0mWLVtWtl3twIEDlfOnvRct6jt6x44due22216dU/ou/+Jf/IuS3SS59dZby7aTZN26dWXbt9xyS8nuddddV7K7kC1btuRTn/pUyfZb3vKWkt0k+dKXvlS2nSRPPPFE2fa//bf/tmz7tttu21k2fgabN2/O7/3e75Vsr1+/vmQ3SXbv3l22nSSf+cxnyrZ/53d+p2z7XFi6dGle//rXl2wfPny4ZDdJLr/88rLtpDbSPv/5z5dtz83NnfZe5KMrAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANqaXMzBTz/9dD784Q+XnMif/dmflewmydVXX122nSQvvvhi2XbV+71z586S3YUcPnw4d9xxR8n27/7u75bsJskjjzxStp0kP/dzP1e2/eijj5ZtnyvLly/PVVddVbL97LPPluwmybve9a6y7ST50pe+VLb9qU99qmT3l3/5l0t2F7J8+fK84Q1vKNleu3ZtyW5S+/MmSZ555pmy7bm5ubLtM/FEBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0NbkYg6emJjIunXrSk5kx44dJbtJcscdd5RtJ8k73/nOsu1Tp06V7I7jWLK7kBUrVuTaa68t2X7ggQdKdpPkIx/5SNl2Unv9/82/+TfLtj/2sY+VbZ/Jc889l1/7tV8r2b766qtLdpPkda97Xdl2kvz1v/7Xy7arvr+q7nELmZ+fz4EDB0q2f/mXf7lkN0m++c1vlm0nyRNPPFG2PTU1VbZ98803n/b3PNEBANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLYmF3PwOI45evRoyYnMz8+X7CbJ1VdfXbadJIcPHy7bvv7660t2v/SlL5XsLmQcx5w4caJk++1vf3vJbpL8wR/8Qdl2klx88cVl2wcPHizbPldmZ2dz3333lW1X+frXv162nSQ7duwo237Tm95UsrtixYqS3YUsXbo0l1xyScn2unXrSnaT5Ed+5EfKtpNk7969ZduPPvpo2faZeKIDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoa3IxBy9ZsiSrV68uOZFrrrmmZDdJzj///LLtJHniiSfKtnfs2FGyOz09XbL7SixZUtPXX//610t2k+Sqq64q206SRx99tGz7U5/6VNn2uTI3N5f9+/eXbF9//fUlu0mya9eusu0kWbVqVdn28uXLS3ar7gcLOXDgQD7/+c+XbB87dqxkN0n+6T/9p2XbSfI7v/M7Zdt79+4t2z4TT3QAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtDeM4vvKDh+GFJDvrToezaMc4jpvO9ou6htpxHfG9cg3xajjtdbSo0AEA+H7ioysAoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2ppczMFr1qwZzz///JITeemll0p2k+TYsWNl29X7J0+eLNsex3EoGz+NtWvXjlu2bCnZnpmZKdlNklOnTpVtJ0nVe5LUXp9PPPHEzDiOm8pe4DTWrFkzbt68uWT74MGDJbtJ/XVUub9jx46S3aeffjozMzNn/V60atWqccOGDSXbExMTJbtJMjm5qB/bi3bkyJGy7cr3Zffu3ae9Fy3qHTv//PPzsY997NU5q+/yuc99rmQ3SR555JGy7SR5/PHHy7b37NlTsjuOY8nuQrZs2ZLf//3fL9n+5Cc/WbKb1P7wS5KPfvSjZduPPvpo2faP/uiP7iwbP4PNmzfnE5/4RMn2F7/4xZLdJDl69GjZdpLMzs6WbVe939ddd13J7kI2bNiQf/gP/2HJ9vr160t2q7eT5M477yzbXrNmTdn2z//8z5/2XuSjKwCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDamlzMwUeOHMldd91VciL79+8v2U2SCy64oGw7+fb7UmX16tUlu08++WTJ7kKOHDmSe+65p2T7t3/7t0t2k+TDH/5w2XaS/Oqv/mrZ9rJly8q2z5Vjx47loYceKtl+5JFHSnaTZPfu3WXbSfLe9763bPsDH/hAye65uhclycTERMlu5c+ct771rWXbSXLJJZeUbR86dKhs+0w80QEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQ1uZiD5+fns3///pITeeyxx0p2k2TJktqeO3nyZNn2tm3bSnb37NlTsvtKjONYsnvFFVeU7CbJpz/96bLtJLnxxhvLtvfu3Vu2fa4899xz+Zf/8l+WbP/oj/5oyW6S3HHHHWXbSXL48OGy7fe85z0lu3fffXfJ7kKGYSj72bBs2bKS3SRZsWJF2XZS+/PyDW94Q9n2mXiiAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtTS7m4JMnT+bZZ58tOZF9+/aV7CbJzp07y7aTZOPGjWXbExMTJbtzc3Mlu6/kdWdmZkq2ly5dWrKbJB/4wAfKtpPk6aefLtt+17veVbZ95513lm2fyerVq/P2t7+9ZHvPnj0lu0ny/PPPl20nyU033VS2/Y/+0T8q2z4Xjh49moceeqhke+XKlSW7SfLYY4+VbSfJ8uXLy7Zvu+22su0z8UQHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQ1jCO4ys++LrrrhvvueeekhP5zd/8zZLdJPnGN75Rtp0kU1NTZdtbtmwp2f34xz+ePXv2DCXjZ1B5DX3uc58r2U2Sq6++umw7Sf7JP/knZdsPPfRQ2fbXvva1e8dxvK7sBU7j2muvHb/85S+XbP/UT/1UyW6SDEPtt9yxY8fKtt/xjneU7P76r//6ObkXTU5Ojuedd17J9v79+0t2k+Q973lP2XaS/Nmf/VnZdtX7nSQHDhw47b3IEx0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbwziOr/zgYXghyc660+Es2jGO46az/aKuoXZcR3yvXEO8Gk57HS0qdAAAvp/46AoAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtiYXc/CqVavG9evXl5zI9PR0yW6SjONYtp0khw4dKtt+6aWXSnbn5uYyPz8/lIyfwcqVK8uuoXXr1pXsJsmSJbV/J5icXNS34qLs27evbPvpp5+eGcdxU9kLnMbk5OS4dOnSku1hqPu2mJqaKttOkqr3JKm7R7/44os5fPjwWb8XrVu3bty6dWvJ9tGjR0t2k/prqHK/8vq89957T3svWtTddf369fnFX/zFV+esvsv27dtLdpNkfn6+bDtJ/uIv/qJs+5ZbbinZ/da3vlWyu5D169fn7/29v1ey/f73v79kN0mWLVtWtp0k559/ftn2Jz/5ybLtn/qpn9pZNn4GS5cuzZVXXlmyXfm1rvw6J8mll15atn3RRReV7P7qr/5qye5Ctm7dms985jMl2w8++GDJbpJccMEFZdvJt9+XKpdccknZ9jAMp70X+egKAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLYmF3PwxMRE1q5dW3Iic3NzJbtJcv/995dtJ8nDDz9ctv3MM8+U7Fa+32dy7Nixsvfr7rvvLtlNknXr1pVtJ8n09HTZ9rFjx8q2z5XZ2dncd999JdurVq0q2U2S6667rmw7Saampsq23/nOd5bsfuITnyjZXcjx48fz+OOPl2zv37+/ZDf59nlXOu+888q2n3rqqbLtM/FEBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0NbkYg5eunRpLrnkkpITue+++0p2k2R6erpsu9rc3Ny5PoVX1YkTJ7J79+6S7ardJNm/f3/ZdpJMTEyUbf/Wb/1W2fa5snbt2txwww0l23/yJ39Sspsk8/PzZdtJsnnz5rLtq666qmR32bJlJbsLOXDgQG6++eaS7aNHj5bsJslll11Wtp0kF110Udn2li1byrbPxBMdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhrcjEHT01N5cILLyw5kYceeqhkN0keeOCBsu0kWbNmTdn2O9/5zpLdO++8s2R3IQcPHswXvvCFku3XvOY1JbtJ8ku/9Etl20kyPT1dtn3vvfeWbd9///1l22eyZMmSLFu2rGT7zW9+c8lukrzjHe8o206Sxx57rGz705/+dMnuiy++WLK7kMOHD+drX/tayfbevXtLdpPk2LFjZdtJ7dej+vo/HU90AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbU0u5uClS5fm0ksvLTmRm2++uWQ3SbZs2VK2nSSbNm0q277iiitKdp944omS3YVMT0/nwgsvLNn+4Ac/WLKbJLt37y7bTpI777yzbHv79u1l2+fK0qVL87rXva5k+8YbbyzZTZIvfOELZdtJcvjw4bLtP/7jPy7ZnZmZKdldyMTERFauXFmyXflneuCBB8q2k2TDhg1l2+fq544nOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaGcRxf+cHD8EKSnXWnw1m0YxzHTWf7RV1D7biO+F65hng1nPY6WlToAAB8P/HRFQDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtTS7m4LVr145btmwpOZFdu3aV7CbJ7Oxs2XaSrFmzpmz7ta99bcnu008/nZmZmaFk/AyGYRirtpcvX141nZUrV5ZtJ8nExETZ9qlTp8q2X3jhhZlxHDeVvcBprFmzZrzgggtKtg8dOlSymyTT09Nl20ly8uTJsu25ubmS3UOHDuXo0aNn/V503nnnjZs21Vy6U1NTJbtJ3dfhbBjHstt/nnzyydPeixYVOlu2bMknP/nJV+WkvtvP/dzPlewmyd133122nSQ/9EM/VLb9p3/6pyW7b33rW0t2z6XLLrusbPttb3tb2XaSnHfeeWXblaH/8Y9/fGfZ+BlccMEF+fVf//WS7a985Sslu0ly0UUXlW0nyQsvvFC2/dxzz5Xs3nTTTSW7C9m0aVP+2T/7ZyXbmzdvLtlNar/GSTIMdc154sSJsu0PfvCDp70X+egKAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0NbmYg+fn5/PSSy+VnMj09HTJbpKM41i2nSTXX3992fbExETZ9rmwYsWKXHHFFSXba9asKdlNkv3795dtJ8mqVavKtv/G3/gbZdsf//jHy7bPZH5+PgcPHizZXrlyZclukvz5n/952XaSHDlypGz7y1/+csnu/Px8ye659MQTT5Rt79u3r2w7SZ566qmy7UcffbRs+0w80QEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQ1uZiDlyxZkhUrVpScyNTUVMlukrzxjW8s206StWvXlm1/5StfKdk9dOhQye5CJicns3HjxpLtyq/DyZMny7aTZNu2bWXbl1xySdn2uTI5OZlNmzaVbD/++OMlu0ny1FNPlW0nyerVq8u25+fny7bPhXXr1uXHf/zHS7ar7ttJ8tWvfrVsO0luvfXWsu1du3aVbZ+JJzoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0NbkYg6en5/PgQMHSk7kB37gB0p2k2RqaqpsO0mee+65su2TJ0+W7B49erRkdyHLli3L5ZdfXrI9NzdXspska9euLdtOkmuuuaZs+9JLLy3bPlcmJiaybt26ku3HH3+8ZDdJduzYUbadJPfcc0/Z9mtf+9qS3d27d5fsLmRubi4zMzMl2xs3bizZTZLzzz+/bDtJpqeny7bf//73l21/9rOfPe3veaIDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoa3IxB69ZsyZ/7a/9tZITec1rXlOymySHDh0q206S48ePl20/9dRTJbsTExMlu6/EMAwlu+M4luwmyQUXXFC2nSRzc3Nl2+fya11lxYoVueaaa0q2r7zyypLdJPnEJz5Rtp0k1113Xdn2Sy+9VLK7d+/ekt2FTExMZNWqVSXb3/rWt0p2k+TAgQNl20nywQ9+sGz7hhtuKNv+7Gc/e9rf80QHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQ1jCO4ys/eBheSLKz7nQ4i3aM47jpbL+oa6gd1xHfK9cQr4bTXkeLCh0AgO8nProCANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDa+n8B2EVCBpmrFKsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(sign_classifier.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 4, 4, 'conv0', size=(10,10))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignClassifier(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Dropout(p=0.3, inplace=False)\n",
       "    (5): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout(p=0.3, inplace=False)\n",
       "    (10): Conv2d(16, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "sign_classifier.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "sign_classifier.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "sign_classifier.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(sign_classifier, dummy_input, onnx_sign_classifier_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "sign_classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 9752.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[-22.68045   -16.781826   -1.9609932  -6.169614   -8.710003    4.4003386\n",
      "   -0.7178692 -10.328957  -18.135715   15.789558 ]]\n",
      "Predictions shape: (1, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_sign_classifier_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
