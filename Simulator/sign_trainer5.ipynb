{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "# NOTE : Sign detecttion uses the folder training_images to load the backgrounds\n",
    "# it uses the folder sign_imgs to load the signs, the rate of examples is important\n",
    "num_channels = 3 # COLOR IMGS\n",
    "SIZE = (32, 32)\n",
    "model_name = 'models/sign_classifier.pt'\n",
    "onnx_sign_classifier_path = \"models/sign_classifier.onnx\"\n",
    "max_load = 1_000 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 examples loaded for 10 class_names\n",
      "example labels: [9, 7, 9, 9, 9, 5, 6, 1, 4, 8, 0, 9, 2, 9, 3]\n"
     ]
    }
   ],
   "source": [
    "# LOAD EXAMPLES\n",
    "# imgs of the examples must be in a specific folder (ex: sign_imgs), and must be named as \"class_<number>.png\"\n",
    "# ex: sign_imgs/stop_1.png, note: start from 1\n",
    "\n",
    "#load examples\n",
    "examples_folder = 'sign_imgs'     \n",
    "class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway', 'nosign']\n",
    "# class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway']\n",
    "\n",
    "file_names = [f for f in os.listdir(examples_folder) if f.endswith('.png')]\n",
    "example_labels = [class_names.index(name.split('_')[0]) for name in file_names if name.split('_')[0] in class_names]\n",
    "example_imgs = [cv.resize(cv.blur(cv.imread(os.path.join(examples_folder, name)), (5,5)), (128,128)) for name in file_names if name.split('_')[0] in class_names]\n",
    "tot_examples = len(example_imgs)\n",
    "tot_classes = len(class_names) \n",
    "print(f'{tot_examples} examples loaded for {tot_classes} class_names')\n",
    "print(f'example labels: {example_labels}')    \n",
    "\n",
    "#show images\n",
    "cv.namedWindow('example', cv.WINDOW_NORMAL)\n",
    "for i in range(tot_examples):\n",
    "    img = example_imgs[i].copy()\n",
    "    # add text label\n",
    "    cv.putText(img, class_names[example_labels[i]], (10,30), cv.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "    cv.imshow('example', img)\n",
    "    key = cv.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE 32x32\n",
    "\n",
    "class SignClassifier(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=num_channels): \n",
    "        super().__init__()\n",
    "        p = 0.2\n",
    "        ### Convoluational layers\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 16, kernel_size=3, stride=1), #out = 12 - 28  \n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=10 - 14\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(p),\n",
    "            nn.Conv2d(16, 8, kernel_size=4, stride=2), #out = 6\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1), #out=5\n",
    "            # nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p),\n",
    "            nn.Conv2d(8, 64, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            # nn.Linear(in_features=4*4*4, out_features=128),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.Dropout(p),\n",
    "            # nn.Linear(in_features=128, out_features=out_dim),\n",
    "            nn.Linear(in_features=64*1*1, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "sign_classifier = SignClassifier(out_dim=tot_classes,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE 16x16\n",
    "\n",
    "# class SignClassifier(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=num_channels): \n",
    "#         super().__init__()\n",
    "#         p = 0.3\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=1), #out=10\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.Conv2d(8, 16, kernel_size=5, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=5\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.Conv2d(16, 128, kernel_size=5, stride=1), #out = 1\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             # nn.Linear(in_features=4*4*4, out_features=128),\n",
    "#             # nn.ReLU(True),\n",
    "#             # nn.Dropout(p),\n",
    "#             # nn.Linear(in_features=128, out_features=out_dim),\n",
    "#             nn.Linear(in_features=128*1*1, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# sign_classifier = SignClassifier(out_dim=tot_classes,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "out shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "sign_classifier.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = sign_classifier(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load coco dataset and create background img in the background folder\n",
    "\n",
    "# import json\n",
    "\n",
    "# MAX_BACKGROUNDS = 500_000\n",
    "\n",
    "# #get all file names inside fodler sign_imgs/coco_val\n",
    "# # coco_val_img_names = [os.path.join('sign_imgs', 'coco_val', f) for f in os.listdir(os.path.join('sign_imgs', 'coco_val'))]\n",
    "# coco_val_img_names = [f for f in os.listdir(os.path.join('sign_imgs', 'coco_val'))]\n",
    "\n",
    "# print(f'{len(coco_val_img_names)} images in coco val')\n",
    "\n",
    "# #get all instances\n",
    "# #load json file\n",
    "# with open('sign_imgs/coco_val/instances_val2017.json') as f:\n",
    "#     data = json.load(f)\n",
    "#     #print name of the file\n",
    "#     print(f'images = {len(data[\"images\"])}')\n",
    "#     print(f'annotations = {len(data[\"annotations\"])}')\n",
    "\n",
    "\n",
    "#     categories = {cat['id']:cat['name'] for cat in data['categories']}\n",
    "#     categories_by_name = {cat['name']:cat['id'] for cat in data['categories']}\n",
    "\n",
    "#     filtered_categories = ['traffic light', 'bicycle', 'car', 'motorcycle', 'bus', 'truck',\n",
    "#                             'fire hydrant', 'stop sign',  'parking meter']\n",
    "#     filtered_categories_idxs = [categories_by_name[c] for c in filtered_categories]\n",
    "\n",
    "#     img_names_by_id = {img['id']:img['file_name'] for img in data['images']}\n",
    "\n",
    "#     categ_by_id = {img['id']:[] for img in data['images']}\n",
    "\n",
    "#     for ann in data['annotations']:\n",
    "#         categ_id = ann['category_id']\n",
    "#         img_id = ann['image_id']\n",
    "#         if img_id in categ_by_id:\n",
    "#             categ_by_id[img_id].append(categories[categ_id])\n",
    "#         if categ_id in filtered_categories_idxs:\n",
    "#             img_names_by_id.pop(img_id, None)\n",
    "#             categ_by_id.pop(img_id, None)\n",
    "    \n",
    "#     for id, c in categ_by_id.items():\n",
    "#         if len(c) == 0:\n",
    "#             img_names_by_id.pop(id, None)\n",
    "\n",
    "#     print(f'final images = {len(img_names_by_id)}')\n",
    "\n",
    "#     BACKGROUND_SIZE = (320,240)\n",
    "#     idx = 0\n",
    "#     for k, img_name in img_names_by_id.items():\n",
    "#         img_name = img_names_by_id[k]\n",
    "#         categories_in_img = categ_by_id[k]\n",
    "#         img = cv.imread(os.path.join('sign_imgs', 'coco_val', img_name))\n",
    "#         img = cv.resize(img, (2*BACKGROUND_SIZE[0], 2*BACKGROUND_SIZE[1]))\n",
    "#         #divide the image into 4 parts\n",
    "#         img_parts = [img[:BACKGROUND_SIZE[1], :BACKGROUND_SIZE[0]],\n",
    "#                     img[:BACKGROUND_SIZE[1], BACKGROUND_SIZE[0]:],\n",
    "#                     img[BACKGROUND_SIZE[1]:, :BACKGROUND_SIZE[0]],\n",
    "#                     img[BACKGROUND_SIZE[1]:, BACKGROUND_SIZE[0]:]]\n",
    "\n",
    "#         for i in range(4):\n",
    "#             img_part = img_parts[i]\n",
    "#             #further divide the image into 4 parts\n",
    "#             img_part_parts = [img_part[:BACKGROUND_SIZE[1]//2, :BACKGROUND_SIZE[0]//2],\n",
    "#                             img_part[:BACKGROUND_SIZE[1]//2, BACKGROUND_SIZE[0]//2:],\n",
    "#                             img_part[BACKGROUND_SIZE[1]//2:, :BACKGROUND_SIZE[0]//2],\n",
    "#                             img_part[BACKGROUND_SIZE[1]//2:, BACKGROUND_SIZE[0]//2:]]\n",
    "#             for j in range(4):\n",
    "#                 img_part_part = img_part_parts[j]\n",
    "#                 cv.imshow(f'img_{i}{j}', img_part_parts[j])\n",
    "#                 idx += 1\n",
    "#                 cv.imwrite(os.path.join('sign_imgs', 'backgrounds', f'background_{idx}.png'), img_part_parts[j])\n",
    "\n",
    "#         print(f'{categories_in_img}')\n",
    "#         key = cv.waitKey(1)\n",
    "#         if key == 27 or idx > MAX_BACKGROUNDS:\n",
    "#             break\n",
    "\n",
    "# cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irong/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/ipykernel_launcher.py:146: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "\n",
    "def draw_box(img, x, y, w):\n",
    "    img = cv.rectangle(img, (int(x-w/2), int(y-w/2)), (int(x+w/2), int(y+w/2)), 255, 2)\n",
    "    return img\n",
    "\n",
    "def load_and_augment_img(img, example_index=0, example_imgs=example_imgs):\n",
    "    # img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "    if img is not None:\n",
    "        #convert to gray\n",
    "        # img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "        # #crop to the top right quadrant\n",
    "        img = img[0:img.shape[1]//2, img.shape[1]//2:]\n",
    "        canv_dim = randint(64//2, 240//2)  ## RANGE OF DIMENSION OF THE SIGN\n",
    "        start_x = randint(0, img.shape[1]-canv_dim) if img.shape[1] > canv_dim else 0\n",
    "        start_y = randint(0, img.shape[0]-canv_dim) if img.shape[0] > canv_dim else 0\n",
    "        img = img[start_y:start_y+canv_dim, start_x:start_x+canv_dim]\n",
    "        # img = cv.resize(img, (2*SIZE[0], 2*SIZE[1]))\n",
    "    else:\n",
    "        img = randint(0,255,(2*SIZE[0], 2*SIZE[1]), dtype=np.uint8)\n",
    "\n",
    "    ## EXAMPLE AUGMENTATION ##############################################################\n",
    "    #load example\n",
    "    example = example_imgs[example_index]\n",
    "    resize_ratio = max(img.shape)/max(example.shape)\n",
    "    example = cv.resize(example, (int(example.shape[1]*resize_ratio), int(example.shape[0]*resize_ratio)))\n",
    "    #Make a black border aroud the example\n",
    "    example[0:2,:,:] = np.array([0,0,0])\n",
    "    example[-3:-1,:,:] = np.array([0,0,0])\n",
    "    example[:,0:2,:] = np.array([0,0,0])\n",
    "    example[:,-3:-1,:] = np.array([0,0,0])\n",
    "\n",
    "    #get example mask\n",
    "    example_mask = np.where(example == np.array([0,0,0]), np.zeros_like(example), 255*np.ones_like(example))\n",
    "    example_mask = example_mask.astype(np.uint8)\n",
    "    example_mask = cv.cvtColor(example_mask, cv.COLOR_BGR2GRAY)\n",
    "    #convert to bitmap\n",
    "    example_mask = np.where(example_mask == 0, 0, 255)\n",
    "    example_mask = example_mask.astype(np.uint8)\n",
    "    #back to BGR\n",
    "    example_mask = cv.cvtColor(example_mask, cv.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # add noise to the example\n",
    "    std = 20\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.subtract(example, noisem)\n",
    "    noisep = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.add(example, noisep)\n",
    "\n",
    "    #set zero where example mask is zero\n",
    "    example = np.where(example_mask == np.array([0,0,0]), np.zeros_like(example), example)\n",
    "\n",
    "    #perspective transform example\n",
    "    perspective_deformation = img.shape[0]//10 ################# DEFORMATION\n",
    "    pts1 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = pts2 + np.float32(randint(0,perspective_deformation,size=pts2.shape))\n",
    "    # print(f'pts2 = \\n{pts2}')\n",
    "    new_size_x = int(np.max(pts2[:,0]) - np.min(pts2[:,0]))\n",
    "    new_size_y = int(np.max(pts2[:,1]) - np.min(pts2[:,1]))\n",
    "    M = cv.getPerspectiveTransform(pts1,pts2)\n",
    "    example = cv.warpPerspective(example,M,(new_size_x,new_size_y))\n",
    "\n",
    "    #resize example keeping proportions\n",
    "    img_example_ratio = min(img.shape[0]/example.shape[0], img.shape[1]/example.shape[1])\n",
    "    scale_factor = np.random.uniform(.5, .98) ##.15, .35############################ PARAM ##############################\n",
    "    scale_factor = scale_factor * img_example_ratio\n",
    "    example = cv.resize(example, (0,0), fx=scale_factor, fy=scale_factor)\n",
    "    #match img shape\n",
    "    example_canvas = np.zeros((img.shape[0], img.shape[1], num_channels), dtype=np.uint8)\n",
    "\n",
    "    #get a random position for the example\n",
    "    example_y = randint(0, img.shape[0] - example.shape[0])\n",
    "    example_x = randint(0, img.shape[1] - example.shape[1])\n",
    "    #paste example on canvas\n",
    "    example_canvas[example_y:example_y+example.shape[0], example_x:example_x+example.shape[1]] = example\n",
    "\n",
    "    #canvas mask\n",
    "    example_canvas_mask = example_canvas.copy()\n",
    "    example_canvas_mask = cv.cvtColor(example_canvas_mask, cv.COLOR_BGR2GRAY)\n",
    "    example_canvas_mask = np.where(example_canvas_mask == 0, 0, 255)\n",
    "    example_canvas_mask = example_canvas_mask.astype(np.uint8)\n",
    "    edge_of_canvas = cv.Canny(example_canvas_mask, 100, 200)\n",
    "    ker_rand = randint(2,5)\n",
    "    edge_of_canvas = cv.dilate(edge_of_canvas, np.ones((ker_rand,ker_rand)))\n",
    "    #erode mask\n",
    "    ker_rand = randint(2,5)\n",
    "    kernel = np.ones((ker_rand,ker_rand),np.uint8)\n",
    "    example_canvas_mask = cv.erode(example_canvas_mask,kernel,iterations = randint(1,3))\n",
    "\n",
    "    #convert to hsv\n",
    "    example_canvas = cv.cvtColor(example_canvas, cv.COLOR_BGR2HSV)\n",
    "    #get the hsv channels\n",
    "    example_canvas_h = example_canvas[:,:,0]\n",
    "    example_canvas_s = example_canvas[:,:,1]\n",
    "    example_canvas_v = example_canvas[:,:,2]\n",
    "\n",
    "    # #reduce brightness\n",
    "    brightness_shift = randint(-80,5)\n",
    "    # example_canvas_v = np.clip(example_canvas_v + brightness_shift, 0, 255).astype(np.uint8)\n",
    "    if brightness_shift > 0:\n",
    "        example_canvas_v = cv.add(example_canvas_v, np.ones_like(example_canvas_v)*brightness_shift)\n",
    "    elif brightness_shift < 0:\n",
    "        example_canvas_v = cv.subtract(example_canvas_v, np.ones_like(example_canvas_v)*abs(brightness_shift))\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.25,.9)\n",
    "    example_canvas_v = np.clip(127*(1-const) + example_canvas_v*const, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #augment hue\n",
    "    hue_shift = randint(-7,7)\n",
    "    example_canvas_h = (example_canvas_h + hue_shift) % 180\n",
    "\n",
    "    #augment saturation\n",
    "    sat_shift = randint(-100,0)\n",
    "    example_canvas_s = np.clip(example_canvas_s + sat_shift, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #rebuild the channels\n",
    "    example_canvas[:,:,0] = example_canvas_h\n",
    "    example_canvas[:,:,1] = example_canvas_s\n",
    "    example_canvas[:,:,2] = example_canvas_v\n",
    "    #back to bgr\n",
    "    example_canvas = cv.cvtColor(example_canvas, cv.COLOR_HSV2BGR)\n",
    "\n",
    "    #paste canvas on img\n",
    "    img_b = img[:,:,0]\n",
    "    img_g = img[:,:,1]\n",
    "    img_r = img[:,:,2]\n",
    "    example_b = example_canvas[:,:,0]\n",
    "    example_g = example_canvas[:,:,1]\n",
    "    example_r = example_canvas[:,:,2]\n",
    "    img_b = np.where(example_canvas_mask > 0, example_b, img_b)\n",
    "    img_g = np.where(example_canvas_mask > 0, example_g, img_g)\n",
    "    img_r = np.where(example_canvas_mask > 0, example_r, img_r)\n",
    "    # img = np.where(example_canvas_mask > 0, example_canvas, img) \n",
    "    img[:,:,0] = img_b\n",
    "    img[:,:,1] = img_g\n",
    "    img[:,:,2] = img_r\n",
    "\n",
    "    ker_rand = randint(2,5)\n",
    "    blurred_img = cv.blur(img, (ker_rand,ker_rand))\n",
    "    img = np.where(edge_of_canvas ==np.array([0,0,0]), blurred_img, img)\n",
    "\n",
    "    ##########################################################################################\n",
    "    \n",
    "    # convert whole img to hsv\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "    #get the hsv channels\n",
    "    img_h = img[:,:,0]\n",
    "    img_s = img[:,:,1]\n",
    "    img_v = img[:,:,2]\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.5,.99)\n",
    "    img_v = np.clip(127*(1-const) + img_v*const, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #reduce brightness\n",
    "    brightness_shift = randint(-40,0)\n",
    "    if brightness_shift > 0:\n",
    "        img_v = cv.add(img_v, np.ones_like(img_v)*brightness_shift)\n",
    "    elif brightness_shift < 0:\n",
    "        img_v = cv.subtract(img_v, np.ones_like(img_v)*abs(brightness_shift))\n",
    "\n",
    "    #augment hue\n",
    "    hue_shift = randint(-2,2)\n",
    "    img_h = (img_h + hue_shift) % 180\n",
    "\n",
    "    #augment saturation\n",
    "    sat_shift = randint(-20,0)\n",
    "    img_s = np.clip(img_s + sat_shift, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #rebuild the channels\n",
    "    img[:,:,0] = img_h\n",
    "    img[:,:,1] = img_s\n",
    "    img[:,:,2] = img_v\n",
    "    #back to bgr\n",
    "    img = cv.cvtColor(img, cv.COLOR_HSV2BGR)\n",
    "\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = img.shape[0]//5\n",
    "    offset = np.random.randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        # img[:offset, :] = np.random.randint(0,255)\n",
    "        img = img[offset:, :]\n",
    "    elif offset < 0:\n",
    "        # img[offset:, :] = np.random.randint(0,255)\n",
    "        img = img[:offset, :]\n",
    "\n",
    "    offset_y = np.random.randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset_y, axis=1)\n",
    "    if offset_y > 0:\n",
    "        # img[:, :offset_y] = np.random.randint(0,255)\n",
    "        img = img[:, offset_y:]\n",
    "    elif offset_y < 0:\n",
    "        # img[:, offset_y:] = np.random.randint(0,255)\n",
    "        img = img[:, :offset_y]\n",
    "\n",
    "    min_dim = min(img.shape[0], img.shape[1])\n",
    "    #crop to square\n",
    "    img = img[:min_dim, :min_dim]\n",
    "\n",
    "    #add noise\n",
    "    std = 50\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "\n",
    "    # # crop into the img at random position\n",
    "    # zoom = randint(0, SIZE[0]//4)\n",
    "    # img = img[zoom:img.shape[0]-zoom, zoom:img.shape[1]-zoom]\n",
    "\n",
    "    # #blur \n",
    "    # b = randint(2,5)\n",
    "    # img = cv.blur(img, (b,b))\n",
    "    \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    #convert to hsv\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('sign_imgs', 'backgrounds',  f'background_{i+1}.png'))\n",
    "    # img = cv.resize(img, (160,120))\n",
    "    # img = None\n",
    "    img = load_and_augment_img(img, example_index=(i%tot_examples))\n",
    "\n",
    "    #to bgr\n",
    "    img = cv.cvtColor(img, cv.COLOR_HSV2BGR)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(200)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = os.path.join('sign_imgs', 'backgrounds')\n",
    "        self.data = []\n",
    "        self.class_names = []\n",
    "        self.channels = channels\n",
    "        self.all_imgs = torch.zeros((max_load*tot_examples, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "        cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "        # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN\n",
    "\n",
    "        for i in tqdm(range(max_load)):\n",
    "            img = cv.imread(os.path.join(self.folder, f'background_{i+1}.png'))\n",
    "            # img = cv.resize(img, (160,120))\n",
    "            for j in range(tot_examples):\n",
    "                img_j = load_and_augment_img(img.copy(), example_index=j)\n",
    "                if i < 100:\n",
    "                    cv.imshow('img', img_j)\n",
    "                    cv.waitKey(1)\n",
    "                    if i == 99:\n",
    "                        cv.destroyAllWindows()\n",
    "                #add a dimension to the image\n",
    "                img_j = img_j[:, :,np.newaxis] if self.channels == 1 else img_j\n",
    "                #convert to tensor\n",
    "                img_j = torch.from_numpy(img_j)\n",
    "                self.all_imgs[i*tot_examples+j] = img_j\n",
    "                self.class_names.append(example_labels[j])\n",
    "        \n",
    "        self.data = torch.from_numpy(np.array(self.data))\n",
    "        self.class_names = torch.from_numpy(np.array(self.class_names))\n",
    "        print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "        print(f'class_names: {self.class_names.shape}')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.class_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        class_label = self.class_names[idx]\n",
    "        return img, class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/home/irong/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/ipykernel_launcher.py:146: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "100%|██████████| 1000/1000 [01:03<00:00, 15.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all imgs: torch.Size([15000, 32, 32, 3])\n",
      "class_names: torch.Size([15000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset(max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8192//3, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8192//3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2730, 3, 32, 32])\n",
      "torch.Size([2730])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, class_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    class_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, class_label) in tqdm(dataloader):\n",
    "        #one hot encode the class label\n",
    "        class_label = torch.eye(tot_classes)[class_label]\n",
    "        # Move the input and target data to the selected device\n",
    "        input, class_label = input.to(device), class_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        class_out = output[:, 0:]\n",
    "\n",
    "        #classification loss\n",
    "        assert class_label.shape == class_out.shape, f'class_label.shape: {class_label.shape}, output.shape: {output.shape}'\n",
    "\n",
    "        class_loss = 1.0*class_loss_fn(class_out, class_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = class_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        class_losses.append(class_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    class_loss = np.mean(class_losses)\n",
    "    return  np.sqrt(class_loss)\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(sign_classifier, val_dataloader, regr_loss_fn, class_loss_fn, device=device):\n",
    "    sign_classifier.eval()\n",
    "    y_losses = []\n",
    "    x_losses = []\n",
    "    w_losses = []\n",
    "    class_losses = []\n",
    "    for (input, class_label) in tqdm(val_dataloader):\n",
    "        #one hot encode the class label\n",
    "        class_label = torch.eye(tot_classes)[class_label]\n",
    "        input, class_label = input.to(device), class_label.to(device)\n",
    "        output = sign_classifier(input)\n",
    "\n",
    "        class_loss = 1.0*class_loss_fn(output[:, 0:], class_label)\n",
    "    \n",
    "        class_losses.append(class_loss.detach().cpu().numpy())\n",
    "    return  np.mean(class_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  200/200\n",
      "class_loss: 0.2725 --- Val: 0.0608\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.002 #0.005\n",
    "epochs = 200 #50+\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 1e-4 #9e-4\n",
    "L2_lambda = 1e-3 #1e-2\n",
    "optimizer = torch.optim.Adam(sign_classifier.parameters(), lr=lr, weight_decay=3e-5) #wd = 2e-3# 9e-5\n",
    "\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "class_loss_fn = nn.CrossEntropyLoss()\n",
    "best_val = 100\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        class_loss = train_epoch(sign_classifier, train_dataloader, regr_loss_fn, class_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_class_loss = val_epoch(sign_classifier, val_dataloader, regr_loss_fn, class_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs}\")\n",
    "    print(f\"class_loss: {class_loss:.4f} --- Val: {val_class_loss:.4f}\")\n",
    "    if val_class_loss < best_val:\n",
    "        torch.save(sign_classifier.state_dict(), model_name)\n",
    "        print(f'Model saved as {model_name}')\n",
    "        best_val = val_class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "# val_dataloader2 = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "# sign_classifier.load_state_dict(torch.load(model_name))\n",
    "# sign_classifier.eval()\n",
    "# for (input, class_label) in tqdm(val_dataloader2):\n",
    "#     #convert img to numpy array\n",
    "#     img = input[0].detach().cpu().numpy()[0]\n",
    "#     #convert to sigle byte\n",
    "#     img = img.astype(np.uint8)\n",
    "#     input, box_label, class_label =input.to(device), box_label.to(device), class_label.to(device)\n",
    "#     output = sign_classifier(input)\n",
    "#     x = output[:, 0]\n",
    "#     y = output[:, 1]\n",
    "#     w = output[:, 2]\n",
    "#     class_out = output[:, 3:]\n",
    "#     # print(class_out)\n",
    "#     class_out = torch.argmax(class_out, dim=1)\n",
    "#     class_out = class_out.detach().cpu().numpy()\n",
    "#     # print(f\"Predicted class: {class_out[0]}, Actual class: {class_label[0]}\")\n",
    "#     class_out = class_out[0]\n",
    "#     class_out = class_names[class_out]\n",
    "#     true_class = class_names[class_label[0]]\n",
    "#     img = draw_box(img, x, y, w)\n",
    "#     #text for labels\n",
    "#     img = cv.putText(img, f\"True: {true_class}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "#     img = cv.putText(img, f\"Pred: {class_out}\", (10, 60), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "#     cv.imshow('img', img)\n",
    "#     key = cv.waitKey(0)\n",
    "#     if key == 27:\n",
    "#         break\n",
    "    \n",
    "# cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 3, 3, 3)\n",
      "(8, 16, 4, 4)\n",
      "(64, 8, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(sign_classifier.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 4, 4, 'conv0', size=(10,10))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignClassifier(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Dropout(p=0.2, inplace=False)\n",
       "    (5): Conv2d(16, 8, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout(p=0.2, inplace=False)\n",
       "    (10): Conv2d(8, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "sign_classifier.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "sign_classifier.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "sign_classifier.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(sign_classifier, dummy_input, onnx_sign_classifier_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "sign_classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 3125.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[-10.947596     3.3478603  -14.169675   -14.477887    -4.80114\n",
      "    4.93336     -9.047274     0.3038184  -16.9109       0.04303336]]\n",
      "Predictions shape: (1, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_sign_classifier_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
