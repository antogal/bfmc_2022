{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "# NOTE : Sign detecttion uses the folder training_images to load the backgrounds\n",
    "# it uses the folder sign_imgs to load the signs, the rate of examples is important\n",
    "num_channels = 3 # COLOR IMGS\n",
    "SIZE = (32, 32)\n",
    "model_name = 'models/sign_classifier.pt'\n",
    "onnx_sign_classifier_path = \"models/sign_classifier.onnx\"\n",
    "max_load = 5_000 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 examples loaded for 10 class_names\n",
      "example labels: [9, 7, 9, 9, 9, 5, 6, 1, 4, 8, 0, 9, 2, 9, 3]\n"
     ]
    }
   ],
   "source": [
    "# LOAD EXAMPLES\n",
    "# imgs of the examples must be in a specific folder (ex: sign_imgs), and must be named as \"class_<number>.png\"\n",
    "# ex: sign_imgs/stop_1.png, note: start from 1\n",
    "\n",
    "#load examples\n",
    "examples_folder = 'sign_imgs'     \n",
    "class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway', 'nosign']\n",
    "# class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway']\n",
    "\n",
    "file_names = [f for f in os.listdir(examples_folder) if f.endswith('.png')]\n",
    "example_labels = [class_names.index(name.split('_')[0]) for name in file_names if name.split('_')[0] in class_names]\n",
    "example_imgs = [cv.resize(cv.blur(cv.imread(os.path.join(examples_folder, name)), (5,5)), (128,128)) for name in file_names if name.split('_')[0] in class_names]\n",
    "tot_examples = len(example_imgs)\n",
    "tot_classes = len(class_names) \n",
    "print(f'{tot_examples} examples loaded for {tot_classes} class_names')\n",
    "print(f'example labels: {example_labels}')    \n",
    "\n",
    "#show images\n",
    "cv.namedWindow('example', cv.WINDOW_NORMAL)\n",
    "for i in range(tot_examples):\n",
    "    img = example_imgs[i].copy()\n",
    "    # add text label\n",
    "    cv.putText(img, class_names[example_labels[i]], (10,30), cv.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "    cv.imshow('example', img)\n",
    "    key = cv.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE 32x32\n",
    "\n",
    "class SignClassifier(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=num_channels): \n",
    "        super().__init__()\n",
    "        p = 0.2\n",
    "        ### Convoluational layers\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 8, kernel_size=3, stride=1), #out = 12 - 28  \n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=10 - 14\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(p),\n",
    "            nn.Conv2d(8, 8, kernel_size=4, stride=2), #out = 6\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1), #out=5\n",
    "            # nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p),\n",
    "            nn.Conv2d(8, 32, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            # nn.Linear(in_features=4*4*4, out_features=128),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.Dropout(p),\n",
    "            # nn.Linear(in_features=128, out_features=out_dim),\n",
    "            nn.Linear(in_features=32*1*1, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "sign_classifier = SignClassifier(out_dim=tot_classes,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE 16x16\n",
    "\n",
    "# class SignClassifier(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=num_channels): \n",
    "#         super().__init__()\n",
    "#         p = 0.3\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=1), #out=10\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.Conv2d(8, 16, kernel_size=5, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=5\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.Conv2d(16, 128, kernel_size=5, stride=1), #out = 1\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             # nn.Linear(in_features=4*4*4, out_features=128),\n",
    "#             # nn.ReLU(True),\n",
    "#             # nn.Dropout(p),\n",
    "#             # nn.Linear(in_features=128, out_features=out_dim),\n",
    "#             nn.Linear(in_features=128*1*1, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# sign_classifier = SignClassifier(out_dim=tot_classes,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "out shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "sign_classifier.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = sign_classifier(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load coco dataset and create background img in the background folder\n",
    "\n",
    "# import json\n",
    "\n",
    "# MAX_BACKGROUNDS = 500_000\n",
    "\n",
    "# #get all file names inside fodler sign_imgs/coco_val\n",
    "# # coco_val_img_names = [os.path.join('sign_imgs', 'coco_val', f) for f in os.listdir(os.path.join('sign_imgs', 'coco_val'))]\n",
    "# coco_val_img_names = [f for f in os.listdir(os.path.join('sign_imgs', 'coco_val'))]\n",
    "\n",
    "# print(f'{len(coco_val_img_names)} images in coco val')\n",
    "\n",
    "# #get all instances\n",
    "# #load json file\n",
    "# with open('sign_imgs/coco_val/instances_val2017.json') as f:\n",
    "#     data = json.load(f)\n",
    "#     #print name of the file\n",
    "#     print(f'images = {len(data[\"images\"])}')\n",
    "#     print(f'annotations = {len(data[\"annotations\"])}')\n",
    "\n",
    "\n",
    "#     categories = {cat['id']:cat['name'] for cat in data['categories']}\n",
    "#     categories_by_name = {cat['name']:cat['id'] for cat in data['categories']}\n",
    "\n",
    "#     filtered_categories = ['traffic light', 'bicycle', 'car', 'motorcycle', 'bus', 'truck',\n",
    "#                             'fire hydrant', 'stop sign',  'parking meter']\n",
    "#     filtered_categories_idxs = [categories_by_name[c] for c in filtered_categories]\n",
    "\n",
    "#     img_names_by_id = {img['id']:img['file_name'] for img in data['images']}\n",
    "\n",
    "#     categ_by_id = {img['id']:[] for img in data['images']}\n",
    "\n",
    "#     for ann in data['annotations']:\n",
    "#         categ_id = ann['category_id']\n",
    "#         img_id = ann['image_id']\n",
    "#         if img_id in categ_by_id:\n",
    "#             categ_by_id[img_id].append(categories[categ_id])\n",
    "#         if categ_id in filtered_categories_idxs:\n",
    "#             img_names_by_id.pop(img_id, None)\n",
    "#             categ_by_id.pop(img_id, None)\n",
    "    \n",
    "#     for id, c in categ_by_id.items():\n",
    "#         if len(c) == 0:\n",
    "#             img_names_by_id.pop(id, None)\n",
    "\n",
    "#     print(f'final images = {len(img_names_by_id)}')\n",
    "\n",
    "#     BACKGROUND_SIZE = (320,240)\n",
    "#     idx = 0\n",
    "#     for k, img_name in img_names_by_id.items():\n",
    "#         img_name = img_names_by_id[k]\n",
    "#         categories_in_img = categ_by_id[k]\n",
    "#         img = cv.imread(os.path.join('sign_imgs', 'coco_val', img_name))\n",
    "#         img = cv.resize(img, (2*BACKGROUND_SIZE[0], 2*BACKGROUND_SIZE[1]))\n",
    "#         #divide the image into 4 parts\n",
    "#         img_parts = [img[:BACKGROUND_SIZE[1], :BACKGROUND_SIZE[0]],\n",
    "#                     img[:BACKGROUND_SIZE[1], BACKGROUND_SIZE[0]:],\n",
    "#                     img[BACKGROUND_SIZE[1]:, :BACKGROUND_SIZE[0]],\n",
    "#                     img[BACKGROUND_SIZE[1]:, BACKGROUND_SIZE[0]:]]\n",
    "\n",
    "#         for i in range(4):\n",
    "#             img_part = img_parts[i]\n",
    "#             #further divide the image into 4 parts\n",
    "#             img_part_parts = [img_part[:BACKGROUND_SIZE[1]//2, :BACKGROUND_SIZE[0]//2],\n",
    "#                             img_part[:BACKGROUND_SIZE[1]//2, BACKGROUND_SIZE[0]//2:],\n",
    "#                             img_part[BACKGROUND_SIZE[1]//2:, :BACKGROUND_SIZE[0]//2],\n",
    "#                             img_part[BACKGROUND_SIZE[1]//2:, BACKGROUND_SIZE[0]//2:]]\n",
    "#             for j in range(4):\n",
    "#                 img_part_part = img_part_parts[j]\n",
    "#                 cv.imshow(f'img_{i}{j}', img_part_parts[j])\n",
    "#                 idx += 1\n",
    "#                 cv.imwrite(os.path.join('sign_imgs', 'backgrounds', f'background_{idx}.png'), img_part_parts[j])\n",
    "\n",
    "#         print(f'{categories_in_img}')\n",
    "#         key = cv.waitKey(1)\n",
    "#         if key == 27 or idx > MAX_BACKGROUNDS:\n",
    "#             break\n",
    "\n",
    "# cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irong/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/ipykernel_launcher.py:146: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "\n",
    "def draw_box(img, x, y, w):\n",
    "    img = cv.rectangle(img, (int(x-w/2), int(y-w/2)), (int(x+w/2), int(y+w/2)), 255, 2)\n",
    "    return img\n",
    "\n",
    "def load_and_augment_img(img, example_index=0, example_imgs=example_imgs):\n",
    "    # img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "    if img is not None:\n",
    "        #convert to gray\n",
    "        # img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "        # #crop to the top right quadrant\n",
    "        img = img[0:img.shape[1]//2, img.shape[1]//2:]\n",
    "        canv_dim = randint(64//2, 240//2)  ## RANGE OF DIMENSION OF THE SIGN\n",
    "        start_x = randint(0, img.shape[1]-canv_dim) if img.shape[1] > canv_dim else 0\n",
    "        start_y = randint(0, img.shape[0]-canv_dim) if img.shape[0] > canv_dim else 0\n",
    "        img = img[start_y:start_y+canv_dim, start_x:start_x+canv_dim]\n",
    "        # img = cv.resize(img, (2*SIZE[0], 2*SIZE[1]))\n",
    "    else:\n",
    "        img = randint(0,255,(2*SIZE[0], 2*SIZE[1]), dtype=np.uint8)\n",
    "\n",
    "    ## EXAMPLE AUGMENTATION ##############################################################\n",
    "    #load example\n",
    "    example = example_imgs[example_index]\n",
    "    resize_ratio = max(img.shape)/max(example.shape)\n",
    "    example = cv.resize(example, (int(example.shape[1]*resize_ratio), int(example.shape[0]*resize_ratio)))\n",
    "    #Make a black border aroud the example\n",
    "    example[0:2,:,:] = np.array([0,0,0])\n",
    "    example[-3:-1,:,:] = np.array([0,0,0])\n",
    "    example[:,0:2,:] = np.array([0,0,0])\n",
    "    example[:,-3:-1,:] = np.array([0,0,0])\n",
    "\n",
    "    #get example mask\n",
    "    example_mask = np.where(example == np.array([0,0,0]), np.zeros_like(example), 255*np.ones_like(example))\n",
    "    example_mask = example_mask.astype(np.uint8)\n",
    "    example_mask = cv.cvtColor(example_mask, cv.COLOR_BGR2GRAY)\n",
    "    #convert to bitmap\n",
    "    example_mask = np.where(example_mask == 0, 0, 255)\n",
    "    example_mask = example_mask.astype(np.uint8)\n",
    "    #back to BGR\n",
    "    example_mask = cv.cvtColor(example_mask, cv.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # add noise to the example\n",
    "    std = 20\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.subtract(example, noisem)\n",
    "    noisep = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.add(example, noisep)\n",
    "\n",
    "    #set zero where example mask is zero\n",
    "    example = np.where(example_mask == np.array([0,0,0]), np.zeros_like(example), example)\n",
    "\n",
    "    #perspective transform example\n",
    "    perspective_deformation = img.shape[0]//10 ################# DEFORMATION\n",
    "    pts1 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = pts2 + np.float32(randint(0,perspective_deformation,size=pts2.shape))\n",
    "    # print(f'pts2 = \\n{pts2}')\n",
    "    new_size_x = int(np.max(pts2[:,0]) - np.min(pts2[:,0]))\n",
    "    new_size_y = int(np.max(pts2[:,1]) - np.min(pts2[:,1]))\n",
    "    M = cv.getPerspectiveTransform(pts1,pts2)\n",
    "    example = cv.warpPerspective(example,M,(new_size_x,new_size_y))\n",
    "\n",
    "    #resize example keeping proportions\n",
    "    img_example_ratio = min(img.shape[0]/example.shape[0], img.shape[1]/example.shape[1])\n",
    "    scale_factor = np.random.uniform(.4, .98) ##.15, .35############################ PARAM ##############################\n",
    "    scale_factor = scale_factor * img_example_ratio\n",
    "    example = cv.resize(example, (0,0), fx=scale_factor, fy=scale_factor)\n",
    "    #match img shape\n",
    "    example_canvas = np.zeros((img.shape[0], img.shape[1], num_channels), dtype=np.uint8)\n",
    "\n",
    "    #get a random position for the example\n",
    "    example_y = randint(0, img.shape[0] - example.shape[0])\n",
    "    example_x = randint(0, img.shape[1] - example.shape[1])\n",
    "    #paste example on canvas\n",
    "    example_canvas[example_y:example_y+example.shape[0], example_x:example_x+example.shape[1]] = example\n",
    "\n",
    "    #canvas mask\n",
    "    example_canvas_mask = example_canvas.copy()\n",
    "    example_canvas_mask = cv.cvtColor(example_canvas_mask, cv.COLOR_BGR2GRAY)\n",
    "    example_canvas_mask = np.where(example_canvas_mask == 0, 0, 255)\n",
    "    example_canvas_mask = example_canvas_mask.astype(np.uint8)\n",
    "    edge_of_canvas = cv.Canny(example_canvas_mask, 100, 200)\n",
    "    ker_rand = randint(2,5)\n",
    "    edge_of_canvas = cv.dilate(edge_of_canvas, np.ones((ker_rand,ker_rand)))\n",
    "    #erode mask\n",
    "    ker_rand = randint(2,5)\n",
    "    kernel = np.ones((ker_rand,ker_rand),np.uint8)\n",
    "    example_canvas_mask = cv.erode(example_canvas_mask,kernel,iterations = randint(1,3))\n",
    "\n",
    "    #convert to hsv\n",
    "    example_canvas = cv.cvtColor(example_canvas, cv.COLOR_BGR2HSV)\n",
    "    #get the hsv channels\n",
    "    example_canvas_h = example_canvas[:,:,0]\n",
    "    example_canvas_s = example_canvas[:,:,1]\n",
    "    example_canvas_v = example_canvas[:,:,2]\n",
    "\n",
    "    # #reduce brightness\n",
    "    brightness_shift = randint(-80,5)\n",
    "    # example_canvas_v = np.clip(example_canvas_v + brightness_shift, 0, 255).astype(np.uint8)\n",
    "    if brightness_shift > 0:\n",
    "        example_canvas_v = cv.add(example_canvas_v, np.ones_like(example_canvas_v)*brightness_shift)\n",
    "    elif brightness_shift < 0:\n",
    "        example_canvas_v = cv.subtract(example_canvas_v, np.ones_like(example_canvas_v)*abs(brightness_shift))\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.25,.9)\n",
    "    example_canvas_v = np.clip(127*(1-const) + example_canvas_v*const, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #augment hue\n",
    "    hue_shift = randint(-7,7)\n",
    "    example_canvas_h = (example_canvas_h + hue_shift) % 180\n",
    "\n",
    "    #augment saturation\n",
    "    sat_shift = randint(-100,0)\n",
    "    example_canvas_s = np.clip(example_canvas_s + sat_shift, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #rebuild the channels\n",
    "    example_canvas[:,:,0] = example_canvas_h\n",
    "    example_canvas[:,:,1] = example_canvas_s\n",
    "    example_canvas[:,:,2] = example_canvas_v\n",
    "    #back to bgr\n",
    "    example_canvas = cv.cvtColor(example_canvas, cv.COLOR_HSV2BGR)\n",
    "\n",
    "    #paste canvas on img\n",
    "    img_b = img[:,:,0]\n",
    "    img_g = img[:,:,1]\n",
    "    img_r = img[:,:,2]\n",
    "    example_b = example_canvas[:,:,0]\n",
    "    example_g = example_canvas[:,:,1]\n",
    "    example_r = example_canvas[:,:,2]\n",
    "    img_b = np.where(example_canvas_mask > 0, example_b, img_b)\n",
    "    img_g = np.where(example_canvas_mask > 0, example_g, img_g)\n",
    "    img_r = np.where(example_canvas_mask > 0, example_r, img_r)\n",
    "    # img = np.where(example_canvas_mask > 0, example_canvas, img) \n",
    "    img[:,:,0] = img_b\n",
    "    img[:,:,1] = img_g\n",
    "    img[:,:,2] = img_r\n",
    "\n",
    "    ker_rand = randint(2,5)\n",
    "    blurred_img = cv.blur(img, (ker_rand,ker_rand))\n",
    "    img = np.where(edge_of_canvas ==np.array([0,0,0]), blurred_img, img)\n",
    "\n",
    "    ##########################################################################################\n",
    "    \n",
    "    # convert whole img to hsv\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "    #get the hsv channels\n",
    "    img_h = img[:,:,0]\n",
    "    img_s = img[:,:,1]\n",
    "    img_v = img[:,:,2]\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.6,.99)\n",
    "    img_v = np.clip(127*(1-const) + img_v*const, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #reduce brightness\n",
    "    brightness_shift = randint(-40,0)\n",
    "    if brightness_shift > 0:\n",
    "        img_v = cv.add(img_v, np.ones_like(img_v)*brightness_shift)\n",
    "    elif brightness_shift < 0:\n",
    "        img_v = cv.subtract(img_v, np.ones_like(img_v)*abs(brightness_shift))\n",
    "\n",
    "    #augment hue\n",
    "    hue_shift = randint(-2,2)\n",
    "    img_h = (img_h + hue_shift) % 180\n",
    "\n",
    "    #augment saturation\n",
    "    sat_shift = randint(-20,0)\n",
    "    img_s = np.clip(img_s + sat_shift, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #rebuild the channels\n",
    "    img[:,:,0] = img_h\n",
    "    img[:,:,1] = img_s\n",
    "    img[:,:,2] = img_v\n",
    "    #back to bgr\n",
    "    img = cv.cvtColor(img, cv.COLOR_HSV2BGR)\n",
    "\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = img.shape[0]//5\n",
    "    offset = np.random.randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        # img[:offset, :] = np.random.randint(0,255)\n",
    "        img = img[offset:, :]\n",
    "    elif offset < 0:\n",
    "        # img[offset:, :] = np.random.randint(0,255)\n",
    "        img = img[:offset, :]\n",
    "\n",
    "    offset_y = np.random.randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset_y, axis=1)\n",
    "    if offset_y > 0:\n",
    "        # img[:, :offset_y] = np.random.randint(0,255)\n",
    "        img = img[:, offset_y:]\n",
    "    elif offset_y < 0:\n",
    "        # img[:, offset_y:] = np.random.randint(0,255)\n",
    "        img = img[:, :offset_y]\n",
    "\n",
    "    min_dim = min(img.shape[0], img.shape[1])\n",
    "    #crop to square\n",
    "    img = img[:min_dim, :min_dim]\n",
    "\n",
    "    #add noise\n",
    "    std = 50\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "\n",
    "    # # crop into the img at random position\n",
    "    # zoom = randint(0, SIZE[0]//4)\n",
    "    # img = img[zoom:img.shape[0]-zoom, zoom:img.shape[1]-zoom]\n",
    "\n",
    "    # #blur \n",
    "    # b = randint(2,5)\n",
    "    # img = cv.blur(img, (b,b))\n",
    "    \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    #convert to hsv\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('sign_imgs', 'backgrounds',  f'background_{i+1}.png'))\n",
    "    # img = cv.resize(img, (160,120))\n",
    "    # img = None\n",
    "    img = load_and_augment_img(img, example_index=(i%tot_examples))\n",
    "\n",
    "    #to bgr\n",
    "    img = cv.cvtColor(img, cv.COLOR_HSV2BGR)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(200)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = os.path.join('sign_imgs', 'backgrounds')\n",
    "        self.data = []\n",
    "        self.class_names = []\n",
    "        self.channels = channels\n",
    "        self.all_imgs = torch.zeros((max_load*tot_examples, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "        cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "        # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN\n",
    "\n",
    "        for i in tqdm(range(max_load)):\n",
    "            img = cv.imread(os.path.join(self.folder, f'background_{i+1}.png'))\n",
    "            # img = cv.resize(img, (160,120))\n",
    "            for j in range(tot_examples):\n",
    "                img_j = load_and_augment_img(img.copy(), example_index=j)\n",
    "                if i < 100:\n",
    "                    cv.imshow('img', img_j)\n",
    "                    cv.waitKey(1)\n",
    "                    if i == 99:\n",
    "                        cv.destroyAllWindows()\n",
    "                #add a dimension to the image\n",
    "                img_j = img_j[:, :,np.newaxis] if self.channels == 1 else img_j\n",
    "                #convert to tensor\n",
    "                img_j = torch.from_numpy(img_j)\n",
    "                self.all_imgs[i*tot_examples+j] = img_j\n",
    "                self.class_names.append(example_labels[j])\n",
    "        \n",
    "        self.data = torch.from_numpy(np.array(self.data))\n",
    "        self.class_names = torch.from_numpy(np.array(self.class_names))\n",
    "        print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "        print(f'class_names: {self.class_names.shape}')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.class_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        class_label = self.class_names[idx]\n",
    "        return img, class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]/home/irong/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/ipykernel_launcher.py:146: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "100%|██████████| 5000/5000 [04:27<00:00, 18.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all imgs: torch.Size([75000, 32, 32, 3])\n",
      "class_names: torch.Size([75000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset(max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4*8192//3, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8192//3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10922, 3, 32, 32])\n",
      "torch.Size([10922])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, class_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    class_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, class_label) in tqdm(dataloader):\n",
    "        #one hot encode the class label\n",
    "        class_label = torch.eye(tot_classes)[class_label]\n",
    "        # Move the input and target data to the selected device\n",
    "        input, class_label = input.to(device), class_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        class_out = output[:, 0:]\n",
    "\n",
    "        #classification loss\n",
    "        assert class_label.shape == class_out.shape, f'class_label.shape: {class_label.shape}, output.shape: {output.shape}'\n",
    "\n",
    "        class_loss = 1.0*class_loss_fn(class_out, class_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = class_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        class_losses.append(class_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    class_loss = np.mean(class_losses)\n",
    "    return  np.sqrt(class_loss)\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(sign_classifier, val_dataloader, regr_loss_fn, class_loss_fn, device=device):\n",
    "    sign_classifier.eval()\n",
    "    y_losses = []\n",
    "    x_losses = []\n",
    "    w_losses = []\n",
    "    class_losses = []\n",
    "    for (input, class_label) in tqdm(val_dataloader):\n",
    "        #one hot encode the class label\n",
    "        class_label = torch.eye(tot_classes)[class_label]\n",
    "        input, class_label = input.to(device), class_label.to(device)\n",
    "        output = sign_classifier(input)\n",
    "\n",
    "        class_loss = 1.0*class_loss_fn(output[:, 0:], class_label)\n",
    "    \n",
    "        class_losses.append(class_loss.detach().cpu().numpy())\n",
    "    return  np.mean(class_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  150/150\n",
      "class_loss: 0.5370 --- Val: 0.1990\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.002 #0.005\n",
    "epochs = 200 #50+\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 0*1e-4 #9e-4\n",
    "L2_lambda = 0*1e-3 #1e-2\n",
    "optimizer = torch.optim.Adam(sign_classifier.parameters(), lr=lr, weight_decay=3e-5) #wd = 2e-3# 9e-5\n",
    "\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "class_loss_fn = nn.CrossEntropyLoss()\n",
    "best_val = 100\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        class_loss = train_epoch(sign_classifier, train_dataloader, regr_loss_fn, class_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_class_loss = val_epoch(sign_classifier, val_dataloader, regr_loss_fn, class_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs}\")\n",
    "    print(f\"class_loss: {class_loss:.4f} --- Val: {val_class_loss:.4f}\")\n",
    "    if val_class_loss < best_val:\n",
    "        torch.save(sign_classifier.state_dict(), model_name)\n",
    "        print(f'Model saved as {model_name}')\n",
    "        best_val = val_class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "# val_dataloader2 = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "# sign_classifier.load_state_dict(torch.load(model_name))\n",
    "# sign_classifier.eval()\n",
    "# for (input, class_label) in tqdm(val_dataloader2):\n",
    "#     #convert img to numpy array\n",
    "#     img = input[0].detach().cpu().numpy()[0]\n",
    "#     #convert to sigle byte\n",
    "#     img = img.astype(np.uint8)\n",
    "#     input, box_label, class_label =input.to(device), box_label.to(device), class_label.to(device)\n",
    "#     output = sign_classifier(input)\n",
    "#     x = output[:, 0]\n",
    "#     y = output[:, 1]\n",
    "#     w = output[:, 2]\n",
    "#     class_out = output[:, 3:]\n",
    "#     # print(class_out)\n",
    "#     class_out = torch.argmax(class_out, dim=1)\n",
    "#     class_out = class_out.detach().cpu().numpy()\n",
    "#     # print(f\"Predicted class: {class_out[0]}, Actual class: {class_label[0]}\")\n",
    "#     class_out = class_out[0]\n",
    "#     class_out = class_names[class_out]\n",
    "#     true_class = class_names[class_label[0]]\n",
    "#     img = draw_box(img, x, y, w)\n",
    "#     #text for labels\n",
    "#     img = cv.putText(img, f\"True: {true_class}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "#     img = cv.putText(img, f\"Pred: {class_out}\", (10, 60), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "#     cv.imshow('img', img)\n",
    "#     key = cv.waitKey(0)\n",
    "#     if key == 27:\n",
    "#         break\n",
    "    \n",
    "# cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 3, 3, 3)\n",
      "(8, 8, 4, 4)\n",
      "(32, 8, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(sign_classifier.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 4, 4, 'conv0', size=(10,10))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignClassifier(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Dropout(p=0.2, inplace=False)\n",
       "    (5): Conv2d(8, 8, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout(p=0.2, inplace=False)\n",
       "    (10): Conv2d(8, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "sign_classifier.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "sign_classifier.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "sign_classifier.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(sign_classifier, dummy_input, onnx_sign_classifier_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "sign_classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 5639.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[-5.2916737   0.85433453 -6.472851   -7.673906   -2.7713714   2.7960768\n",
      "  -1.2932487  -1.6018399  -6.7858233   3.257645  ]]\n",
      "Predictions shape: (1, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_sign_classifier_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
