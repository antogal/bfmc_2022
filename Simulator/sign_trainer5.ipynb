{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "# NOTE : Sign detecttion uses the folder training_images to load the backgrounds\n",
    "# it uses the folder sign_imgs to load the signs, the rate of examples is important\n",
    "num_channels = 3 # COLOR IMGS\n",
    "SIZE = (16, 16)\n",
    "model_name = 'models/sign_classifier.pt'\n",
    "onnx_sign_classifier_path = \"models/sign_classifier.onnx\"\n",
    "max_load = 5_000 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 examples loaded for 11 class_names\n",
      "example labels: [9, 4, 8, 7, 6, 9, 9, 9, 5, 10, 5, 10, 2, 6, 1, 7, 1, 0, 8, 3, 4, 2, 10, 8, 7, 0, 4, 6, 3, 0, 5, 1, 9, 2, 9, 3]\n"
     ]
    }
   ],
   "source": [
    "# LOAD EXAMPLES\n",
    "# imgs of the examples must be in a specific folder (ex: sign_imgs), and must be named as \"class_<number>.png\"\n",
    "# ex: sign_imgs/stop_1.png, note: start from 1\n",
    "\n",
    "#load examples\n",
    "examples_folder = 'sign_imgs'     \n",
    "class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway', 'nosign', 'trafficlight']\n",
    "# class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway']\n",
    "\n",
    "file_names = [f for f in os.listdir(examples_folder) if f.endswith('.png')]\n",
    "example_labels = [class_names.index(name.split('_')[0]) for name in file_names if name.split('_')[0] in class_names]\n",
    "example_imgs = [cv.resize(cv.blur(cv.imread(os.path.join(examples_folder, name)), (5,5)), (128,128)) for name in file_names if name.split('_')[0] in class_names]\n",
    "tot_examples = len(example_imgs)\n",
    "tot_classes = len(class_names) \n",
    "print(f'{tot_examples} examples loaded for {tot_classes} class_names')\n",
    "print(f'example labels: {example_labels}')    \n",
    "\n",
    "#show images\n",
    "cv.namedWindow('example', cv.WINDOW_NORMAL)\n",
    "for i in range(tot_examples):\n",
    "    img = example_imgs[i].copy()\n",
    "    # add text label\n",
    "    cv.putText(img, class_names[example_labels[i]], (10,30), cv.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "    cv.imshow('example', img)\n",
    "    key = cv.waitKey(200)\n",
    "    if key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE 32x32\n",
    "\n",
    "# class SignClassifier(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=num_channels): \n",
    "#         super().__init__()\n",
    "#         p = 0.2\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 16, kernel_size=3, stride=1), #out = 12 - 28  \n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=10 - 14\n",
    "#             nn.BatchNorm2d(16),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.Conv2d(16, 8, kernel_size=4, stride=2), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=5\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.Conv2d(8, 64, kernel_size=5, stride=1), #out = 1\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             # nn.Linear(in_features=4*4*4, out_features=128),\n",
    "#             # nn.ReLU(True),\n",
    "#             # nn.Dropout(p),\n",
    "#             # nn.Linear(in_features=128, out_features=out_dim),\n",
    "#             nn.Linear(in_features=64*1*1, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# sign_classifier = SignClassifier(out_dim=tot_classes,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE 16x16\n",
    "\n",
    "class SignClassifier(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=num_channels): \n",
    "        super().__init__()\n",
    "        p = 0.3\n",
    "        ### Convoluational layers\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 16, kernel_size=5, stride=1), #out = 12\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1), #out=10\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(p),\n",
    "            nn.Conv2d(16, 16, kernel_size=5, stride=1), #out = 6\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1), #out=5\n",
    "            # nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p),\n",
    "            nn.Conv2d(16, 64, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            # nn.Linear(in_features=4*4*4, out_features=128),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.Dropout(p),\n",
    "            # nn.Linear(in_features=128, out_features=out_dim),\n",
    "            nn.Linear(in_features=64*1*1, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "sign_classifier = SignClassifier(out_dim=tot_classes,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 16, 16])\n",
      "out shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "sign_classifier.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = sign_classifier(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load coco dataset and create background img in the background folder\n",
    "\n",
    "# import json\n",
    "\n",
    "# MAX_BACKGROUNDS = 500_000\n",
    "\n",
    "# #get all file names inside fodler sign_imgs/coco_val\n",
    "# # coco_val_img_names = [os.path.join('sign_imgs', 'coco_val', f) for f in os.listdir(os.path.join('sign_imgs', 'coco_val'))]\n",
    "# coco_val_img_names = [f for f in os.listdir(os.path.join('sign_imgs', 'coco_val'))]\n",
    "\n",
    "# print(f'{len(coco_val_img_names)} images in coco val')\n",
    "\n",
    "# #get all instances\n",
    "# #load json file\n",
    "# with open('sign_imgs/coco_val/instances_val2017.json') as f:\n",
    "#     data = json.load(f)\n",
    "#     #print name of the file\n",
    "#     print(f'images = {len(data[\"images\"])}')\n",
    "#     print(f'annotations = {len(data[\"annotations\"])}')\n",
    "\n",
    "\n",
    "#     categories = {cat['id']:cat['name'] for cat in data['categories']}\n",
    "#     categories_by_name = {cat['name']:cat['id'] for cat in data['categories']}\n",
    "\n",
    "#     filtered_categories = ['traffic light', 'bicycle', 'car', 'motorcycle', 'bus', 'truck',\n",
    "#                             'fire hydrant', 'stop sign',  'parking meter']\n",
    "#     filtered_categories_idxs = [categories_by_name[c] for c in filtered_categories]\n",
    "\n",
    "#     img_names_by_id = {img['id']:img['file_name'] for img in data['images']}\n",
    "\n",
    "#     categ_by_id = {img['id']:[] for img in data['images']}\n",
    "\n",
    "#     for ann in data['annotations']:\n",
    "#         categ_id = ann['category_id']\n",
    "#         img_id = ann['image_id']\n",
    "#         if img_id in categ_by_id:\n",
    "#             categ_by_id[img_id].append(categories[categ_id])\n",
    "#         if categ_id in filtered_categories_idxs:\n",
    "#             img_names_by_id.pop(img_id, None)\n",
    "#             categ_by_id.pop(img_id, None)\n",
    "    \n",
    "#     for id, c in categ_by_id.items():\n",
    "#         if len(c) == 0:\n",
    "#             img_names_by_id.pop(id, None)\n",
    "\n",
    "#     print(f'final images = {len(img_names_by_id)}')\n",
    "\n",
    "#     BACKGROUND_SIZE = (320,240)\n",
    "#     idx = 0\n",
    "#     for k, img_name in img_names_by_id.items():\n",
    "#         img_name = img_names_by_id[k]\n",
    "#         categories_in_img = categ_by_id[k]\n",
    "#         img = cv.imread(os.path.join('sign_imgs', 'coco_val', img_name))\n",
    "#         img = cv.resize(img, (2*BACKGROUND_SIZE[0], 2*BACKGROUND_SIZE[1]))\n",
    "#         #divide the image into 4 parts\n",
    "#         img_parts = [img[:BACKGROUND_SIZE[1], :BACKGROUND_SIZE[0]],\n",
    "#                     img[:BACKGROUND_SIZE[1], BACKGROUND_SIZE[0]:],\n",
    "#                     img[BACKGROUND_SIZE[1]:, :BACKGROUND_SIZE[0]],\n",
    "#                     img[BACKGROUND_SIZE[1]:, BACKGROUND_SIZE[0]:]]\n",
    "\n",
    "#         for i in range(4):\n",
    "#             img_part = img_parts[i]\n",
    "#             #further divide the image into 4 parts\n",
    "#             img_part_parts = [img_part[:BACKGROUND_SIZE[1]//2, :BACKGROUND_SIZE[0]//2],\n",
    "#                             img_part[:BACKGROUND_SIZE[1]//2, BACKGROUND_SIZE[0]//2:],\n",
    "#                             img_part[BACKGROUND_SIZE[1]//2:, :BACKGROUND_SIZE[0]//2],\n",
    "#                             img_part[BACKGROUND_SIZE[1]//2:, BACKGROUND_SIZE[0]//2:]]\n",
    "#             for j in range(4):\n",
    "#                 img_part_part = img_part_parts[j]\n",
    "#                 cv.imshow(f'img_{i}{j}', img_part_parts[j])\n",
    "#                 idx += 1\n",
    "#                 cv.imwrite(os.path.join('sign_imgs', 'backgrounds', f'background_{idx}.png'), img_part_parts[j])\n",
    "\n",
    "#         print(f'{categories_in_img}')\n",
    "#         key = cv.waitKey(1)\n",
    "#         if key == 27 or idx > MAX_BACKGROUNDS:\n",
    "#             break\n",
    "\n",
    "# cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irong/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/ipykernel_launcher.py:155: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "\n",
    "def draw_box(img, x, y, w):\n",
    "    img = cv.rectangle(img, (int(x-w/2), int(y-w/2)), (int(x+w/2), int(y+w/2)), 255, 2)\n",
    "    return img\n",
    "\n",
    "def load_and_augment_img(img, example_index=0, example_imgs=example_imgs):\n",
    "    # img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "    if img is not None:\n",
    "        #convert to gray\n",
    "        # img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "        # #crop to the top right quadrant\n",
    "        img = img[0:img.shape[1]//2, img.shape[1]//2:]\n",
    "        canv_dim = randint(64//2, 240//2)  ## RANGE OF DIMENSION OF THE SIGN\n",
    "        start_x = randint(0, img.shape[1]-canv_dim) if img.shape[1] > canv_dim else 0\n",
    "        start_y = randint(0, img.shape[0]-canv_dim) if img.shape[0] > canv_dim else 0\n",
    "        img = img[start_y:start_y+canv_dim, start_x:start_x+canv_dim]\n",
    "        # img = cv.resize(img, (2*SIZE[0], 2*SIZE[1]))\n",
    "    else:\n",
    "        img = randint(0,255,(2*SIZE[0], 2*SIZE[1]), dtype=np.uint8)\n",
    "\n",
    "\n",
    "    background_avg_brightness = np.mean(img)\n",
    "\n",
    "    ## EXAMPLE AUGMENTATION ##############################################################\n",
    "    #load example\n",
    "    example = example_imgs[example_index]\n",
    "    resize_ratio = max(img.shape)/max(example.shape)\n",
    "    example = cv.resize(example, (int(example.shape[1]*resize_ratio), int(example.shape[0]*resize_ratio)))\n",
    "    #Make a black border aroud the example\n",
    "    example[0:2,:,:] = np.array([0,0,0])\n",
    "    example[-3:-1,:,:] = np.array([0,0,0])\n",
    "    example[:,0:2,:] = np.array([0,0,0])\n",
    "    example[:,-3:-1,:] = np.array([0,0,0])\n",
    "\n",
    "    #get example mask\n",
    "    example_mask = np.where(example == np.array([0,0,0]), np.zeros_like(example), 255*np.ones_like(example))\n",
    "    example_mask = example_mask.astype(np.uint8)\n",
    "    example_mask = cv.cvtColor(example_mask, cv.COLOR_BGR2GRAY)\n",
    "    #convert to bitmap\n",
    "    example_mask = np.where(example_mask == 0, 0, 255)\n",
    "    example_mask = example_mask.astype(np.uint8)\n",
    "    #back to BGR\n",
    "    example_mask = cv.cvtColor(example_mask, cv.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # add noise to the example\n",
    "    std = 20\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.subtract(example, noisem)\n",
    "    noisep = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.add(example, noisep)\n",
    "\n",
    "    #set zero where example mask is zero\n",
    "    example = np.where(example_mask == np.array([0,0,0]), np.zeros_like(example), example)\n",
    "\n",
    "    #perspective transform example\n",
    "    perspective_deformation = img.shape[0]//8 ################# DEFORMATION\n",
    "    pts1 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = pts2 + np.float32(randint(0,perspective_deformation,size=pts2.shape))\n",
    "    # print(f'pts2 = \\n{pts2}')\n",
    "    new_size_x = int(np.max(pts2[:,0]) - np.min(pts2[:,0]))\n",
    "    new_size_y = int(np.max(pts2[:,1]) - np.min(pts2[:,1]))\n",
    "    M = cv.getPerspectiveTransform(pts1,pts2)\n",
    "    example = cv.warpPerspective(example,M,(new_size_x,new_size_y))\n",
    "\n",
    "    #resize example keeping proportions\n",
    "    img_example_ratio = min(img.shape[0]/example.shape[0], img.shape[1]/example.shape[1])\n",
    "    scale_factor = np.random.uniform(.5, .98) ##.15, .35############################ PARAM ##############################\n",
    "    scale_factor = scale_factor * img_example_ratio\n",
    "    example = cv.resize(example, (0,0), fx=scale_factor, fy=scale_factor)\n",
    "    #match img shape\n",
    "    example_canvas = np.zeros((img.shape[0], img.shape[1], num_channels), dtype=np.uint8)\n",
    "\n",
    "    #get a random position for the example\n",
    "    example_y = randint(0, img.shape[0] - example.shape[0])\n",
    "    example_x = randint(0, img.shape[1] - example.shape[1])\n",
    "    #paste example on canvas\n",
    "    example_canvas[example_y:example_y+example.shape[0], example_x:example_x+example.shape[1]] = example\n",
    "\n",
    "    #canvas mask\n",
    "    example_canvas_mask = example_canvas.copy()\n",
    "    example_canvas_mask = cv.cvtColor(example_canvas_mask, cv.COLOR_BGR2GRAY)\n",
    "    example_canvas_mask = np.where(example_canvas_mask == 0, 0, 255)\n",
    "    example_canvas_mask = example_canvas_mask.astype(np.uint8)\n",
    "    edge_of_canvas = cv.Canny(example_canvas_mask, 100, 200)\n",
    "    ker_rand = randint(2,5)\n",
    "    edge_of_canvas = cv.dilate(edge_of_canvas, np.ones((ker_rand,ker_rand)))\n",
    "    #erode mask\n",
    "    ker_rand = randint(2,5) if SIZE == (32,32) else randint(1,3)\n",
    "    kernel = np.ones((ker_rand,ker_rand),np.uint8)\n",
    "    iter_rand = randint(1,3) if SIZE == (32,32) else randint(1,2)\n",
    "    example_canvas_mask = cv.erode(example_canvas_mask,kernel,iterations=iter_rand)\n",
    "\n",
    "    #convert to hsv\n",
    "    example_canvas = cv.cvtColor(example_canvas, cv.COLOR_BGR2HSV)\n",
    "    #get the hsv channels\n",
    "    example_canvas_h = example_canvas[:,:,0]\n",
    "    example_canvas_s = example_canvas[:,:,1]\n",
    "    example_canvas_v = example_canvas[:,:,2]\n",
    "\n",
    "    # #reduce brightness\n",
    "    example_avg_brightness = np.mean(example_canvas_v)\n",
    "    if example_avg_brightness > 0.0:\n",
    "        example_avg_brightness = np.mean(example_canvas_v, where=example_canvas_v>0)\n",
    "    diff = example_avg_brightness - background_avg_brightness\n",
    "    brightness_shift = int(round(diff) + randint(-50,5))\n",
    "    brightness_shift = np.clip(brightness_shift, -255, 255)\n",
    "    # example_canvas_v = np.clip(example_canvas_v + brightness_shift, 0, 255).astype(np.uint8)\n",
    "    if brightness_shift > 0:\n",
    "        example_canvas_v = cv.add(example_canvas_v, np.ones_like(example_canvas_v)*brightness_shift)\n",
    "    elif brightness_shift < 0:\n",
    "        example_canvas_v = cv.subtract(example_canvas_v, np.ones_like(example_canvas_v)*abs(brightness_shift))\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.25,.9)\n",
    "    example_canvas_v = np.clip(127*(1-const) + example_canvas_v*const, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #augment hue\n",
    "    hue_shift = randint(-7,7)\n",
    "    example_canvas_h = (example_canvas_h + hue_shift) % 180\n",
    "\n",
    "    #augment saturation\n",
    "    sat_shift = randint(-120,0)\n",
    "    example_canvas_s = np.clip(example_canvas_s + sat_shift, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #rebuild the channels\n",
    "    example_canvas[:,:,0] = example_canvas_h\n",
    "    example_canvas[:,:,1] = example_canvas_s\n",
    "    example_canvas[:,:,2] = example_canvas_v\n",
    "    #back to bgr\n",
    "    example_canvas = cv.cvtColor(example_canvas, cv.COLOR_HSV2BGR)\n",
    "\n",
    "    #paste canvas on img\n",
    "    img_b = img[:,:,0]\n",
    "    img_g = img[:,:,1]\n",
    "    img_r = img[:,:,2]\n",
    "    example_b = example_canvas[:,:,0]\n",
    "    example_g = example_canvas[:,:,1]\n",
    "    example_r = example_canvas[:,:,2]\n",
    "    img_b = np.where(example_canvas_mask > 0, example_b, img_b)\n",
    "    img_g = np.where(example_canvas_mask > 0, example_g, img_g)\n",
    "    img_r = np.where(example_canvas_mask > 0, example_r, img_r)\n",
    "    # img = np.where(example_canvas_mask > 0, example_canvas, img) \n",
    "    img[:,:,0] = img_b\n",
    "    img[:,:,1] = img_g\n",
    "    img[:,:,2] = img_r\n",
    "\n",
    "    ker_rand = randint(2,5)\n",
    "    blurred_img = cv.blur(img, (ker_rand,ker_rand))\n",
    "    img = np.where(edge_of_canvas ==np.array([0,0,0]), blurred_img, img)\n",
    "\n",
    "    ##########################################################################################\n",
    "    \n",
    "    # convert whole img to hsv\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "    #get the hsv channels\n",
    "    img_h = img[:,:,0]\n",
    "    img_s = img[:,:,1]\n",
    "    img_v = img[:,:,2]\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.5,.99)\n",
    "    img_v = np.clip(127*(1-const) + img_v*const, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #reduce brightness\n",
    "    brightness_shift = randint(-40,0)\n",
    "    if brightness_shift > 0:\n",
    "        img_v = cv.add(img_v, np.ones_like(img_v)*brightness_shift)\n",
    "    elif brightness_shift < 0:\n",
    "        img_v = cv.subtract(img_v, np.ones_like(img_v)*abs(brightness_shift))\n",
    "\n",
    "    #augment hue\n",
    "    hue_shift = randint(-2,2)\n",
    "    img_h = (img_h + hue_shift) % 180\n",
    "\n",
    "    #augment saturation\n",
    "    sat_shift = randint(-20,0)\n",
    "    img_s = np.clip(img_s + sat_shift, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #rebuild the channels\n",
    "    img[:,:,0] = img_h\n",
    "    img[:,:,1] = img_s\n",
    "    img[:,:,2] = img_v\n",
    "    #back to bgr\n",
    "    img = cv.cvtColor(img, cv.COLOR_HSV2BGR)\n",
    "\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = img.shape[0]//4\n",
    "    offset = np.random.randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        # img[:offset, :] = np.random.randint(0,255)\n",
    "        img = img[offset:, :]\n",
    "    elif offset < 0:\n",
    "        # img[offset:, :] = np.random.randint(0,255)\n",
    "        img = img[:offset, :]\n",
    "\n",
    "    offset_y = np.random.randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset_y, axis=1)\n",
    "    if offset_y > 0:\n",
    "        # img[:, :offset_y] = np.random.randint(0,255)\n",
    "        img = img[:, offset_y:]\n",
    "    elif offset_y < 0:\n",
    "        # img[:, offset_y:] = np.random.randint(0,255)\n",
    "        img = img[:, :offset_y]\n",
    "\n",
    "    min_dim = min(img.shape[0], img.shape[1])\n",
    "    #crop to square\n",
    "    img = img[:min_dim, :min_dim]\n",
    "\n",
    "    #add noise\n",
    "    std = 50\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "\n",
    "    # # crop into the img at random position\n",
    "    # zoom = randint(0, SIZE[0]//4)\n",
    "    # img = img[zoom:img.shape[0]-zoom, zoom:img.shape[1]-zoom]\n",
    "\n",
    "    #blur \n",
    "    b = randint(1,4) \n",
    "    img = cv.blur(img, (b,b))\n",
    "    \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    #convert to hsv\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('sign_imgs', 'backgrounds',  f'background_{i+1}.png'))\n",
    "    # img = cv.resize(img, (160,120))\n",
    "    # img = None\n",
    "    img = load_and_augment_img(img, example_index=(i%tot_examples))\n",
    "\n",
    "    #to bgr\n",
    "    img = cv.cvtColor(img, cv.COLOR_HSV2BGR)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(200)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = os.path.join('sign_imgs', 'backgrounds')\n",
    "        self.data = []\n",
    "        self.class_names = []\n",
    "        self.channels = channels\n",
    "        self.all_imgs = torch.zeros((max_load*tot_examples, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "        cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "        # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN\n",
    "\n",
    "        for i in tqdm(range(max_load)):\n",
    "            img = cv.imread(os.path.join(self.folder, f'background_{i+1}.png'))\n",
    "            # img = cv.resize(img, (160,120))\n",
    "            for j in range(tot_examples):\n",
    "                img_j = load_and_augment_img(img.copy(), example_index=j)\n",
    "                if i < 100:\n",
    "                    cv.imshow('img', img_j)\n",
    "                    cv.waitKey(1)\n",
    "                    if i == 99:\n",
    "                        cv.destroyAllWindows()\n",
    "                #add a dimension to the image\n",
    "                img_j = img_j[:, :,np.newaxis] if self.channels == 1 else img_j\n",
    "                #convert to tensor\n",
    "                img_j = torch.from_numpy(img_j)\n",
    "                self.all_imgs[i*tot_examples+j] = img_j\n",
    "                self.class_names.append(example_labels[j])\n",
    "        \n",
    "        self.data = torch.from_numpy(np.array(self.data))\n",
    "        self.class_names = torch.from_numpy(np.array(self.class_names))\n",
    "        print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "        print(f'class_names: {self.class_names.shape}')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.class_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        class_label = self.class_names[idx]\n",
    "        return img, class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/home/irong/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/ipykernel_launcher.py:155: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "100%|██████████| 10000/10000 [09:02<00:00, 18.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all imgs: torch.Size([150000, 16, 16, 3])\n",
      "class_names: torch.Size([150000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset(max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=5*8192//3, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8192//3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13653, 3, 16, 16])\n",
      "torch.Size([13653])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, class_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    class_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, class_label) in tqdm(dataloader):\n",
    "        #one hot encode the class label\n",
    "        class_label = torch.eye(tot_classes)[class_label]\n",
    "        # Move the input and target data to the selected device\n",
    "        input, class_label = input.to(device), class_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        class_out = output[:, 0:]\n",
    "\n",
    "        #classification loss\n",
    "        assert class_label.shape == class_out.shape, f'class_label.shape: {class_label.shape}, output.shape: {output.shape}'\n",
    "\n",
    "        class_loss = 1.0*class_loss_fn(class_out, class_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = class_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        class_losses.append(class_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    class_loss = np.mean(class_losses)\n",
    "    return  np.sqrt(class_loss)\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(sign_classifier, val_dataloader, regr_loss_fn, class_loss_fn, device=device):\n",
    "    sign_classifier.eval()\n",
    "    y_losses = []\n",
    "    x_losses = []\n",
    "    w_losses = []\n",
    "    class_losses = []\n",
    "    for (input, class_label) in tqdm(val_dataloader):\n",
    "        #one hot encode the class label\n",
    "        class_label = torch.eye(tot_classes)[class_label]\n",
    "        input, class_label = input.to(device), class_label.to(device)\n",
    "        output = sign_classifier(input)\n",
    "\n",
    "        class_loss = 1.0*class_loss_fn(output[:, 0:], class_label)\n",
    "    \n",
    "        class_losses.append(class_loss.detach().cpu().numpy())\n",
    "    return  np.mean(class_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  200/200\n",
      "class_loss: 0.3199 --- Val: 0.1009\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.002 #0.005\n",
    "epochs = 200 #50+\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 1e-4 #9e-4\n",
    "L2_lambda = 1e-3 #1e-2\n",
    "optimizer = torch.optim.Adam(sign_classifier.parameters(), lr=lr, weight_decay=3e-5) #wd = 2e-3# 9e-5\n",
    "\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "class_loss_fn = nn.CrossEntropyLoss()\n",
    "best_val = 100\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        class_loss = train_epoch(sign_classifier, train_dataloader, regr_loss_fn, class_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_class_loss = val_epoch(sign_classifier, val_dataloader, regr_loss_fn, class_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs}\")\n",
    "    print(f\"class_loss: {class_loss:.4f} --- Val: {val_class_loss:.4f}\")\n",
    "    if val_class_loss < best_val:\n",
    "        torch.save(sign_classifier.state_dict(), model_name)\n",
    "        print(f'Model saved as {model_name}')\n",
    "        best_val = val_class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "# val_dataloader2 = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "# sign_classifier.load_state_dict(torch.load(model_name))\n",
    "# sign_classifier.eval()\n",
    "# for (input, class_label) in tqdm(val_dataloader2):\n",
    "#     #convert img to numpy array\n",
    "#     img = input[0].detach().cpu().numpy()[0]\n",
    "#     #convert to sigle byte\n",
    "#     img = img.astype(np.uint8)\n",
    "#     input, box_label, class_label =input.to(device), box_label.to(device), class_label.to(device)\n",
    "#     output = sign_classifier(input)\n",
    "#     x = output[:, 0]\n",
    "#     y = output[:, 1]\n",
    "#     w = output[:, 2]\n",
    "#     class_out = output[:, 3:]\n",
    "#     # print(class_out)\n",
    "#     class_out = torch.argmax(class_out, dim=1)\n",
    "#     class_out = class_out.detach().cpu().numpy()\n",
    "#     # print(f\"Predicted class: {class_out[0]}, Actual class: {class_label[0]}\")\n",
    "#     class_out = class_out[0]\n",
    "#     class_out = class_names[class_out]\n",
    "#     true_class = class_names[class_label[0]]\n",
    "#     img = draw_box(img, x, y, w)\n",
    "#     #text for labels\n",
    "#     img = cv.putText(img, f\"True: {true_class}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "#     img = cv.putText(img, f\"Pred: {class_out}\", (10, 60), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "#     cv.imshow('img', img)\n",
    "#     key = cv.waitKey(0)\n",
    "#     if key == 27:\n",
    "#         break\n",
    "    \n",
    "# cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 3, 5, 5)\n",
      "(16, 16, 5, 5)\n",
      "(64, 16, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(sign_classifier.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 4, 4, 'conv0', size=(10,10))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignClassifier(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Dropout(p=0.3, inplace=False)\n",
       "    (5): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout(p=0.3, inplace=False)\n",
       "    (10): Conv2d(16, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "sign_classifier.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "sign_classifier.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "sign_classifier.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(sign_classifier, dummy_input, onnx_sign_classifier_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "sign_classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 6582.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[-1.3253173e+01 -1.3685614e-02 -4.9231353e+00 -1.9703695e+01\n",
      "   3.7501073e+00  2.4160879e+00  6.6257277e+00 -7.7648468e+00\n",
      "  -1.8188124e+01  1.0895952e+00]]\n",
      "Predictions shape: (1, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_sign_classifier_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
