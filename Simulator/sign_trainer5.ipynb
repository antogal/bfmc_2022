{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "# NOTE : Sign detecttion uses the folder training_images to load the backgrounds\n",
    "# it uses the folder sign_imgs to load the signs, the rate of examples is important\n",
    "num_channels = 1 # COLOR IMGS\n",
    "SIZE = (16, 16)\n",
    "model_name = 'models/sign_classifier.pt'\n",
    "onnx_sign_classifier_path = \"models/sign_classifier.onnx\"\n",
    "max_load = 5000  #63000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 examples loaded for 10 class_names\n",
      "example labels: [7, 9, 9, 9, 5, 6, 1, 4, 8, 0, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# LOAD EXAMPLES\n",
    "# imgs of the examples must be in a specific folder (ex: sign_imgs), and must be named as \"class_<number>.png\"\n",
    "# ex: sign_imgs/stop_1.png, note: start from 1\n",
    "\n",
    "#load examples\n",
    "examples_folder = 'sign_imgs'     \n",
    "class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway', 'nosign'] \n",
    "# class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway', 'trafficlight', 'nosign']  # with traffic_light\n",
    "\n",
    "# class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway']\n",
    "\n",
    "file_names = [f for f in os.listdir(examples_folder) if f.endswith('.png')]\n",
    "example_labels = [class_names.index(name.split('_')[0]) for name in file_names if name.split('_')[0] in class_names]\n",
    "example_imgs = [cv.resize(cv.blur(cv.imread(os.path.join(examples_folder, name)), (5,5)), (128,128)) for name in file_names if name.split('_')[0] in class_names]\n",
    "example_masks = []\n",
    "for example in example_imgs:\n",
    "    example_mask = np.where(example == np.array([0,0,0]), np.zeros_like(example), 255*np.ones_like(example))\n",
    "    example_mask = example_mask.astype(np.uint8)\n",
    "    example_mask = cv.cvtColor(example_mask, cv.COLOR_BGR2GRAY)\n",
    "    #convert to bitmap\n",
    "    example_mask = np.where(example_mask == 0, 0, 255)\n",
    "    example_mask = example_mask.astype(np.uint8)\n",
    "    #back to BGR\n",
    "    example_mask = cv.cvtColor(example_mask, cv.COLOR_GRAY2BGR)\n",
    "    if np.max(example_mask) > 0:\n",
    "        example_masks.append(example_mask)\n",
    "\n",
    "tot_examples = len(example_imgs)\n",
    "tot_classes = len(class_names) \n",
    "print(f'{tot_examples} examples loaded for {tot_classes} class_names')\n",
    "print(f'example labels: {example_labels}')    \n",
    "\n",
    "#show images\n",
    "cv.namedWindow('example', cv.WINDOW_NORMAL)\n",
    "for i in range(tot_examples):\n",
    "    img = example_imgs[i].copy()\n",
    "    # add text label\n",
    "    cv.putText(img, class_names[example_labels[i]], (10,30), cv.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "    cv.imshow('example', img)\n",
    "    # cv.imshow('example_mask', example_masks[i])\n",
    "    key = cv.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE 32x32\n",
    "\n",
    "# class SignClassifier(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=num_channels): \n",
    "#         super().__init__()\n",
    "#         p = 0.2\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=3, stride=1), #out = 12 - 28  \n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=10 - 14\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.Conv2d(8, 4, kernel_size=4, stride=2), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=5\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.Conv2d(4, 32, kernel_size=5, stride=1), #out = 1\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             # nn.Linear(in_features=4*4*4, out_features=128),\n",
    "#             # nn.ReLU(True),\n",
    "#             # nn.Dropout(p),\n",
    "#             # nn.Linear(in_features=128, out_features=out_dim),\n",
    "#             nn.Linear(in_features=32*1*1, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# sign_classifier = SignClassifier(out_dim=tot_classes,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE 16x16\n",
    "\n",
    "class SignClassifier(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=num_channels): \n",
    "        super().__init__()\n",
    "        p = 0.3\n",
    "        ### Convoluational layers\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 4, kernel_size=5, stride=1), #out = 12\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1), #out=10\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p),\n",
    "            nn.Conv2d(4, 8, kernel_size=5, stride=1), #out = 6\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1), #out=5\n",
    "            # nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p),\n",
    "            nn.Conv2d(8, 64, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            # nn.Linear(in_features=4*4*4, out_features=128),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.Dropout(p),\n",
    "            # nn.Linear(in_features=128, out_features=out_dim),\n",
    "            nn.Linear(in_features=64*1*1, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "sign_classifier = SignClassifier(out_dim=tot_classes,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 16, 16])\n",
      "out shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "sign_classifier.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = sign_classifier(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load coco dataset and create background img in the background folder\n",
    "\n",
    "# import json\n",
    "\n",
    "# MAX_BACKGROUNDS = 500_000\n",
    "\n",
    "# #get all file names inside fodler sign_imgs/coco_val\n",
    "# # coco_val_img_names = [os.path.join('sign_imgs', 'coco_val', f) for f in os.listdir(os.path.join('sign_imgs', 'coco_val'))]\n",
    "# coco_val_img_names = [f for f in os.listdir(os.path.join('sign_imgs', 'coco_val'))]\n",
    "\n",
    "# print(f'{len(coco_val_img_names)} images in coco val')\n",
    "\n",
    "# #get all instances\n",
    "# #load json file\n",
    "# with open('sign_imgs/coco_val/instances_val2017.json') as f:\n",
    "#     data = json.load(f)\n",
    "#     #print name of the file\n",
    "#     print(f'images = {len(data[\"images\"])}')\n",
    "#     print(f'annotations = {len(data[\"annotations\"])}')\n",
    "\n",
    "\n",
    "#     categories = {cat['id']:cat['name'] for cat in data['categories']}\n",
    "#     categories_by_name = {cat['name']:cat['id'] for cat in data['categories']}\n",
    "\n",
    "#     filtered_categories = ['traffic light', 'bicycle', 'car', 'motorcycle', 'bus', 'truck',\n",
    "#                             'fire hydrant', 'stop sign',  'parking meter']\n",
    "#     filtered_categories_idxs = [categories_by_name[c] for c in filtered_categories]\n",
    "\n",
    "#     img_names_by_id = {img['id']:img['file_name'] for img in data['images']}\n",
    "\n",
    "#     categ_by_id = {img['id']:[] for img in data['images']}\n",
    "\n",
    "#     for ann in data['annotations']:\n",
    "#         categ_id = ann['category_id']\n",
    "#         img_id = ann['image_id']\n",
    "#         if img_id in categ_by_id:\n",
    "#             categ_by_id[img_id].append(categories[categ_id])\n",
    "#         if categ_id in filtered_categories_idxs:\n",
    "#             img_names_by_id.pop(img_id, None)\n",
    "#             categ_by_id.pop(img_id, None)\n",
    "    \n",
    "#     for id, c in categ_by_id.items():\n",
    "#         if len(c) == 0:\n",
    "#             img_names_by_id.pop(id, None)\n",
    "\n",
    "#     print(f'final images = {len(img_names_by_id)}')\n",
    "\n",
    "#     BACKGROUND_SIZE = (320,240)\n",
    "#     idx = 0\n",
    "#     for k, img_name in img_names_by_id.items():\n",
    "#         img_name = img_names_by_id[k]\n",
    "#         categories_in_img = categ_by_id[k]\n",
    "#         img = cv.imread(os.path.join('sign_imgs', 'coco_val', img_name))\n",
    "#         img = cv.resize(img, (2*BACKGROUND_SIZE[0], 2*BACKGROUND_SIZE[1]))\n",
    "#         #divide the image into 4 parts\n",
    "#         img_parts = [img[:BACKGROUND_SIZE[1], :BACKGROUND_SIZE[0]],\n",
    "#                     img[:BACKGROUND_SIZE[1], BACKGROUND_SIZE[0]:],\n",
    "#                     img[BACKGROUND_SIZE[1]:, :BACKGROUND_SIZE[0]],\n",
    "#                     img[BACKGROUND_SIZE[1]:, BACKGROUND_SIZE[0]:]]\n",
    "\n",
    "#         for i in range(4):\n",
    "#             img_part = img_parts[i]\n",
    "#             #further divide the image into 4 parts\n",
    "#             img_part_parts = [img_part[:BACKGROUND_SIZE[1]//2, :BACKGROUND_SIZE[0]//2],\n",
    "#                             img_part[:BACKGROUND_SIZE[1]//2, BACKGROUND_SIZE[0]//2:],\n",
    "#                             img_part[BACKGROUND_SIZE[1]//2:, :BACKGROUND_SIZE[0]//2],\n",
    "#                             img_part[BACKGROUND_SIZE[1]//2:, BACKGROUND_SIZE[0]//2:]]\n",
    "#             for j in range(4):\n",
    "#                 img_part_part = img_part_parts[j]\n",
    "#                 cv.imshow(f'img_{i}{j}', img_part_parts[j])\n",
    "#                 idx += 1\n",
    "#                 cv.imwrite(os.path.join('sign_imgs', 'backgrounds', f'background_{idx}.png'), img_part_parts[j])\n",
    "\n",
    "#         print(f'{categories_in_img}')\n",
    "#         key = cv.waitKey(1)\n",
    "#         if key == 27 or idx > MAX_BACKGROUNDS:\n",
    "#             break\n",
    "\n",
    "# cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irong/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/ipykernel_launcher.py:164: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "import random\n",
    "\n",
    "def draw_box(img, x, y, w):\n",
    "    img = cv.rectangle(img, (int(x-w/2), int(y-w/2)), (int(x+w/2), int(y+w/2)), 255, 2)\n",
    "    return img\n",
    "\n",
    "def load_and_augment_img(img, example_index=0, example_imgs=example_imgs):\n",
    "    # img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "    if img is not None:\n",
    "        #convert to gray\n",
    "        # img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "        # #crop to the top right quadrant\n",
    "        img = img[0:img.shape[1]//2, img.shape[1]//2:]\n",
    "        canv_dim = randint(64//2, 160//2)  ## RANGE OF DIMENSION OF THE SIGN\n",
    "        start_x = randint(0, img.shape[1]-canv_dim) if img.shape[1] > canv_dim else 0\n",
    "        start_y = randint(0, img.shape[0]-canv_dim) if img.shape[0] > canv_dim else 0\n",
    "        img = img[start_y:start_y+canv_dim, start_x:start_x+canv_dim]\n",
    "        # img = cv.resize(img, (2*SIZE[0], 2*SIZE[1]))\n",
    "    else:\n",
    "        img = randint(0,255,(2*SIZE[0], 2*SIZE[1]), dtype=np.uint8)\n",
    "\n",
    "\n",
    "    background_avg_brightness = np.mean(img)\n",
    "\n",
    "    ## EXAMPLE AUGMENTATION ##############################################################\n",
    "    #load example\n",
    "    example = example_imgs[example_index]\n",
    "    random_example_mask = random.choice(example_masks)\n",
    "    resize_ratio = max(img.shape)/max(example.shape)\n",
    "    example = cv.resize(example, (int(example.shape[1]*resize_ratio), int(example.shape[0]*resize_ratio)))\n",
    "    random_example_mask = cv.resize(random_example_mask, (example.shape[1], example.shape[0]))\n",
    "    #Make a black border aroud the example\n",
    "    example[0:2,:,:] = np.array([0,0,0])\n",
    "    example[-3:-1,:,:] = np.array([0,0,0])\n",
    "    example[:,0:2,:] = np.array([0,0,0])\n",
    "    example[:,-3:-1,:] = np.array([0,0,0])\n",
    "    random_example_mask[0:2,:,:] = 0\n",
    "    random_example_mask[-3:-1,:,:] = 0\n",
    "    random_example_mask[:,0:2,:] = 0\n",
    "    random_example_mask[:,-3:-1,:] = 0\n",
    "\n",
    "    #get example mask\n",
    "    example_mask = np.where(example == np.array([0,0,0]), np.zeros_like(example), 255*np.ones_like(example))\n",
    "    example_mask = example_mask.astype(np.uint8)\n",
    "    example_mask = cv.cvtColor(example_mask, cv.COLOR_BGR2GRAY)\n",
    "    #convert to bitmap\n",
    "    example_mask = np.where(example_mask == 0, 0, 255)\n",
    "    example_mask = example_mask.astype(np.uint8)\n",
    "    #back to BGR\n",
    "    example_mask = cv.cvtColor(example_mask, cv.COLOR_GRAY2BGR)\n",
    "    #bitwise and of the 2 masks\n",
    "    example_mask = cv.bitwise_and(example_mask, random_example_mask)\n",
    "    \n",
    "    # add noise to the example\n",
    "    std = 20\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.subtract(example, noisem)\n",
    "    noisep = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.add(example, noisep)\n",
    "\n",
    "    #set zero where example mask is zero\n",
    "    example = np.where(example_mask == np.array([0,0,0]), np.zeros_like(example), example)\n",
    "\n",
    "    #perspective transform example\n",
    "    perspective_deformation = img.shape[0]//8 ################# DEFORMATION\n",
    "    pts1 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = pts2 + np.float32(randint(0,perspective_deformation,size=pts2.shape))\n",
    "    # print(f'pts2 = \\n{pts2}')\n",
    "    new_size_x = int(np.max(pts2[:,0]) - np.min(pts2[:,0]))\n",
    "    new_size_y = int(np.max(pts2[:,1]) - np.min(pts2[:,1]))\n",
    "    M = cv.getPerspectiveTransform(pts1,pts2)\n",
    "    example = cv.warpPerspective(example,M,(new_size_x,new_size_y))\n",
    "\n",
    "    #resize example keeping proportions\n",
    "    img_example_ratio = min(img.shape[0]/example.shape[0], img.shape[1]/example.shape[1])\n",
    "    scale_factor = np.random.uniform(.5, .98) ##.15, .35############################ PARAM ##############################\n",
    "    scale_factor = scale_factor * img_example_ratio\n",
    "    example = cv.resize(example, (0,0), fx=scale_factor, fy=scale_factor)\n",
    "    #match img shape\n",
    "    example_canvas = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    #get a random position for the example\n",
    "    example_y = randint(0, img.shape[0] - example.shape[0])\n",
    "    example_x = randint(0, img.shape[1] - example.shape[1])\n",
    "    #paste example on canvas\n",
    "    example_canvas[example_y:example_y+example.shape[0], example_x:example_x+example.shape[1]] = example\n",
    "\n",
    "    #canvas mask\n",
    "    example_canvas_mask = example_canvas.copy()\n",
    "    example_canvas_mask = cv.cvtColor(example_canvas_mask, cv.COLOR_BGR2GRAY)\n",
    "    example_canvas_mask = np.where(example_canvas_mask == 0, 0, 255)\n",
    "    example_canvas_mask = example_canvas_mask.astype(np.uint8)\n",
    "    edge_of_canvas = cv.Canny(example_canvas_mask, 100, 200)\n",
    "    ker_rand = randint(2,5)\n",
    "    edge_of_canvas = cv.dilate(edge_of_canvas, np.ones((ker_rand,ker_rand)))\n",
    "    #erode mask\n",
    "    ker_rand = randint(2,5) if SIZE == (32,32) else randint(1,3)\n",
    "    kernel = np.ones((ker_rand,ker_rand),np.uint8)\n",
    "    iter_rand = randint(1,3) if SIZE == (32,32) else randint(1,3)\n",
    "    example_canvas_mask = cv.erode(example_canvas_mask,kernel,iterations=iter_rand)\n",
    "\n",
    "    #convert to hsv\n",
    "    example_canvas = cv.cvtColor(example_canvas, cv.COLOR_BGR2HSV)\n",
    "    #get the hsv channels\n",
    "    example_canvas_h = example_canvas[:,:,0]\n",
    "    example_canvas_s = example_canvas[:,:,1]\n",
    "    example_canvas_v = example_canvas[:,:,2]\n",
    "\n",
    "    # #reduce brightness\n",
    "    example_avg_brightness = np.mean(example_canvas_v)\n",
    "    if example_avg_brightness > 0.0:\n",
    "        example_avg_brightness = np.mean(example_canvas_v, where=example_canvas_v>0)\n",
    "    diff = -example_avg_brightness +background_avg_brightness\n",
    "    brightness_shift = int(round(diff) + randint(-150,150))\n",
    "    brightness_shift = np.clip(brightness_shift, -255, 255)\n",
    "    # example_canvas_v = np.clip(example_canvas_v + brightness_shift, 0, 255).astype(np.uint8)\n",
    "    if brightness_shift > 0:\n",
    "        example_canvas_v = cv.add(example_canvas_v, np.ones_like(example_canvas_v)*brightness_shift)\n",
    "    elif brightness_shift < 0:\n",
    "        example_canvas_v = cv.subtract(example_canvas_v, np.ones_like(example_canvas_v)*abs(brightness_shift))\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.25,.9)\n",
    "    example_canvas_v = np.clip(127*(1-const) + example_canvas_v*const, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #augment hue\n",
    "    hue_shift = randint(-10,10)\n",
    "    example_canvas_h = (example_canvas_h + hue_shift) % 180\n",
    "\n",
    "    #augment saturation\n",
    "    sat_shift = randint(-100,0)\n",
    "    example_canvas_s = np.clip(example_canvas_s + sat_shift, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #rebuild the channels\n",
    "    example_canvas[:,:,0] = example_canvas_h\n",
    "    example_canvas[:,:,1] = example_canvas_s\n",
    "    example_canvas[:,:,2] = example_canvas_v\n",
    "    #back to bgr\n",
    "    example_canvas = cv.cvtColor(example_canvas, cv.COLOR_HSV2BGR)\n",
    "\n",
    "    #paste canvas on img\n",
    "    img_b = img[:,:,0]\n",
    "    img_g = img[:,:,1]\n",
    "    img_r = img[:,:,2]\n",
    "    example_b = example_canvas[:,:,0]\n",
    "    example_g = example_canvas[:,:,1]\n",
    "    example_r = example_canvas[:,:,2]\n",
    "    img_b = np.where(example_canvas_mask > 0, example_b, img_b)\n",
    "    img_g = np.where(example_canvas_mask > 0, example_g, img_g)\n",
    "    img_r = np.where(example_canvas_mask > 0, example_r, img_r)\n",
    "    # img = np.where(example_canvas_mask > 0, example_canvas, img) \n",
    "    img[:,:,0] = img_b\n",
    "    img[:,:,1] = img_g\n",
    "    img[:,:,2] = img_r\n",
    "\n",
    "    ker_rand = randint(2,5)\n",
    "    blurred_img = cv.blur(img, (ker_rand,ker_rand))\n",
    "    img = np.where(edge_of_canvas ==np.array([0,0,0]), blurred_img, img)\n",
    "\n",
    "    ##########################################################################################\n",
    "    \n",
    "    # convert whole img to hsv\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "    #get the hsv channels\n",
    "    img_h = img[:,:,0]\n",
    "    img_s = img[:,:,1]\n",
    "    img_v = img[:,:,2]\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.5,.99)\n",
    "    img_v = np.clip(127*(1-const) + img_v*const, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #reduce brightness\n",
    "    brightness_shift = randint(-40,0)\n",
    "    if brightness_shift > 0:\n",
    "        img_v = cv.add(img_v, np.ones_like(img_v)*brightness_shift)\n",
    "    elif brightness_shift < 0:\n",
    "        img_v = cv.subtract(img_v, np.ones_like(img_v)*abs(brightness_shift))\n",
    "\n",
    "    #augment hue\n",
    "    hue_shift = randint(-2,2)\n",
    "    img_h = (img_h + hue_shift) % 180\n",
    "\n",
    "    #augment saturation\n",
    "    sat_shift = randint(-20,0)\n",
    "    img_s = np.clip(img_s + sat_shift, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #rebuild the channels\n",
    "    img[:,:,0] = img_h\n",
    "    img[:,:,1] = img_s\n",
    "    img[:,:,2] = img_v\n",
    "    #back to bgr\n",
    "    img = cv.cvtColor(img, cv.COLOR_HSV2BGR)\n",
    "\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = img.shape[0]//4\n",
    "    offset = np.random.randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        # img[:offset, :] = np.random.randint(0,255)\n",
    "        img = img[offset:, :]\n",
    "    elif offset < 0:\n",
    "        # img[offset:, :] = np.random.randint(0,255)\n",
    "        img = img[:offset, :]\n",
    "\n",
    "    offset_y = np.random.randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset_y, axis=1)\n",
    "    if offset_y > 0:\n",
    "        # img[:, :offset_y] = np.random.randint(0,255)\n",
    "        img = img[:, offset_y:]\n",
    "    elif offset_y < 0:\n",
    "        # img[:, offset_y:] = np.random.randint(0,255)\n",
    "        img = img[:, :offset_y]\n",
    "\n",
    "    min_dim = min(img.shape[0], img.shape[1])\n",
    "    #crop to square\n",
    "    img = img[:min_dim, :min_dim]\n",
    "\n",
    "    #add noise\n",
    "    std = 50\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "\n",
    "    # # crop into the img at random position\n",
    "    # zoom = randint(0, SIZE[0]//4)\n",
    "    # img = img[zoom:img.shape[0]-zoom, zoom:img.shape[1]-zoom]\n",
    "\n",
    "    #blur \n",
    "    b = randint(2,3) \n",
    "    img = cv.blur(img, (b,b))\n",
    "    \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    #convert to gray\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY) if num_channels == 1 else img\n",
    "\n",
    "    # #convert to hsv\n",
    "    # img = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('sign_imgs', 'backgrounds',  f'background_{i+1}.png'))\n",
    "    # img = cv.resize(img, (160,120))\n",
    "    # img = None\n",
    "    img = load_and_augment_img(img, example_index=(i%tot_examples))\n",
    "\n",
    "    #to bgr\n",
    "    # img = cv.cvtColor(img, cv.COLOR_HSV2BGR)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(200)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = os.path.join('sign_imgs', 'backgrounds')\n",
    "        self.data = []\n",
    "        self.class_names = []\n",
    "        self.channels = channels\n",
    "        self.all_imgs = torch.zeros((max_load*tot_examples, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "        cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "        # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN\n",
    "\n",
    "        for i in tqdm(range(max_load)):\n",
    "            img = cv.imread(os.path.join(self.folder, f'background_{i+1}.png'))\n",
    "            # img = cv.resize(img, (160,120))\n",
    "            for j in range(tot_examples):\n",
    "                img_j = load_and_augment_img(img.copy(), example_index=j)\n",
    "                if i < 50:\n",
    "                    cv.imshow('img', img_j)\n",
    "                    cv.waitKey(1)\n",
    "                    if i == 49:\n",
    "                        cv.destroyAllWindows()\n",
    "                #add a dimension to the image\n",
    "                img_j = img_j[:, :,np.newaxis] if self.channels == 1 else img_j\n",
    "                #convert to tensor\n",
    "                img_j = torch.from_numpy(img_j)\n",
    "                self.all_imgs[i*tot_examples+j] = img_j\n",
    "                self.class_names.append(example_labels[j])\n",
    "        \n",
    "        self.data = torch.from_numpy(np.array(self.data))\n",
    "        self.class_names = torch.from_numpy(np.array(self.class_names))\n",
    "        print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "        print(f'class_names: {self.class_names.shape}')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.class_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        class_label = self.class_names[idx]\n",
    "        return img, class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]/home/irong/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/ipykernel_launcher.py:164: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "100%|██████████| 5000/5000 [02:55<00:00, 28.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all imgs: torch.Size([60000, 16, 16, 1])\n",
      "class_names: torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset(max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=5*8192//3, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8192//3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13653, 1, 16, 16])\n",
      "torch.Size([13653])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, class_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    class_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, class_label) in tqdm(dataloader):\n",
    "        #one hot encode the class label\n",
    "        class_label = torch.eye(tot_classes)[class_label]\n",
    "        # Move the input and target data to the selected device\n",
    "        input, class_label = input.to(device), class_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        class_out = output[:, 0:]\n",
    "\n",
    "        #classification loss\n",
    "        assert class_label.shape == class_out.shape, f'class_label.shape: {class_label.shape}, output.shape: {output.shape}'\n",
    "\n",
    "        class_loss = 1.0*class_loss_fn(class_out, class_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = class_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        class_losses.append(class_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    class_loss = np.mean(class_losses)\n",
    "    return  np.sqrt(class_loss)\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(sign_classifier, val_dataloader, regr_loss_fn, class_loss_fn, device=device):\n",
    "    sign_classifier.eval()\n",
    "    y_losses = []\n",
    "    x_losses = []\n",
    "    w_losses = []\n",
    "    class_losses = []\n",
    "    for (input, class_label) in tqdm(val_dataloader):\n",
    "        #one hot encode the class label\n",
    "        class_label = torch.eye(tot_classes)[class_label]\n",
    "        input, class_label = input.to(device), class_label.to(device)\n",
    "        output = sign_classifier(input)\n",
    "\n",
    "        class_loss = 1.0*class_loss_fn(output[:, 0:], class_label)\n",
    "    \n",
    "        class_losses.append(class_loss.detach().cpu().numpy())\n",
    "    return  np.mean(class_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  400/400\n",
      "class_loss: 1.0161 --- Val: 0.9228\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.002 #0.005\n",
    "epochs = 400 #50+\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 1e-4 #9e-4\n",
    "L2_lambda = 1e-3 #1e-2\n",
    "optimizer = torch.optim.Adam(sign_classifier.parameters(), lr=lr, weight_decay=3e-5) #wd = 2e-3# 9e-5\n",
    "\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "class_loss_fn = nn.CrossEntropyLoss()\n",
    "best_val = 100\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        class_loss = train_epoch(sign_classifier, train_dataloader, regr_loss_fn, class_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_class_loss = val_epoch(sign_classifier, val_dataloader, regr_loss_fn, class_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs}\")\n",
    "    print(f\"class_loss: {class_loss:.4f} --- Val: {val_class_loss:.4f}\")\n",
    "    if val_class_loss < best_val:\n",
    "        torch.save(sign_classifier.state_dict(), model_name)\n",
    "        print(f'Model saved as {model_name}')\n",
    "        best_val = val_class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "# val_dataloader2 = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "# sign_classifier.load_state_dict(torch.load(model_name))\n",
    "# sign_classifier.eval()\n",
    "# for (input, class_label) in tqdm(val_dataloader2):\n",
    "#     #convert img to numpy array\n",
    "#     img = input[0].detach().cpu().numpy()[0]\n",
    "#     #convert to sigle byte\n",
    "#     img = img.astype(np.uint8)\n",
    "#     input, box_label, class_label =input.to(device), box_label.to(device), class_label.to(device)\n",
    "#     output = sign_classifier(input)\n",
    "#     x = output[:, 0]\n",
    "#     y = output[:, 1]\n",
    "#     w = output[:, 2]\n",
    "#     class_out = output[:, 3:]\n",
    "#     # print(class_out)\n",
    "#     class_out = torch.argmax(class_out, dim=1)\n",
    "#     class_out = class_out.detach().cpu().numpy()\n",
    "#     # print(f\"Predicted class: {class_out[0]}, Actual class: {class_label[0]}\")\n",
    "#     class_out = class_out[0]\n",
    "#     class_out = class_names[class_out]\n",
    "#     true_class = class_names[class_label[0]]\n",
    "#     img = draw_box(img, x, y, w)\n",
    "#     #text for labels\n",
    "#     img = cv.putText(img, f\"True: {true_class}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "#     img = cv.putText(img, f\"Pred: {class_out}\", (10, 60), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "#     cv.imshow('img', img)\n",
    "#     key = cv.waitKey(0)\n",
    "#     if key == 27:\n",
    "#         break\n",
    "    \n",
    "# cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 5, 5)\n",
      "(8, 4, 5, 5)\n",
      "(64, 8, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(sign_classifier.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 4, 4, 'conv0', size=(10,10))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignClassifier(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Dropout(p=0.3, inplace=False)\n",
       "    (5): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout(p=0.3, inplace=False)\n",
       "    (10): Conv2d(8, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "sign_classifier.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "sign_classifier.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "sign_classifier.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(sign_classifier, dummy_input, onnx_sign_classifier_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "sign_classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 10881.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[ 0.57760376  1.8871096  -1.641702   -2.181599    0.8890129  -0.63093925\n",
      "  -1.3986773  -1.8220811  -1.9220866   1.0033664 ]]\n",
      "Predictions shape: (1, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_sign_classifier_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
