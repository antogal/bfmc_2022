{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "# NOTE : Sign detecttion uses the folder training_images to load the backgrounds\n",
    "# it uses the folder sign_imgs to load the signs, the rate of examples is important\n",
    "num_channels = 3 # COLOR IMGS\n",
    "SIZE = (16, 16)\n",
    "model_name = 'models/sign_classifier.pt'\n",
    "onnx_sign_classifier_path = \"models/sign_classifier.onnx\"\n",
    "max_load = 5_000 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 examples loaded for 10 class_names\n",
      "example labels: [9, 7, 9, 9, 9, 5, 6, 1, 4, 8, 0, 9, 2, 9, 3]\n"
     ]
    }
   ],
   "source": [
    "# LOAD EXAMPLES\n",
    "# imgs of the examples must be in a specific folder (ex: sign_imgs), and must be named as \"class_<number>.png\"\n",
    "# ex: sign_imgs/stop_1.png, note: start from 1\n",
    "\n",
    "#load examples\n",
    "examples_folder = 'sign_imgs'     \n",
    "class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway', 'nosign']\n",
    "# class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway']\n",
    "\n",
    "file_names = [f for f in os.listdir(examples_folder) if f.endswith('.png')]\n",
    "example_labels = [class_names.index(name.split('_')[0]) for name in file_names if name.split('_')[0] in class_names]\n",
    "example_imgs = [cv.resize(cv.imread(os.path.join(examples_folder, name)), (128,128)) for name in file_names if name.split('_')[0] in class_names]\n",
    "tot_examples = len(example_imgs)\n",
    "tot_classes = len(class_names) \n",
    "print(f'{tot_examples} examples loaded for {tot_classes} class_names')\n",
    "print(f'example labels: {example_labels}')    \n",
    "\n",
    "#show images\n",
    "cv.namedWindow('example', cv.WINDOW_NORMAL)\n",
    "for i in range(tot_examples):\n",
    "    img = example_imgs[i].copy()\n",
    "    # add text label\n",
    "    cv.putText(img, class_names[example_labels[i]], (10,30), cv.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "    cv.imshow('example', img)\n",
    "    key = cv.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE 32x32\n",
    "\n",
    "# class SignClassifier(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=num_channels): \n",
    "#         super().__init__()\n",
    "#         p = 0.2\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=3, stride=1), #out = 12 - 28  \n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=10 - 14\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.Conv2d(8, 8, kernel_size=4, stride=2), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=5\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.Conv2d(8, 32, kernel_size=5, stride=1), #out = 1\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             # nn.Linear(in_features=4*4*4, out_features=128),\n",
    "#             # nn.ReLU(True),\n",
    "#             # nn.Dropout(p),\n",
    "#             # nn.Linear(in_features=128, out_features=out_dim),\n",
    "#             nn.Linear(in_features=32*1*1, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# sign_classifier = SignClassifier(out_dim=tot_classes,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE 16x16\n",
    "\n",
    "class SignClassifier(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=num_channels): \n",
    "        super().__init__()\n",
    "        p = 0.3\n",
    "        ### Convoluational layers\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 12\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1), #out=10\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(p),\n",
    "            nn.Conv2d(8, 16, kernel_size=5, stride=1), #out = 6\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1), #out=5\n",
    "            # nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p),\n",
    "            nn.Conv2d(16, 64, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            # nn.Linear(in_features=4*4*4, out_features=128),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.Dropout(p),\n",
    "            # nn.Linear(in_features=128, out_features=out_dim),\n",
    "            nn.Linear(in_features=64*1*1, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "sign_classifier = SignClassifier(out_dim=tot_classes,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 16, 16])\n",
      "out shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "sign_classifier.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = sign_classifier(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "\n",
    "def draw_box(img, x, y, w):\n",
    "    img = cv.rectangle(img, (int(x-w/2), int(y-w/2)), (int(x+w/2), int(y+w/2)), 255, 2)\n",
    "    return img\n",
    "\n",
    "def load_and_augment_img(img, example_index=0, example_imgs=example_imgs):\n",
    "    # img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "    if img is not None:\n",
    "        #convert to gray\n",
    "        # img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "        # #crop to the top right quadrant\n",
    "        img = img[0:img.shape[1]//2, img.shape[1]//2:]\n",
    "        canv_dim = randint(64//2, 90//2)  ## RANGE OF DIMENSION OF THE SIGN\n",
    "        start_x = randint(0, img.shape[1]-canv_dim)\n",
    "        start_y = randint(0, img.shape[0]-canv_dim)\n",
    "        img = img[start_y:start_y+canv_dim, start_x:start_x+canv_dim]\n",
    "        # img = cv.resize(img, (2*SIZE[0], 2*SIZE[1]))\n",
    "    else:\n",
    "        img = randint(0,255,(2*SIZE[0], 2*SIZE[1]), dtype=np.uint8)\n",
    "\n",
    "    ## EXAMPLE AUGMENTATION ##############################################################\n",
    "    #load example\n",
    "    example = example_imgs[example_index]\n",
    "    resize_ratio = max(img.shape)/max(example.shape)\n",
    "    example = cv.resize(example, (int(example.shape[1]*resize_ratio), int(example.shape[0]*resize_ratio)))\n",
    "\n",
    "    #get example mask\n",
    "    example_mask = np.where(example == np.array([0,0,0]), np.zeros_like(example), 255*np.ones_like(example))\n",
    "    #blur the example\n",
    "    # example = cv.blur(example, (randint(3,9),randint(3,9)))\n",
    "\n",
    "    # add noise to the example\n",
    "    std = 20\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.subtract(example, noisem)\n",
    "    noisep = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.add(example, noisep)\n",
    "\n",
    "    #set zero where example mask is zero\n",
    "    example = np.where(example_mask == np.array([0,0,0]), np.zeros_like(example), example)\n",
    "    # cv.imshow('test', example)\n",
    "\n",
    "    #perspective transform example\n",
    "    perspective_deformation = img.shape[0]//10\n",
    "    pts1 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = pts2 + np.float32(randint(0,perspective_deformation,size=pts2.shape))\n",
    "    # print(f'pts2 = \\n{pts2}')\n",
    "    new_size_x = int(np.max(pts2[:,0]) - np.min(pts2[:,0]))\n",
    "    new_size_y = int(np.max(pts2[:,1]) - np.min(pts2[:,1]))\n",
    "    M = cv.getPerspectiveTransform(pts1,pts2)\n",
    "    example = cv.warpPerspective(example,M,(new_size_x,new_size_y))\n",
    "\n",
    "    #resize example keeping proportions\n",
    "    img_example_ratio = min(img.shape[0]/example.shape[0], img.shape[1]/example.shape[1])\n",
    "    scale_factor = np.random.uniform(.6, .98) ##.15, .35############################ PARAM ##############################\n",
    "    scale_factor = scale_factor * img_example_ratio\n",
    "    example = cv.resize(example, (0,0), fx=scale_factor, fy=scale_factor)\n",
    "    #match img shape\n",
    "    example_canvas = np.zeros((img.shape[0], img.shape[1], num_channels), dtype=np.uint8)\n",
    "\n",
    "    #get a random position for the example\n",
    "    example_y = randint(0, img.shape[0] - example.shape[0])\n",
    "    example_x = randint(0, img.shape[1] - example.shape[1])\n",
    "    #paste example on canvas\n",
    "    example_canvas[example_y:example_y+example.shape[0], example_x:example_x+example.shape[1]] = example\n",
    "\n",
    "    old_example_canvas = example_canvas.copy()\n",
    "\n",
    "    #convert to hsv\n",
    "    example_canvas = cv.cvtColor(example_canvas, cv.COLOR_BGR2HSV)\n",
    "    #get the hsv channels\n",
    "    example_canvas_h = example_canvas[:,:,0]\n",
    "    example_canvas_s = example_canvas[:,:,1]\n",
    "    example_canvas_v = example_canvas[:,:,2]\n",
    "\n",
    "    # #reduce brightness\n",
    "    brightness_shift = randint(-80,5)\n",
    "    # example_canvas_v = np.clip(example_canvas_v + brightness_shift, 0, 255).astype(np.uint8)\n",
    "    if brightness_shift > 0:\n",
    "        example_canvas_v = cv.add(example_canvas_v, np.ones_like(example_canvas_v)*brightness_shift)\n",
    "    elif brightness_shift < 0:\n",
    "        example_canvas_v = cv.subtract(example_canvas_v, np.ones_like(example_canvas_v)*abs(brightness_shift))\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.25,.9)\n",
    "    example_canvas_v = np.clip(127*(1-const) + example_canvas_v*const, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #augment hue\n",
    "    hue_shift = randint(-7,7)\n",
    "    example_canvas_h = (example_canvas_h + hue_shift) % 180\n",
    "\n",
    "    #augment saturation\n",
    "    sat_shift = randint(-100,0)\n",
    "    example_canvas_s = np.clip(example_canvas_s + sat_shift, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #rebuild the channels\n",
    "    example_canvas[:,:,0] = example_canvas_h\n",
    "    example_canvas[:,:,1] = example_canvas_s\n",
    "    example_canvas[:,:,2] = example_canvas_v\n",
    "    #back to bgr\n",
    "    example_canvas = cv.cvtColor(example_canvas, cv.COLOR_HSV2BGR)\n",
    "\n",
    "    #paste canvas on img\n",
    "    old_example_canvas = cv.cvtColor(old_example_canvas, cv.COLOR_BGR2GRAY)\n",
    "    img_b = img[:,:,0]\n",
    "    img_g = img[:,:,1]\n",
    "    img_r = img[:,:,2]\n",
    "    example_b = example_canvas[:,:,0]\n",
    "    example_g = example_canvas[:,:,1]\n",
    "    example_r = example_canvas[:,:,2]\n",
    "    img_b = np.where(old_example_canvas > 0, example_b, img_b)\n",
    "    img_g = np.where(old_example_canvas > 0, example_g, img_g)\n",
    "    img_r = np.where(old_example_canvas > 0, example_r, img_r)\n",
    "    # img = np.where(old_example_canvas > 0, example_canvas, img) \n",
    "    img[:,:,0] = img_b\n",
    "    img[:,:,1] = img_g\n",
    "    img[:,:,2] = img_r\n",
    "\n",
    "    # # blur example\n",
    "    # b = randint(2,5)\n",
    "    # example_canvas = cv.blur(example_canvas, (b,b))\n",
    "\n",
    "    ##########################################################################################\n",
    "    \n",
    "    # convert whole img to hsv\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "    #get the hsv channels\n",
    "    img_h = img[:,:,0]\n",
    "    img_s = img[:,:,1]\n",
    "    img_v = img[:,:,2]\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.7,.99)\n",
    "    img_v = np.clip(127*(1-const) + img_v*const, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #reduce brightness\n",
    "    brightness_shift = randint(-30,0)\n",
    "    if brightness_shift > 0:\n",
    "        img_v = cv.add(img_v, np.ones_like(img_v)*brightness_shift)\n",
    "    elif brightness_shift < 0:\n",
    "        img_v = cv.subtract(img_v, np.ones_like(img_v)*abs(brightness_shift))\n",
    "\n",
    "    #augment hue\n",
    "    hue_shift = randint(-2,2)\n",
    "    img_h = (img_h + hue_shift) % 180\n",
    "\n",
    "    #augment saturation\n",
    "    sat_shift = randint(-20,0)\n",
    "    img_s = np.clip(img_s + sat_shift, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #rebuild the channels\n",
    "    img[:,:,0] = img_h\n",
    "    img[:,:,1] = img_s\n",
    "    img[:,:,2] = img_v\n",
    "    #back to bgr\n",
    "    img = cv.cvtColor(img, cv.COLOR_HSV2BGR)\n",
    "\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = img.shape[0]//5\n",
    "    offset = np.random.randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        # img[:offset, :] = np.random.randint(0,255)\n",
    "        img = img[offset:, :]\n",
    "    elif offset < 0:\n",
    "        # img[offset:, :] = np.random.randint(0,255)\n",
    "        img = img[:offset, :]\n",
    "\n",
    "    offset_y = np.random.randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset_y, axis=1)\n",
    "    if offset_y > 0:\n",
    "        # img[:, :offset_y] = np.random.randint(0,255)\n",
    "        img = img[:, offset_y:]\n",
    "    elif offset_y < 0:\n",
    "        # img[:, offset_y:] = np.random.randint(0,255)\n",
    "        img = img[:, :offset_y]\n",
    "\n",
    "    min_dim = min(img.shape[0], img.shape[1])\n",
    "    #crop to square\n",
    "    img = img[:min_dim, :min_dim]\n",
    "\n",
    "    #add noise\n",
    "    std = 50\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "\n",
    "    # # crop into the img at random position\n",
    "    # zoom = randint(0, SIZE[0]//4)\n",
    "    # img = img[zoom:img.shape[0]-zoom, zoom:img.shape[1]-zoom]\n",
    "\n",
    "    # #blur \n",
    "    # b = randint(2,5)\n",
    "    # img = cv.blur(img, (b,b))\n",
    "    \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    #convert to hsv\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = cv.resize(img, (160,120))\n",
    "    # img = None\n",
    "    img = load_and_augment_img(img, example_index=(i%tot_examples))\n",
    "\n",
    "    #to bgr\n",
    "    img = cv.cvtColor(img, cv.COLOR_HSV2BGR)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(200)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.class_names = []\n",
    "        self.channels = channels\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            # self.all_imgs = torch.zeros((2*max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8) #adding flipped img\n",
    "            self.all_imgs = torch.zeros((max_load*tot_examples, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN\n",
    "\n",
    "            for i in tqdm(range(max_load)):\n",
    "                img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "                img = cv.resize(img, (160,120))\n",
    "                for j in range(tot_examples):\n",
    "                    img_j = load_and_augment_img(img.copy(), example_index=j)\n",
    "                    if i < 100:\n",
    "                        cv.imshow('img', img_j)\n",
    "                        cv.waitKey(1)\n",
    "                        if i == 99:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img_j = img_j[:, :,np.newaxis] if self.channels == 1 else img_j\n",
    "                    #convert to tensor\n",
    "                    img_j = torch.from_numpy(img_j)\n",
    "                    self.all_imgs[i*tot_examples+j] = img_j\n",
    "                    self.class_names.append(example_labels[j])\n",
    "            \n",
    "            self.data = torch.from_numpy(np.array(self.data))\n",
    "            self.class_names = torch.from_numpy(np.array(self.class_names))\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'class_names: {self.class_names.shape}')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.class_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        class_label = self.class_names[idx]\n",
    "        return img, class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:59<00:00, 41.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all imgs: torch.Size([75000, 16, 16, 3])\n",
      "class_names: torch.Size([75000])\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4*8192//3, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8192//3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10922, 3, 16, 16])\n",
      "torch.Size([10922])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, class_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    class_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, class_label) in tqdm(dataloader):\n",
    "        #one hot encode the class label\n",
    "        class_label = torch.eye(tot_classes)[class_label]\n",
    "        # Move the input and target data to the selected device\n",
    "        input, class_label = input.to(device), class_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        class_out = output[:, 0:]\n",
    "\n",
    "        #classification loss\n",
    "        assert class_label.shape == class_out.shape, f'class_label.shape: {class_label.shape}, output.shape: {output.shape}'\n",
    "\n",
    "        class_loss = 1.0*class_loss_fn(class_out, class_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = class_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        class_losses.append(class_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    class_loss = np.mean(class_losses)\n",
    "    return  np.sqrt(class_loss)\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(sign_classifier, val_dataloader, regr_loss_fn, class_loss_fn, device=device):\n",
    "    sign_classifier.eval()\n",
    "    y_losses = []\n",
    "    x_losses = []\n",
    "    w_losses = []\n",
    "    class_losses = []\n",
    "    for (input, class_label) in tqdm(val_dataloader):\n",
    "        #one hot encode the class label\n",
    "        class_label = torch.eye(tot_classes)[class_label]\n",
    "        input, class_label = input.to(device), class_label.to(device)\n",
    "        output = sign_classifier(input)\n",
    "\n",
    "        class_loss = 1.0*class_loss_fn(output[:, 0:], class_label)\n",
    "    \n",
    "        class_losses.append(class_loss.detach().cpu().numpy())\n",
    "    return  np.mean(class_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50/50\n",
      "class_loss: 0.2250 --- Val: 0.0311\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.005 #0.005\n",
    "epochs = 50 #50+\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 1e-4 #9e-4\n",
    "L2_lambda = 1e-3 #1e-2\n",
    "optimizer = torch.optim.Adam(sign_classifier.parameters(), lr=lr, weight_decay=3e-5) #wd = 2e-3# 9e-5\n",
    "\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "class_loss_fn = nn.CrossEntropyLoss()\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        class_loss = train_epoch(sign_classifier, train_dataloader, regr_loss_fn, class_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_class_loss = val_epoch(sign_classifier, val_dataloader, regr_loss_fn, class_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs}\")\n",
    "    print(f\"class_loss: {class_loss:.4f} --- Val: {val_class_loss:.4f}\")\n",
    "    torch.save(sign_classifier.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "# val_dataloader2 = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "# sign_classifier.load_state_dict(torch.load(model_name))\n",
    "# sign_classifier.eval()\n",
    "# for (input, class_label) in tqdm(val_dataloader2):\n",
    "#     #convert img to numpy array\n",
    "#     img = input[0].detach().cpu().numpy()[0]\n",
    "#     #convert to sigle byte\n",
    "#     img = img.astype(np.uint8)\n",
    "#     input, box_label, class_label =input.to(device), box_label.to(device), class_label.to(device)\n",
    "#     output = sign_classifier(input)\n",
    "#     x = output[:, 0]\n",
    "#     y = output[:, 1]\n",
    "#     w = output[:, 2]\n",
    "#     class_out = output[:, 3:]\n",
    "#     # print(class_out)\n",
    "#     class_out = torch.argmax(class_out, dim=1)\n",
    "#     class_out = class_out.detach().cpu().numpy()\n",
    "#     # print(f\"Predicted class: {class_out[0]}, Actual class: {class_label[0]}\")\n",
    "#     class_out = class_out[0]\n",
    "#     class_out = class_names[class_out]\n",
    "#     true_class = class_names[class_label[0]]\n",
    "#     img = draw_box(img, x, y, w)\n",
    "#     #text for labels\n",
    "#     img = cv.putText(img, f\"True: {true_class}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "#     img = cv.putText(img, f\"Pred: {class_out}\", (10, 60), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "#     cv.imshow('img', img)\n",
    "#     key = cv.waitKey(0)\n",
    "#     if key == 27:\n",
    "#         break\n",
    "    \n",
    "# cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 3, 5, 5)\n",
      "(16, 8, 5, 5)\n",
      "(64, 16, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(sign_classifier.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 4, 4, 'conv0', size=(10,10))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignClassifier(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Dropout(p=0.3, inplace=False)\n",
       "    (5): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout(p=0.3, inplace=False)\n",
       "    (10): Conv2d(16, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "sign_classifier.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "sign_classifier.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "sign_classifier.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(sign_classifier, dummy_input, onnx_sign_classifier_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "sign_classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 6103.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[ -5.6092186  -2.7584789  -4.2974863 -13.402407   -2.6736312   5.9858317\n",
      "   -0.8276823  -4.1547713 -14.161067    3.7992122]]\n",
      "Predictions shape: (1, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_sign_classifier_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
