{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "# NOTE : Sign detecttion uses the folder training_images to load the backgrounds\n",
    "# it uses the folder sign_imgs to load the signs, the rate of examples is important\n",
    "num_channels = 3 # COLOR IMGS\n",
    "SIZE = (16, 16)\n",
    "model_name = 'models/sign_classifier.pt'\n",
    "onnx_sign_classifier_path = \"models/sign_classifier.onnx\"\n",
    "max_load = 1_000 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 examples loaded for 11 class_names\n",
      "example labels: [10, 4, 8, 10, 10, 7, 6, 10, 10, 10, 5, 9, 5, 10, 9, 2, 6, 10, 1, 7, 1, 0, 8, 3, 10, 4, 2, 9, 8, 7, 0, 4, 6, 3, 0, 10, 5, 1, 10, 2, 10, 3]\n"
     ]
    }
   ],
   "source": [
    "# LOAD EXAMPLES\n",
    "# imgs of the examples must be in a specific folder (ex: sign_imgs), and must be named as \"class_<number>.png\"\n",
    "# ex: sign_imgs/stop_1.png, note: start from 1\n",
    "\n",
    "#load examples\n",
    "examples_folder = 'sign_imgs'     \n",
    "# class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway', 'trafficlight', 'nosign'] \n",
    "class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway', 'trafficlight', 'nosign']  # with traffic_light\n",
    "\n",
    "# class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway']\n",
    "\n",
    "file_names = [f for f in os.listdir(examples_folder) if f.endswith('.png')]\n",
    "example_labels = [class_names.index(name.split('_')[0]) for name in file_names if name.split('_')[0] in class_names]\n",
    "example_imgs = [cv.resize(cv.blur(cv.imread(os.path.join(examples_folder, name)), (5,5)), (128,128)) for name in file_names if name.split('_')[0] in class_names]\n",
    "tot_examples = len(example_imgs)\n",
    "tot_classes = len(class_names) \n",
    "print(f'{tot_examples} examples loaded for {tot_classes} class_names')\n",
    "print(f'example labels: {example_labels}')    \n",
    "\n",
    "#show images\n",
    "cv.namedWindow('example', cv.WINDOW_NORMAL)\n",
    "for i in range(tot_examples):\n",
    "    img = example_imgs[i].copy()\n",
    "    # add text label\n",
    "    cv.putText(img, class_names[example_labels[i]], (10,30), cv.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "    cv.imshow('example', img)\n",
    "    key = cv.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE 32x32\n",
    "\n",
    "# class SignClassifier(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=num_channels): \n",
    "#         super().__init__()\n",
    "#         p = 0.2\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 16, kernel_size=3, stride=1), #out = 12 - 28  \n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=10 - 14\n",
    "#             nn.BatchNorm2d(16),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.Conv2d(16, 8, kernel_size=4, stride=2), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=5\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.Conv2d(8, 64, kernel_size=5, stride=1), #out = 1\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             # nn.Linear(in_features=4*4*4, out_features=128),\n",
    "#             # nn.ReLU(True),\n",
    "#             # nn.Dropout(p),\n",
    "#             # nn.Linear(in_features=128, out_features=out_dim),\n",
    "#             nn.Linear(in_features=64*1*1, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# sign_classifier = SignClassifier(out_dim=tot_classes,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE 16x16\n",
    "\n",
    "class SignClassifier(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=num_channels): \n",
    "        super().__init__()\n",
    "        p = 0.3\n",
    "        ### Convoluational layers\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 16, kernel_size=5, stride=1), #out = 12\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1), #out=10\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(p),\n",
    "            nn.Conv2d(16, 16, kernel_size=5, stride=1), #out = 6\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1), #out=5\n",
    "            # nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p),\n",
    "            nn.Conv2d(16, 64, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            # nn.Linear(in_features=4*4*4, out_features=128),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.Dropout(p),\n",
    "            # nn.Linear(in_features=128, out_features=out_dim),\n",
    "            nn.Linear(in_features=64*1*1, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "sign_classifier = SignClassifier(out_dim=tot_classes,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 16, 16])\n",
      "out shape: torch.Size([1, 11])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "sign_classifier.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = sign_classifier(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load coco dataset and create background img in the background folder\n",
    "\n",
    "# import json\n",
    "\n",
    "# MAX_BACKGROUNDS = 500_000\n",
    "\n",
    "# #get all file names inside fodler sign_imgs/coco_val\n",
    "# # coco_val_img_names = [os.path.join('sign_imgs', 'coco_val', f) for f in os.listdir(os.path.join('sign_imgs', 'coco_val'))]\n",
    "# coco_val_img_names = [f for f in os.listdir(os.path.join('sign_imgs', 'coco_val'))]\n",
    "\n",
    "# print(f'{len(coco_val_img_names)} images in coco val')\n",
    "\n",
    "# #get all instances\n",
    "# #load json file\n",
    "# with open('sign_imgs/coco_val/instances_val2017.json') as f:\n",
    "#     data = json.load(f)\n",
    "#     #print name of the file\n",
    "#     print(f'images = {len(data[\"images\"])}')\n",
    "#     print(f'annotations = {len(data[\"annotations\"])}')\n",
    "\n",
    "\n",
    "#     categories = {cat['id']:cat['name'] for cat in data['categories']}\n",
    "#     categories_by_name = {cat['name']:cat['id'] for cat in data['categories']}\n",
    "\n",
    "#     filtered_categories = ['traffic light', 'bicycle', 'car', 'motorcycle', 'bus', 'truck',\n",
    "#                             'fire hydrant', 'stop sign',  'parking meter']\n",
    "#     filtered_categories_idxs = [categories_by_name[c] for c in filtered_categories]\n",
    "\n",
    "#     img_names_by_id = {img['id']:img['file_name'] for img in data['images']}\n",
    "\n",
    "#     categ_by_id = {img['id']:[] for img in data['images']}\n",
    "\n",
    "#     for ann in data['annotations']:\n",
    "#         categ_id = ann['category_id']\n",
    "#         img_id = ann['image_id']\n",
    "#         if img_id in categ_by_id:\n",
    "#             categ_by_id[img_id].append(categories[categ_id])\n",
    "#         if categ_id in filtered_categories_idxs:\n",
    "#             img_names_by_id.pop(img_id, None)\n",
    "#             categ_by_id.pop(img_id, None)\n",
    "    \n",
    "#     for id, c in categ_by_id.items():\n",
    "#         if len(c) == 0:\n",
    "#             img_names_by_id.pop(id, None)\n",
    "\n",
    "#     print(f'final images = {len(img_names_by_id)}')\n",
    "\n",
    "#     BACKGROUND_SIZE = (320,240)\n",
    "#     idx = 0\n",
    "#     for k, img_name in img_names_by_id.items():\n",
    "#         img_name = img_names_by_id[k]\n",
    "#         categories_in_img = categ_by_id[k]\n",
    "#         img = cv.imread(os.path.join('sign_imgs', 'coco_val', img_name))\n",
    "#         img = cv.resize(img, (2*BACKGROUND_SIZE[0], 2*BACKGROUND_SIZE[1]))\n",
    "#         #divide the image into 4 parts\n",
    "#         img_parts = [img[:BACKGROUND_SIZE[1], :BACKGROUND_SIZE[0]],\n",
    "#                     img[:BACKGROUND_SIZE[1], BACKGROUND_SIZE[0]:],\n",
    "#                     img[BACKGROUND_SIZE[1]:, :BACKGROUND_SIZE[0]],\n",
    "#                     img[BACKGROUND_SIZE[1]:, BACKGROUND_SIZE[0]:]]\n",
    "\n",
    "#         for i in range(4):\n",
    "#             img_part = img_parts[i]\n",
    "#             #further divide the image into 4 parts\n",
    "#             img_part_parts = [img_part[:BACKGROUND_SIZE[1]//2, :BACKGROUND_SIZE[0]//2],\n",
    "#                             img_part[:BACKGROUND_SIZE[1]//2, BACKGROUND_SIZE[0]//2:],\n",
    "#                             img_part[BACKGROUND_SIZE[1]//2:, :BACKGROUND_SIZE[0]//2],\n",
    "#                             img_part[BACKGROUND_SIZE[1]//2:, BACKGROUND_SIZE[0]//2:]]\n",
    "#             for j in range(4):\n",
    "#                 img_part_part = img_part_parts[j]\n",
    "#                 cv.imshow(f'img_{i}{j}', img_part_parts[j])\n",
    "#                 idx += 1\n",
    "#                 cv.imwrite(os.path.join('sign_imgs', 'backgrounds', f'background_{idx}.png'), img_part_parts[j])\n",
    "\n",
    "#         print(f'{categories_in_img}')\n",
    "#         key = cv.waitKey(1)\n",
    "#         if key == 27 or idx > MAX_BACKGROUNDS:\n",
    "#             break\n",
    "\n",
    "# cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irong/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/ipykernel_launcher.py:155: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "\n",
    "def draw_box(img, x, y, w):\n",
    "    img = cv.rectangle(img, (int(x-w/2), int(y-w/2)), (int(x+w/2), int(y+w/2)), 255, 2)\n",
    "    return img\n",
    "\n",
    "def load_and_augment_img(img, example_index=0, example_imgs=example_imgs):\n",
    "    # img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "    if img is not None:\n",
    "        #convert to gray\n",
    "        # img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "        # #crop to the top right quadrant\n",
    "        img = img[0:img.shape[1]//2, img.shape[1]//2:]\n",
    "        canv_dim = randint(64//2, 240//2)  ## RANGE OF DIMENSION OF THE SIGN\n",
    "        start_x = randint(0, img.shape[1]-canv_dim) if img.shape[1] > canv_dim else 0\n",
    "        start_y = randint(0, img.shape[0]-canv_dim) if img.shape[0] > canv_dim else 0\n",
    "        img = img[start_y:start_y+canv_dim, start_x:start_x+canv_dim]\n",
    "        # img = cv.resize(img, (2*SIZE[0], 2*SIZE[1]))\n",
    "    else:\n",
    "        img = randint(0,255,(2*SIZE[0], 2*SIZE[1]), dtype=np.uint8)\n",
    "\n",
    "\n",
    "    background_avg_brightness = np.mean(img)\n",
    "\n",
    "    ## EXAMPLE AUGMENTATION ##############################################################\n",
    "    #load example\n",
    "    example = example_imgs[example_index]\n",
    "    resize_ratio = max(img.shape)/max(example.shape)\n",
    "    example = cv.resize(example, (int(example.shape[1]*resize_ratio), int(example.shape[0]*resize_ratio)))\n",
    "    #Make a black border aroud the example\n",
    "    example[0:2,:,:] = np.array([0,0,0])\n",
    "    example[-3:-1,:,:] = np.array([0,0,0])\n",
    "    example[:,0:2,:] = np.array([0,0,0])\n",
    "    example[:,-3:-1,:] = np.array([0,0,0])\n",
    "\n",
    "    #get example mask\n",
    "    example_mask = np.where(example == np.array([0,0,0]), np.zeros_like(example), 255*np.ones_like(example))\n",
    "    example_mask = example_mask.astype(np.uint8)\n",
    "    example_mask = cv.cvtColor(example_mask, cv.COLOR_BGR2GRAY)\n",
    "    #convert to bitmap\n",
    "    example_mask = np.where(example_mask == 0, 0, 255)\n",
    "    example_mask = example_mask.astype(np.uint8)\n",
    "    #back to BGR\n",
    "    example_mask = cv.cvtColor(example_mask, cv.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # add noise to the example\n",
    "    std = 20\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.subtract(example, noisem)\n",
    "    noisep = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.add(example, noisep)\n",
    "\n",
    "    #set zero where example mask is zero\n",
    "    example = np.where(example_mask == np.array([0,0,0]), np.zeros_like(example), example)\n",
    "\n",
    "    #perspective transform example\n",
    "    perspective_deformation = img.shape[0]//8 ################# DEFORMATION\n",
    "    pts1 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = pts2 + np.float32(randint(0,perspective_deformation,size=pts2.shape))\n",
    "    # print(f'pts2 = \\n{pts2}')\n",
    "    new_size_x = int(np.max(pts2[:,0]) - np.min(pts2[:,0]))\n",
    "    new_size_y = int(np.max(pts2[:,1]) - np.min(pts2[:,1]))\n",
    "    M = cv.getPerspectiveTransform(pts1,pts2)\n",
    "    example = cv.warpPerspective(example,M,(new_size_x,new_size_y))\n",
    "\n",
    "    #resize example keeping proportions\n",
    "    img_example_ratio = min(img.shape[0]/example.shape[0], img.shape[1]/example.shape[1])\n",
    "    scale_factor = np.random.uniform(.5, .98) ##.15, .35############################ PARAM ##############################\n",
    "    scale_factor = scale_factor * img_example_ratio\n",
    "    example = cv.resize(example, (0,0), fx=scale_factor, fy=scale_factor)\n",
    "    #match img shape\n",
    "    example_canvas = np.zeros((img.shape[0], img.shape[1], num_channels), dtype=np.uint8)\n",
    "\n",
    "    #get a random position for the example\n",
    "    example_y = randint(0, img.shape[0] - example.shape[0])\n",
    "    example_x = randint(0, img.shape[1] - example.shape[1])\n",
    "    #paste example on canvas\n",
    "    example_canvas[example_y:example_y+example.shape[0], example_x:example_x+example.shape[1]] = example\n",
    "\n",
    "    #canvas mask\n",
    "    example_canvas_mask = example_canvas.copy()\n",
    "    example_canvas_mask = cv.cvtColor(example_canvas_mask, cv.COLOR_BGR2GRAY)\n",
    "    example_canvas_mask = np.where(example_canvas_mask == 0, 0, 255)\n",
    "    example_canvas_mask = example_canvas_mask.astype(np.uint8)\n",
    "    edge_of_canvas = cv.Canny(example_canvas_mask, 100, 200)\n",
    "    ker_rand = randint(2,5)\n",
    "    edge_of_canvas = cv.dilate(edge_of_canvas, np.ones((ker_rand,ker_rand)))\n",
    "    #erode mask\n",
    "    ker_rand = randint(2,5) if SIZE == (32,32) else randint(1,3)\n",
    "    kernel = np.ones((ker_rand,ker_rand),np.uint8)\n",
    "    iter_rand = randint(1,3) if SIZE == (32,32) else randint(1,2)\n",
    "    example_canvas_mask = cv.erode(example_canvas_mask,kernel,iterations=iter_rand)\n",
    "\n",
    "    #convert to hsv\n",
    "    example_canvas = cv.cvtColor(example_canvas, cv.COLOR_BGR2HSV)\n",
    "    #get the hsv channels\n",
    "    example_canvas_h = example_canvas[:,:,0]\n",
    "    example_canvas_s = example_canvas[:,:,1]\n",
    "    example_canvas_v = example_canvas[:,:,2]\n",
    "\n",
    "    # #reduce brightness\n",
    "    example_avg_brightness = np.mean(example_canvas_v)\n",
    "    if example_avg_brightness > 0.0:\n",
    "        example_avg_brightness = np.mean(example_canvas_v, where=example_canvas_v>0)\n",
    "    diff = -example_avg_brightness +background_avg_brightness\n",
    "    brightness_shift = int(round(diff) + randint(-80,80))\n",
    "    brightness_shift = np.clip(brightness_shift, -255, 255)\n",
    "    # example_canvas_v = np.clip(example_canvas_v + brightness_shift, 0, 255).astype(np.uint8)\n",
    "    if brightness_shift > 0:\n",
    "        example_canvas_v = cv.add(example_canvas_v, np.ones_like(example_canvas_v)*brightness_shift)\n",
    "    elif brightness_shift < 0:\n",
    "        example_canvas_v = cv.subtract(example_canvas_v, np.ones_like(example_canvas_v)*abs(brightness_shift))\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.25,.9)\n",
    "    example_canvas_v = np.clip(127*(1-const) + example_canvas_v*const, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #augment hue\n",
    "    hue_shift = randint(-7,7)\n",
    "    example_canvas_h = (example_canvas_h + hue_shift) % 180\n",
    "\n",
    "    #augment saturation\n",
    "    sat_shift = randint(-120,0)\n",
    "    example_canvas_s = np.clip(example_canvas_s + sat_shift, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #rebuild the channels\n",
    "    example_canvas[:,:,0] = example_canvas_h\n",
    "    example_canvas[:,:,1] = example_canvas_s\n",
    "    example_canvas[:,:,2] = example_canvas_v\n",
    "    #back to bgr\n",
    "    example_canvas = cv.cvtColor(example_canvas, cv.COLOR_HSV2BGR)\n",
    "\n",
    "    #paste canvas on img\n",
    "    img_b = img[:,:,0]\n",
    "    img_g = img[:,:,1]\n",
    "    img_r = img[:,:,2]\n",
    "    example_b = example_canvas[:,:,0]\n",
    "    example_g = example_canvas[:,:,1]\n",
    "    example_r = example_canvas[:,:,2]\n",
    "    img_b = np.where(example_canvas_mask > 0, example_b, img_b)\n",
    "    img_g = np.where(example_canvas_mask > 0, example_g, img_g)\n",
    "    img_r = np.where(example_canvas_mask > 0, example_r, img_r)\n",
    "    # img = np.where(example_canvas_mask > 0, example_canvas, img) \n",
    "    img[:,:,0] = img_b\n",
    "    img[:,:,1] = img_g\n",
    "    img[:,:,2] = img_r\n",
    "\n",
    "    ker_rand = randint(2,5)\n",
    "    blurred_img = cv.blur(img, (ker_rand,ker_rand))\n",
    "    img = np.where(edge_of_canvas ==np.array([0,0,0]), blurred_img, img)\n",
    "\n",
    "    ##########################################################################################\n",
    "    \n",
    "    # convert whole img to hsv\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "    #get the hsv channels\n",
    "    img_h = img[:,:,0]\n",
    "    img_s = img[:,:,1]\n",
    "    img_v = img[:,:,2]\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.5,.99)\n",
    "    img_v = np.clip(127*(1-const) + img_v*const, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #reduce brightness\n",
    "    brightness_shift = randint(-40,0)\n",
    "    if brightness_shift > 0:\n",
    "        img_v = cv.add(img_v, np.ones_like(img_v)*brightness_shift)\n",
    "    elif brightness_shift < 0:\n",
    "        img_v = cv.subtract(img_v, np.ones_like(img_v)*abs(brightness_shift))\n",
    "\n",
    "    #augment hue\n",
    "    hue_shift = randint(-2,2)\n",
    "    img_h = (img_h + hue_shift) % 180\n",
    "\n",
    "    #augment saturation\n",
    "    sat_shift = randint(-20,0)\n",
    "    img_s = np.clip(img_s + sat_shift, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #rebuild the channels\n",
    "    img[:,:,0] = img_h\n",
    "    img[:,:,1] = img_s\n",
    "    img[:,:,2] = img_v\n",
    "    #back to bgr\n",
    "    img = cv.cvtColor(img, cv.COLOR_HSV2BGR)\n",
    "\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = img.shape[0]//4\n",
    "    offset = np.random.randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        # img[:offset, :] = np.random.randint(0,255)\n",
    "        img = img[offset:, :]\n",
    "    elif offset < 0:\n",
    "        # img[offset:, :] = np.random.randint(0,255)\n",
    "        img = img[:offset, :]\n",
    "\n",
    "    offset_y = np.random.randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset_y, axis=1)\n",
    "    if offset_y > 0:\n",
    "        # img[:, :offset_y] = np.random.randint(0,255)\n",
    "        img = img[:, offset_y:]\n",
    "    elif offset_y < 0:\n",
    "        # img[:, offset_y:] = np.random.randint(0,255)\n",
    "        img = img[:, :offset_y]\n",
    "\n",
    "    min_dim = min(img.shape[0], img.shape[1])\n",
    "    #crop to square\n",
    "    img = img[:min_dim, :min_dim]\n",
    "\n",
    "    #add noise\n",
    "    std = 50\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "\n",
    "    # # crop into the img at random position\n",
    "    # zoom = randint(0, SIZE[0]//4)\n",
    "    # img = img[zoom:img.shape[0]-zoom, zoom:img.shape[1]-zoom]\n",
    "\n",
    "    #blur \n",
    "    b = randint(1,4) \n",
    "    img = cv.blur(img, (b,b))\n",
    "    \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    #convert to hsv\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('sign_imgs', 'backgrounds',  f'background_{i+1}.png'))\n",
    "    # img = cv.resize(img, (160,120))\n",
    "    # img = None\n",
    "    img = load_and_augment_img(img, example_index=(i%tot_examples))\n",
    "\n",
    "    #to bgr\n",
    "    img = cv.cvtColor(img, cv.COLOR_HSV2BGR)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(200)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = os.path.join('sign_imgs', 'backgrounds')\n",
    "        self.data = []\n",
    "        self.class_names = []\n",
    "        self.channels = channels\n",
    "        self.all_imgs = torch.zeros((max_load*tot_examples, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "        cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "        # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN\n",
    "\n",
    "        for i in tqdm(range(max_load)):\n",
    "            img = cv.imread(os.path.join(self.folder, f'background_{i+1}.png'))\n",
    "            # img = cv.resize(img, (160,120))\n",
    "            for j in range(tot_examples):\n",
    "                img_j = load_and_augment_img(img.copy(), example_index=j)\n",
    "                if i < 100:\n",
    "                    cv.imshow('img', img_j)\n",
    "                    cv.waitKey(1)\n",
    "                    if i == 99:\n",
    "                        cv.destroyAllWindows()\n",
    "                #add a dimension to the image\n",
    "                img_j = img_j[:, :,np.newaxis] if self.channels == 1 else img_j\n",
    "                #convert to tensor\n",
    "                img_j = torch.from_numpy(img_j)\n",
    "                self.all_imgs[i*tot_examples+j] = img_j\n",
    "                self.class_names.append(example_labels[j])\n",
    "        \n",
    "        self.data = torch.from_numpy(np.array(self.data))\n",
    "        self.class_names = torch.from_numpy(np.array(self.class_names))\n",
    "        print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "        print(f'class_names: {self.class_names.shape}')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.class_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        class_label = self.class_names[idx]\n",
    "        return img, class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/home/irong/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/ipykernel_launcher.py:155: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "100%|██████████| 1000/1000 [02:45<00:00,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all imgs: torch.Size([42000, 16, 16, 3])\n",
      "class_names: torch.Size([42000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset(max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=5*8192//3, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8192//3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13653, 3, 16, 16])\n",
      "torch.Size([13653])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, class_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    class_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, class_label) in tqdm(dataloader):\n",
    "        #one hot encode the class label\n",
    "        class_label = torch.eye(tot_classes)[class_label]\n",
    "        # Move the input and target data to the selected device\n",
    "        input, class_label = input.to(device), class_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        class_out = output[:, 0:]\n",
    "\n",
    "        #classification loss\n",
    "        assert class_label.shape == class_out.shape, f'class_label.shape: {class_label.shape}, output.shape: {output.shape}'\n",
    "\n",
    "        class_loss = 1.0*class_loss_fn(class_out, class_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = class_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        class_losses.append(class_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    class_loss = np.mean(class_losses)\n",
    "    return  np.sqrt(class_loss)\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(sign_classifier, val_dataloader, regr_loss_fn, class_loss_fn, device=device):\n",
    "    sign_classifier.eval()\n",
    "    y_losses = []\n",
    "    x_losses = []\n",
    "    w_losses = []\n",
    "    class_losses = []\n",
    "    for (input, class_label) in tqdm(val_dataloader):\n",
    "        #one hot encode the class label\n",
    "        class_label = torch.eye(tot_classes)[class_label]\n",
    "        input, class_label = input.to(device), class_label.to(device)\n",
    "        output = sign_classifier(input)\n",
    "\n",
    "        class_loss = 1.0*class_loss_fn(output[:, 0:], class_label)\n",
    "    \n",
    "        class_losses.append(class_loss.detach().cpu().numpy())\n",
    "    return  np.mean(class_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  200/200\n",
      "class_loss: 0.4856 --- Val: 0.2572\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.002 #0.005\n",
    "epochs = 200 #50+\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 1e-4 #9e-4\n",
    "L2_lambda = 1e-3 #1e-2\n",
    "optimizer = torch.optim.Adam(sign_classifier.parameters(), lr=lr, weight_decay=3e-5) #wd = 2e-3# 9e-5\n",
    "\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "class_loss_fn = nn.CrossEntropyLoss()\n",
    "best_val = 100\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        class_loss = train_epoch(sign_classifier, train_dataloader, regr_loss_fn, class_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_class_loss = val_epoch(sign_classifier, val_dataloader, regr_loss_fn, class_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs}\")\n",
    "    print(f\"class_loss: {class_loss:.4f} --- Val: {val_class_loss:.4f}\")\n",
    "    if val_class_loss < best_val:\n",
    "        torch.save(sign_classifier.state_dict(), model_name)\n",
    "        print(f'Model saved as {model_name}')\n",
    "        best_val = val_class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "# val_dataloader2 = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "# sign_classifier.load_state_dict(torch.load(model_name))\n",
    "# sign_classifier.eval()\n",
    "# for (input, class_label) in tqdm(val_dataloader2):\n",
    "#     #convert img to numpy array\n",
    "#     img = input[0].detach().cpu().numpy()[0]\n",
    "#     #convert to sigle byte\n",
    "#     img = img.astype(np.uint8)\n",
    "#     input, box_label, class_label =input.to(device), box_label.to(device), class_label.to(device)\n",
    "#     output = sign_classifier(input)\n",
    "#     x = output[:, 0]\n",
    "#     y = output[:, 1]\n",
    "#     w = output[:, 2]\n",
    "#     class_out = output[:, 3:]\n",
    "#     # print(class_out)\n",
    "#     class_out = torch.argmax(class_out, dim=1)\n",
    "#     class_out = class_out.detach().cpu().numpy()\n",
    "#     # print(f\"Predicted class: {class_out[0]}, Actual class: {class_label[0]}\")\n",
    "#     class_out = class_out[0]\n",
    "#     class_out = class_names[class_out]\n",
    "#     true_class = class_names[class_label[0]]\n",
    "#     img = draw_box(img, x, y, w)\n",
    "#     #text for labels\n",
    "#     img = cv.putText(img, f\"True: {true_class}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "#     img = cv.putText(img, f\"Pred: {class_out}\", (10, 60), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "#     cv.imshow('img', img)\n",
    "#     key = cv.waitKey(0)\n",
    "#     if key == 27:\n",
    "#         break\n",
    "    \n",
    "# cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 3, 5, 5)\n",
      "(16, 16, 5, 5)\n",
      "(64, 16, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAThCAYAAAA1YTsWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAA2PUlEQVR4nO3af4zfB33n+ddnPJ7xeGyP7diJ4/xyUyBNAiSUFCjQCkR3W2172q6oRMtVvWortt2VrrtURerpTqj3R9tTtToWddWWUo4WKAG2dyUpq0ZFbEsbGgIuUEIIpcSJ7YTE+NfYjn+M58fn/oCuWLT2ZETenuN9j4eERDKfvL6feD7fzzzn880wjmMAADqaWO8TAACoInQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdYN0Mw3DtMAz3DsPw1WEYxmEY9q33OQG9CB1gPa0kuS/J69f7RICehA7w3xiG4YZhGP6fYRiODsNwfBiG/zgMw8QwDP/bMAwHh2H42jAM7xmGYe4bx+/7xtOY/2kYhkPDMBwbhuF//cbX9g7DcH4Yhp3ftP+SbxyzcRzHI+M4/naST6/Tvy7QnNAB/qthGDYk+UiSg0n2JbkuyQeS/Ow3/vfaJDcn2ZLkP37LP/7qJLckeV2Stw7DcOs4jl9N8kD+2yc2b0zyx+M4Llb9ewD8I6EDfLOXJdmb5C3jOJ4dx/HCOI73J/kfk/yf4zgeGMfxmST/S5KfHIZh8pv+2f99HMfz4zj+XZK/S3LHN/7++5P8VJIMwzAk+clv/D2AckIH+GY3JDk4juPSt/z9vfn6U55/dDDJZJJrvunvPf1N//9cvv7UJ0n+7yTfPwzDtUl+MF//73L++rk8aYBLmVz9EOD/Rw4nuXEYhslviZ2vJrnpm/76xiRLSY4kuf5yg+M4nhyG4c+TvCHJrUk+MI7j+NyeNsB/nyc6wDf7VJKnkvwfwzDMDsOwaRiGVyW5O8mbh2H4rmEYtiT59SQf/O88+bmU9yf5mSQ/kW/52GoYhk1Jpr/xl9Pf+GuA54TQAf6rcRyXk/wPSZ6X5FCSJ/L1JzH/V5L3JvmrJI8luZDkf17D9L1Jnp/k6W/8Nzzf7HySZ77x/7/0jb8GeE4MniADAF15ogMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbk2s5eHp6ety8eXPJiYzjWLKbJNPT02XbSXLu3Lmy7WEYSnYvXLiQixcv1oxfxtzc3Lhnz56S7cpraNOmTWXbSTI1NVW2ffHixbLthx566Ng4jrvLXuASdu3aNd54440l25Xv54WFhbLtJFlcXCzbXllZKdmdn5/PuXPnrvi9aGZmZty2bVvJdtVuUvt+TpKzZ8+WbR8/frxsO8kl70VrCp3NmzfnNa95zXNyRt+q8ofUvn37yraT5O/+7u/Kticn1/QtetYefPDBkt3V7NmzJ7/7u79bsl15A7j99tvLtpPk+uuvL9s+ePBg2fa+ffvqxi/jxhtvzF/91V+VbH/uc58r2U2SAwcOlG0nyZEjR8q2n3nmmZLdd77znSW7q9m2bVve+MY3lmy/7nWvK9lNkieffLJsO0keeOCBsu0//MM/LNtOcsl7kY+uAIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhrci0HLy8v55lnnik5kRe+8IUlu0myadOmsu0ked7znle2PTFR06Kf+9znSnZXc+TIkbztbW8r2d63b1/JbpI8+uijZdtJ8t3f/d1l2+M4lm2vl6WlpZw4caJke3Z2tmQ3SXbu3Fm2nSRPPPFE2XbVvWi9bN26Na997WtLtu+4446S3aT++1B5DVVe/5e7H/S6cgEAvonQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKCtybUcvLy8nBMnTpScyAMPPFCymyRHjhwp206SLVu2lG1PTNS06NmzZ0t2V7O4uFj2/Th37lzJbpIMw1C2nSQzMzNl2wsLC2Xb62Xjxo3Zs2dPyfbc3FzJ7pXw2c9+tmz7wIEDJbsXL14s2V3NzMxMbrvttpLtjRs3luwmyfXXX1+2nSRXXXVV2fatt95atv2JT3zikl/zRAcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2ppcy8ErKytZWFgoOZGpqamS3SQ5d+5c2XaSLC4ulm1/7/d+b8nuwYMHS3ZXs7i4mCeeeKJk+4477ijZTZKJidrfCc6ePVu6380wDGX3jOXl5ZLdJHn88cfLtpNkHMey7c9//vMlu9X350uZnJzMrl27Sra3b99espskO3bsKNtOkk2bNpVtV16fn/jEJy75NU90AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbU2u5eALFy7k4YcfLjmR2267rWQ3Sfbt21e2nSR33HFH2fbv/d7vlezeddddJburWVxczFe/+tWS7ampqZLd6u0kOXXqVNn2I488Ura9Xk6dOpU/+7M/K9nevHlzyW6SfOYznynbTpIvf/nLZduf+9znyrbXw9LSUo4dO1ayPTm5ph+t/5+yffv278jty/FEBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0NYwjuOzP3gYjiY5WHc6XEE3jeO4+0q/qGuoHdcR3y7XEM+FS15HawodAIDvJD66AgDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKCtybUcPDc3N+7Zs6fkRGZmZkp2k+TJJ58s26520003lew+/vjjOXbs2FAyfhlbt24dd+/eXbJ98eLFkt0kWVxcLNtOaq//Xbt2lW3/7d/+7bFxHGu+oZcxOzs77ty5s2T7/PnzJbtJsrCwULadJFu2bCnbXllZKdk9ffp0zp8/f8XvRTMzM+O2bdtKtqt2k2Rubq5sO6m91x08eLBs+9SpU5e8F60pdPbs2ZPf+73fe27O6lvceuutJbtJ8ta3vrVsu9rv/u7vluzeddddJbur2b17d37913+9ZLvyTXTkyJGy7SS5/fbby7Z/7ud+rmx7GIa6P/TL2LlzZ9785jeXbH/hC18o2U2SAwcOlG0nyStf+cqy7XPnzpXs3n333SW7q9m2bVve8IY3lGz/03/6T0t2k+THfuzHyraT5Kmnnirb/lf/6l+VbX/kIx+55L3IR1cAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtDW5loOffPLJ/Mqv/ErJifz8z/98yW6SnDx5smw7SVZWVsq2Dxw4ULK7sLBQsruas2fP5sEHHyzZfvLJJ0t2k2R+fr5sO0nOnTtXtv2CF7ygbHs9DcNQsnvs2LGS3ST57Gc/W7adJI899ljZ9sREze/F1e+tS9myZUt+4Ad+oGT7yJEjJbtJcv/995dtJ8nMzEzZ9o4dO8q2L8cTHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFuTazl4YWEhBw4cKDmR+++/v2Q3SQ4dOlS2Xb3/oQ99qGT35MmTJburOXfuXD7zmc+UbD/99NMlu0ly8eLFsu0kOX/+fNn2/v37y7bXy+TkZHbt2lWyffjw4ZLdJJmamirbTpKVlZWy7VOnTpXsLi8vl+w+G8MwlOw+8MADJbtJ8sEPfrBsO0luvPHGsu1rr722bPtyPNEBANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLYm13Lw0tJSvva1r5WcyMMPP1yymyTjOJZtJ8nKykrZ9s6dO0t2N2zYULK7mgsXLuRLX/pSyfbi4mLJblL/53Xs2LGy7a9+9atl2+tl48aN2bNnT8n28573vJLdJDly5EjZdpJMTq7plr4mCwsLJbuV98/VXvfs2bMl21/5yldKdpPkmWeeKdtOko9+9KNl2694xSvKti/HEx0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbk+t9Av9o+/btZduHDx8u206S2dnZsu03vOENJbu/8zu/U7K7mpWVlVy4cKFk+/Tp0yW7STIxUfs7wc6dO8u2z58/X7a9XrZt25Z/8k/+Scn2/v37S3aT+u/F/Px82fbjjz9ett3N008/XbZ9ww03lG0nyfXXX1+2vbKyUrZ9OZ7oAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hrGcXz2Bw/D0SQH606HK+imcRx3X+kXdQ214zri2+Ua4rlwyetoTaEDAPCdxEdXAEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQ1uZaDN27cOE5PT5ecSNVukiwuLpZtJ8nKykrZ9uzsbMnu6dOnc/78+aFk/DKGYRirtq+99tqq6Wzfvr1sO0k2btxYtr20tFS2/cUvfvHYOI67y17gEiqvo8nJNd0W12TDhg1l20kyMVH3u+vWrVtLdtfrXjQ3NzdeffXVJdunT58u2U2SU6dOlW0ntfeL5eXlsu0kl7wXrekdPT09nRe+8IXPzSl9i5tvvrlkN0mefvrpsu0kOX/+fNn293//95fs/tEf/VHJ7rNR9YPk537u50p2k+T1r3992XaS7Nmzp2z7a1/7Wtn2HXfccbBsfJ3s3LmzbLs6mDdv3ly2/ZrXvKZk9/3vf3/J7mquvvrqvO1tbyvZ/i//5b+U7CbJn/7pn5ZtJ8nRo0fLtosj7ZL3Ih9dAQBtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtibXcvDCwkIeffTRkhPZuXNnyW6SLC4ulm0nya5du8q2X/WqV5Xs3nPPPSW7z8bS0lLJ7szMTMluktx5551l20ly+vTpsu0PfOADZdvrZWZmJrfcckvJ9ste9rKS3STZu3dv2XaSbNiwoWz7woULJbsTE+vz+/bk5GSuvvrqku2NGzeW7CbJi170orLtJJmfny/bfvLJJ8u2v/zlL1/ya57oAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2ppcy8ErKytZWFgoOZGTJ0+W7CbJtddeW7adJHv27CnbvuGGG0p2p6amSnZXMwxDJifXdNk9a5/73OdKdpPk7rvvLttOkjvvvLNs++abby7bXi/bt2/PP//n/7xk++Uvf3nJbpJs3LixbDv5+vuryuOPP16yOz09XbK7mnEcc+HChZLtW265pWQ3SRYXF8u2k+TMmTNl2+fOnSvbvhxPdACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgrcm1HLyyspIzZ86UnMj09HTJbpLs3r27bDtJXvjCF5Zt33XXXSW7s7OzJburmZiYyObNm0u2Dxw4ULKbJA899FDZdpLs2bOnbHu9vteVNm/enJe85CUl23v37i3ZTZJxHMu2k2QYhrLtrVu3luzOzMyU7D6b133Ri160Lq/97aj+8/rKV75Str2wsFC2/eCDD17ya57oAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2ppcy8EbN27Mnj17Sk7k+c9/fslukszOzpZtJ8mWLVvKticmerXo8vJyTp06VbI9PT1dspskR48eLdtOkg9/+MNl25XvrfWyZcuWvPKVryzZnpxc021xTarfz3Nzc2XbX/7yl0t2N2zYULL7bF53x44dJdvbt28v2U2SpaWlsu2k9hodhqFs+3J6/RQFAPgmQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoZxHJ/9wcNwNMnButPhCrppHMfdV/pFXUPtuI74drmGeC5c8jpaU+gAAHwn8dEVANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG1NruXgXbt2jfv27Ss5kfn5+ZLdJFlZWSnbTpINGzaUbS8sLJTsnjhxIs8888xQMn4ZMzMz49zcXMn20aNHS3aT+mto69atZdvbt28v2z58+PCxcRx3l73AJWzfvn3cu3dvyfby8nLJblJ7n0uSs2fPlm1XXUcnT57M2bNnr/i96KqrrhpvuOGGku1nnnmmZDdJTp8+XbadJGfOnCnbvnDhQtl2kkvei9YUOvv27cv+/fufm1P6Fvfee2/JblJ70SXJtm3byrYff/zxkt3f/M3fLNldzdzcXH76p3+6ZPt3fud3SnaT8jdo7rrrrrLtH//xHy/b/rf/9t8eLBu/jL179+a9731vyXblD5J77rmnbDtJHnzwwbLtquvot37rt0p2V3PDDTfkYx/7WMn2/fffX7KbJB/96EfLtpPk4x//eNn2F77whbLtJJe8F/noCgBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2Jtdy8JkzZ/Kxj32s5EQefvjhkt0kWVhYKNtOkr1795ZtX3XVVSW7k5Nr+tY/Z1ZWVnL27NmS7YsXL5bsJl8/70obN24s215cXCzbXi/Hjh3Lu971rpLtV7ziFSW7SfK3f/u3ZdtJ8pWvfKVs++Mf/3jJ7pkzZ0p2n42q9/Xu3btLdpPkzjvvLNtOau9127dvL9u+//77L/k1T3QAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtTa7l4Pn5+dx7770lJ3L48OGS3SRZWloq206Sp59+umz72muvLdm9cOFCye5qdu7cmZ/6qZ8q2b799ttLdpPk3LlzZdtJ8vDDD5dtb968uWx7vczPz+cjH/lIyfaBAwdKdpNk//79ZdtJMj09XbZ99OjRkt3q+/OljONY9trbtm0r2U2S66+/vmw7SU6ePFm2vWnTprLt+++//5Jf80QHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANqaXMvB58+fzyOPPFJyIidPnizZTZJxHMu2k+TIkSNl21/5yldKdp955pmS3dXMzs7mZS97Wcn24uJiyW6SHDp0qGw7SW688cay7dOnT5dtr5elpaUcO3asZPtLX/pSyW7y9fOutGnTprLtH/7hHy7ZfeKJJ0p2VzM5OZmrrrqqZHt+fr5kN0muueaasu0k2bhxY9n2k08+WbZ9OZ7oAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2ppcy8GLi4t58sknS07k6aefLtlNkhMnTpRtJ8nMzEzZ9jXXXFOyu7CwULK7mmEYsmnTppLt7du3l+wmyQMPPFC2nST79+8v2/6Zn/mZsu31Mo5jzp8/X7J92223lewmyY/8yI+UbSfJHXfcUbb9r//1vy7Zve+++0p2VzMMQ6ampkq25+bmSnaTZPPmzWXbSbJ79+6y7euvv75s+3I80QEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQ1jOP47A8ehqNJDtadDlfQTeM47r7SL+oaasd1xLfLNcRz4ZLX0ZpCBwDgO4mPrgCAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoa3ItBw/DME5M1LTR9u3bS3aTZOvWrWXbSXLq1Kmy7fn5+bLtcRyHsvFLmJiYGDds2FCyPY5jyW6SbNq0qWw7SbZs2VK2ffHixbLtkydPHhvHcXfZC1zCzMzMODc3V7I9NTVVspskw3DF33LPmcXFxZLd+fn5nDt37or/wUxNTY2bN28u2d65c2fJblJ7fSbJuXPnyraPHz9etn3u3LlL3ovWFDoTExNlN+Qf/dEfLdlNkte+9rVl20nyp3/6p2Xbf/Inf1K2vR42bNiQHTt2lGwvLS2V7CbJrbfeWradJK9+9avLtg8dOlS2/YEPfOBg2fhlzM3N5ad/+qdLtm+66aaS3SSZnFzTLXfNqn6JSJLDhw+X7L7rXe8q2V3N5s2b84M/+IMl2z/5kz9Zspsk+/btK9tOks985jNl23/4h39Ytr1///5L3ot8dAUAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW5NrOfjmm2/O2972tpoTmVzTqazJxERtz/3Lf/kvy7Zf9apXlez+h//wH0p2V7Nhw4bMzc2VbG/atKlkN0lWVlbKtpNkcXGxbPvLX/5y2fZ6mZqayo033liy/fznP79kN0l27NhRtp0ks7OzZdvvete7SnaXl5dLdp+Nqp8Ne/fuLdlNkhe/+MVl20mybdu2su3PfOYzZdv79++/5Nc80QEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQ1uZaDp6enc/PNN5ecyMLCQslukpw9e7ZsO0m2bNlStl315z09PV2yu5phGMpee3FxsWQ3SSYn1/RWWbO3vOUtZdsPPvhg2fZ62bBhQ7Zv316yfejQoZLdJPmRH/mRsu0k+Yd/+Iey7ZmZmZLdiYn1+X17GIay1962bVvJbpJs3ry5bDtJXvjCF5Zt33777WXbl+OJDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0NbmWg4dhyMaNG0tO5OLFiyW7SfLqV7+6bDtJHnnkkbLtI0eOlG2vhwsXLuThhx8u2f6e7/mekt0kmZio/Z1genq6bPvNb35z2fbf/M3flG1fzpkzZ/KXf/mXJdvXXXddyW6SvOUtbynbTpLNmzeXbb/kJS8p2f3gBz9YsruaxcXFPPXUUyXbx44dK9lN6u9F4ziWbVf+nL8cT3QAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtTa7l4IsXL+aJJ54oOZHz58+X7CbJ6dOny7aTZHFxsWz77//+70t2L1y4ULK7mpmZmbzgBS9Yl9f+dvzar/1a6f6hQ4fKtn/iJ36ibHu9HD9+PO9+97tLtiu/1/fdd1/ZdpK89KUvLdt+3eteV7K7cePGkt3VnD17Np/85CdLtt/znveU7CbJpz71qbLtpPbn5e///u+XbV+OJzoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hnEcn/3Bw3A0ycG60+EKumkcx91X+kVdQ+24jvh2uYZ4LlzyOlpT6AAAfCfx0RUA0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbU2u5eBhGMaqE9m7d2/VdGZmZsq2k2T79u1l24cOHSrZPXPmTM6fPz+UjF/G7OzsuHPnzpLtTZs2lewmyYYNG8q2k2Qcy95aWVhYKNs+ePDgsXEcd5e9wCVs3bp1vOqqq0q2Z2dnS3aT5KmnnirbTpKzZ8+WbQ9Dze1icXExy8vLV/xetHXr1nHXrl0l22fOnCnZTZL5+fmy7aT259k111xTtv3FL37xkveiNYVOpZ//+Z8v237Ri15Utp0k/+Jf/Iuy7X/zb/5Nye4f//Efl+yuZufOnfl3/+7flWzfeuutJbtJ7Zs/qY2RRx99tGz7TW9608Gy8cu46qqr8ta3vrVk+6UvfWnJbpL8xm/8Rtl2kjz44INl25OTNT8uDh8+XLK7ml27duVXf/VXS7b/4i/+omQ3Se65556y7ST50R/90bLtX/qlXyrbvvPOOy95L/LRFQDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtTa7l4F27duXHf/zHS05keXm5ZDdJ5ufny7aT5O1vf3vZ9tNPP12yu7i4WLK7mmEYMjU1VbI9Obmmy3lNtm7dWradJC9+8YvLto8fP162vV6OHTuWd77znSXb73vf+0p2k/rvxezsbNn2OI4lu8MwlOyu5sKFC3nkkUdKtp944omS3aT+59mRI0fKtt/97neXbV+OJzoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2Jtdy8KZNm3LLLbeUnMj58+dLdpPk6NGjZdtJcu7cubLtF73oRSW7n/zkJ0t2VzMxMZHZ2dmS7e/5nu8p2U2S6enpsu0k+fznP1+2XX39r4eFhYU8+uijJdtbt24t2U2+fv1X2rVrV9n2yZMnS3bHcSzZXc3Zs2fzqU99qmR7eXm5ZPdKuHDhQtn2j/3Yj5Vtv/3tb7/k1zzRAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2Jtdy8DiOWV5eLjmRw4cPl+wmyXXXXVe2nSQXLlwo27777rtLdo8fP16yu5qJiYlMTU2VbJ88ebJkN0nuuOOOsu0keeihh8q2z5w5U7a9XpaWlnL06NGS7WuvvbZkN0ne9KY3lW0nye233162/dd//dclu+94xztKdlczOzubl7/85SXbH/7wh0t2k2Tr1q1l28nX79FV7rvvvrLty/FEBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0NbkWg5eWlrK8ePHS07klltuKdlNkn379pVtJ8kXvvCFsu2f/dmfLdl95zvfWbK7momJiWzbtq1k+2tf+1rJbpK8//3vL9tOki9+8Ytl27/2a79Wtr1epqeny97Xn/70p0t2k2RqaqpsO0nOnTtXtv3xj3+8ZHccx5Ld1Vx33XX5jd/4jZLtqp+TSfKf//N/LttOkoWFhbLt6p/Fl+KJDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoK1hHMdnf/AwHE1ysO50uIJuGsdx95V+UddQO64jvl2uIZ4Ll7yO1hQ6AADfSXx0BQC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbk2s5eG5ubrzmmmtKTmTDhg0lu1fCMAzrfQpr9tRTT+XkyZNX/MS3bt067tq1q2R78+bNJbtJ/ff49OnTZdtnzpwp256fnz82juPushe4hLm5ufHqq68u2V5cXCzZTZJnnnmmbDup/V4vLy+X7K6srGRlZeWK34t27do17tu3r2T7scceK9lNkqWlpbLtJJmcXFMWrMnERN2zlWPHjl3yXrSmf6Nrrrkmv/3bv/3cnNW3mJubK9lN6i+Mqampsu2qAHzjG99YsruaXbt25Vd/9VdLtu+8886S3SSZmZkp206SP/uzPyvb/su//Muy7Q9/+MMHy8Yv4+qrr87b3/72ku0nn3yyZDdJHnjggbLtJPnYxz5Wtn3q1KmS3er4u5R9+/Zl//79JduV99f5+fmy7STZvn172fa2bdvKtt/xjndc8l7koysAoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2ppcy8EbNmzI7OxsyYksLCyU7CbJ0tJS2XaSXLhwoWx7ZmamZHccx5Ld1UxNTeX6668v2d67d2/JbpIsLy+XbSfJ61//+rLtJ554omx7Pa2srJTsHj9+vGQ3SbZv3162nSS33HJL2XbV+/bee+8t2V3N8ePH8wd/8Acl2y9+8YtLdpPkz//8z8u2k+TcuXNl2y972cvKti/HEx0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbk2s5eGFhIY899ljJiVy8eLFkN0m+67u+q2y72ite8YqS3dnZ2ZLd1YzjmKWlpZLtlZWVkt0kmZio/Z2g8vvxy7/8y2Xb//7f//uy7cvZuHFj9uzZU7L9yCOPlOwmyZkzZ8q2k+Q1r3lN2XbVe2DTpk0lu6tZWVnJ+fPnS7Y//OEPl+wmydVXX122nSTbtm0r3V8PnugAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFuTazn4xIkT+eAHP1hyIrfcckvJbpJs3ry5bDtJrr322rLtj370oyW7p0+fLtl9Nq9b9e/0fd/3fSW7SbJt27ay7SSZnFzTW3FNHnroobLt9bJ58+bcddddJduf/OQnS3aT2ntFkszMzJRtj+NYsjsMQ8nuaubn53PPPfeUbF999dUlu0ly8eLFsu0kWVpaKtv+xCc+UbZ9OZ7oAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2ppcy8GnTp3KvffeW3Ii+/btK9lNkkceeaRsO0n+2T/7Z2Xbu3fvLtldXl4u2V3N0tJSTpw4UbJ98eLFkt0kOXbsWNl2kvzRH/1R2fYv//Ivl22vlxMnTuR973tfyfanP/3pkt0kec973lO2nSR33nln2fYP/dAPlewuLi6W7D4bwzCU7H72s58t2U2S6enpsu0kefTRR8u2JyfXlBzPGU90AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQ3jOD77g4fhaJKDdafDFXTTOI67r/SLuobacR3x7XIN8Vy45HW0ptABAPhO4qMrAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANqaXMvB09PT45YtW0pOZHZ2tmQ3STZs2FC2nSQLCwtl2ydOnCjZXVxczPLy8lAyfhm7du0a9+3bV7J9+vTpkt0rYXl5uWx7cXGxbPvw4cPHxnHcXfYCl7Bjx47xuuuuu9Iv+22bmpoq3a+8F83Pz5ftnj179orfi4ZhGKu2t27dWjWdlZWVsu0kmZioe/5ReX1evHjxkveiNYXOli1b8sM//MPPzVl9i1e84hUlu0kyNzdXtp0kjz/+eNn2+973vpLdw4cPl+yuZt++fdm/f3/J9n333VeyeyVURtqRI0fKtn/xF3/xYNn4ZVx33XX50Ic+VLJd+YtRdZw99thjZdt/8id/UrL7jne8o2R3Pd11111l2+fPny/bTpKZmZmy7crr8/HHH7/kvchHVwBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoK3JtRw8OzubV77ylSUnsnnz5pLdJHn5y19etp0kN998c9n2f/pP/6lkdxiGkt3VHDt2LO985ztLtg8ePFiymyQ33HBD2XaSbNiwoWz70KFDZdvrZRzHrKyslGzv3bu3ZDdJtmzZUradfP3Ppcq+fftKdqempkp2V7Np06Z893d/d8n2xo0bS3aTZGFhoWy72o033li2/fjjj1/ya57oAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2ppc7xP4Ry996UvLtnfs2FG2nSRzc3Nl21dffXXJ7qFDh0p2VzOOY5aWlkq2b7vttpLdJDl9+nTZdpIcOHCgbHu9vteVFhYWSv/MqkxO1t5yN27cWLa9b9++kt3p6emS3dVMTU3lhhtuKNleWVkp2U2SnTt3lm0nyYULF0r314MnOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQ1uRaDt6xY0de//rXV51LmU996lOl+zt27Cjbvummm0p2P//5z5fsrmZpaSlHjx4t2T537lzJbpKcPXu2bDtJjh8/Xrb9oQ99qGx7vZw4cSLvfe97S7Z/6Zd+qWQ3SR577LGy7SQZhqFs++677y7ZPXHiRMnuasZxzNLSUsn2mTNnSnaT5KGHHirbTpLbbrutdH89eKIDALQldACAtoQOANCW0AEA2hI6AEBbQgcAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoa3ItB8/Pz+eee+4pOZFf+IVfKNlNkh07dpRtJ8kTTzxRtv0Hf/AHZdvr4fjx43nPe95Tsv2Lv/iLJbtJMjc3V7adJBMTdb9zTE1NlW1fvHixbPtylpaWcvz48ZLtT3ziEyW7SbK8vFy2nSR///d/X7b97ne/u2x7PbzgBS/IRz/60ZLtN73pTSW7SXLkyJGy7aT25+XJkyfLti/HEx0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGhL6AAAbQkdAKAtoQMAtCV0AIC2hA4A0JbQAQDaEjoAQFtCBwBoS+gAAG0JHQCgLaEDALQldACAtoQOANCW0AEA2hI6AEBbwziOz/7gYTia5GDd6XAF3TSO4+4r/aKuoXZcR3y7XEM8Fy55Ha0pdAAAvpP46AoAaEvoAABtCR0AoC2hAwC0JXQAgLaEDgDQltABANoSOgBAW0IHAGjr/wXcc0FDzylS1QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(sign_classifier.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 4, 4, 'conv0', size=(10,10))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignClassifier(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Dropout(p=0.3, inplace=False)\n",
       "    (5): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout(p=0.3, inplace=False)\n",
       "    (10): Conv2d(16, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=11, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "sign_classifier.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "sign_classifier.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "sign_classifier.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(sign_classifier, dummy_input, onnx_sign_classifier_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "sign_classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 823.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[-19.194838    2.7842922  -4.7225633 -18.989916    4.6124454   8.458703\n",
      "   -6.439257  -11.774608  -16.28502    -2.518611    4.2926397]]\n",
      "Predictions shape: (1, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_sign_classifier_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
