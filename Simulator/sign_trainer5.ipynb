{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "# NOTE : Sign detecttion uses the folder training_images to load the backgrounds\n",
    "# it uses the folder sign_imgs to load the signs, the rate of examples is important\n",
    "num_channels = 1 # COLOR IMGS\n",
    "SIZE = (16, 16)\n",
    "model_name = 'models/sign_classifier.pt'\n",
    "onnx_sign_classifier_path = \"models/sign_classifier.onnx\"\n",
    "max_load = 5000  #63000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 examples loaded for 10 class_names\n",
      "example labels: [7, 9, 9, 9, 5, 6, 1, 4, 8, 0, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# LOAD EXAMPLES\n",
    "# imgs of the examples must be in a specific folder (ex: sign_imgs), and must be named as \"class_<number>.png\"\n",
    "# ex: sign_imgs/stop_1.png, note: start from 1\n",
    "\n",
    "#load examples\n",
    "examples_folder = 'sign_imgs'     \n",
    "class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway', 'nosign'] \n",
    "# class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway', 'trafficlight', 'nosign']  # with traffic_light\n",
    "\n",
    "# class_names = ['park', 'closedroad', 'highwayexit', 'highwayenter', 'stop', 'roundabout', 'priority', 'crosswalk', 'oneway']\n",
    "\n",
    "file_names = [f for f in os.listdir(examples_folder) if f.endswith('.png')]\n",
    "example_labels = [class_names.index(name.split('_')[0]) for name in file_names if name.split('_')[0] in class_names]\n",
    "example_imgs = [cv.resize(cv.blur(cv.imread(os.path.join(examples_folder, name)), (5,5)), (128,128)) for name in file_names if name.split('_')[0] in class_names]\n",
    "example_masks = []\n",
    "for example in example_imgs:\n",
    "    example_mask = np.where(example == np.array([0,0,0]), np.zeros_like(example), 255*np.ones_like(example))\n",
    "    example_mask = example_mask.astype(np.uint8)\n",
    "    example_mask = cv.cvtColor(example_mask, cv.COLOR_BGR2GRAY)\n",
    "    #convert to bitmap\n",
    "    example_mask = np.where(example_mask == 0, 0, 255)\n",
    "    example_mask = example_mask.astype(np.uint8)\n",
    "    #back to BGR\n",
    "    example_mask = cv.cvtColor(example_mask, cv.COLOR_GRAY2BGR)\n",
    "    if np.max(example_mask) > 0:\n",
    "        example_masks.append(example_mask)\n",
    "\n",
    "tot_examples = len(example_imgs)\n",
    "tot_classes = len(class_names) \n",
    "print(f'{tot_examples} examples loaded for {tot_classes} class_names')\n",
    "print(f'example labels: {example_labels}')    \n",
    "\n",
    "#show images\n",
    "cv.namedWindow('example', cv.WINDOW_NORMAL)\n",
    "for i in range(tot_examples):\n",
    "    img = example_imgs[i].copy()\n",
    "    # add text label\n",
    "    cv.putText(img, class_names[example_labels[i]], (10,30), cv.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "    cv.imshow('example', img)\n",
    "    # cv.imshow('example_mask', example_masks[i])\n",
    "    key = cv.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE 32x32\n",
    "\n",
    "# class SignClassifier(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=num_channels): \n",
    "#         super().__init__()\n",
    "#         p = 0.2\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=3, stride=1), #out = 12 - 28  \n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=10 - 14\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.Conv2d(8, 4, kernel_size=4, stride=2), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=5\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Dropout(p),\n",
    "#             nn.Conv2d(4, 32, kernel_size=5, stride=1), #out = 1\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(p),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             # nn.Linear(in_features=4*4*4, out_features=128),\n",
    "#             # nn.ReLU(True),\n",
    "#             # nn.Dropout(p),\n",
    "#             # nn.Linear(in_features=128, out_features=out_dim),\n",
    "#             nn.Linear(in_features=32*1*1, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# sign_classifier = SignClassifier(out_dim=tot_classes,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE 16x16\n",
    "\n",
    "class SignClassifier(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=num_channels): \n",
    "        super().__init__()\n",
    "        p = 0.3\n",
    "        ### Convoluational layers\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 4, kernel_size=5, stride=1), #out = 12\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1), #out=10\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p),\n",
    "            nn.Conv2d(4, 8, kernel_size=5, stride=1), #out = 6\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1), #out=5\n",
    "            # nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p),\n",
    "            nn.Conv2d(8, 64, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            # nn.Linear(in_features=4*4*4, out_features=128),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.Dropout(p),\n",
    "            # nn.Linear(in_features=128, out_features=out_dim),\n",
    "            nn.Linear(in_features=64*1*1, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "sign_classifier = SignClassifier(out_dim=tot_classes,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 16, 16])\n",
      "out shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "sign_classifier.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = sign_classifier(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load coco dataset and create background img in the background folder\n",
    "\n",
    "# import json\n",
    "\n",
    "# MAX_BACKGROUNDS = 500_000\n",
    "\n",
    "# #get all file names inside fodler sign_imgs/coco_val\n",
    "# # coco_val_img_names = [os.path.join('sign_imgs', 'coco_val', f) for f in os.listdir(os.path.join('sign_imgs', 'coco_val'))]\n",
    "# coco_val_img_names = [f for f in os.listdir(os.path.join('sign_imgs', 'coco_val'))]\n",
    "\n",
    "# print(f'{len(coco_val_img_names)} images in coco val')\n",
    "\n",
    "# #get all instances\n",
    "# #load json file\n",
    "# with open('sign_imgs/coco_val/instances_val2017.json') as f:\n",
    "#     data = json.load(f)\n",
    "#     #print name of the file\n",
    "#     print(f'images = {len(data[\"images\"])}')\n",
    "#     print(f'annotations = {len(data[\"annotations\"])}')\n",
    "\n",
    "\n",
    "#     categories = {cat['id']:cat['name'] for cat in data['categories']}\n",
    "#     categories_by_name = {cat['name']:cat['id'] for cat in data['categories']}\n",
    "\n",
    "#     filtered_categories = ['traffic light', 'bicycle', 'car', 'motorcycle', 'bus', 'truck',\n",
    "#                             'fire hydrant', 'stop sign',  'parking meter']\n",
    "#     filtered_categories_idxs = [categories_by_name[c] for c in filtered_categories]\n",
    "\n",
    "#     img_names_by_id = {img['id']:img['file_name'] for img in data['images']}\n",
    "\n",
    "#     categ_by_id = {img['id']:[] for img in data['images']}\n",
    "\n",
    "#     for ann in data['annotations']:\n",
    "#         categ_id = ann['category_id']\n",
    "#         img_id = ann['image_id']\n",
    "#         if img_id in categ_by_id:\n",
    "#             categ_by_id[img_id].append(categories[categ_id])\n",
    "#         if categ_id in filtered_categories_idxs:\n",
    "#             img_names_by_id.pop(img_id, None)\n",
    "#             categ_by_id.pop(img_id, None)\n",
    "    \n",
    "#     for id, c in categ_by_id.items():\n",
    "#         if len(c) == 0:\n",
    "#             img_names_by_id.pop(id, None)\n",
    "\n",
    "#     print(f'final images = {len(img_names_by_id)}')\n",
    "\n",
    "#     BACKGROUND_SIZE = (320,240)\n",
    "#     idx = 0\n",
    "#     for k, img_name in img_names_by_id.items():\n",
    "#         img_name = img_names_by_id[k]\n",
    "#         categories_in_img = categ_by_id[k]\n",
    "#         img = cv.imread(os.path.join('sign_imgs', 'coco_val', img_name))\n",
    "#         img = cv.resize(img, (2*BACKGROUND_SIZE[0], 2*BACKGROUND_SIZE[1]))\n",
    "#         #divide the image into 4 parts\n",
    "#         img_parts = [img[:BACKGROUND_SIZE[1], :BACKGROUND_SIZE[0]],\n",
    "#                     img[:BACKGROUND_SIZE[1], BACKGROUND_SIZE[0]:],\n",
    "#                     img[BACKGROUND_SIZE[1]:, :BACKGROUND_SIZE[0]],\n",
    "#                     img[BACKGROUND_SIZE[1]:, BACKGROUND_SIZE[0]:]]\n",
    "\n",
    "#         for i in range(4):\n",
    "#             img_part = img_parts[i]\n",
    "#             #further divide the image into 4 parts\n",
    "#             img_part_parts = [img_part[:BACKGROUND_SIZE[1]//2, :BACKGROUND_SIZE[0]//2],\n",
    "#                             img_part[:BACKGROUND_SIZE[1]//2, BACKGROUND_SIZE[0]//2:],\n",
    "#                             img_part[BACKGROUND_SIZE[1]//2:, :BACKGROUND_SIZE[0]//2],\n",
    "#                             img_part[BACKGROUND_SIZE[1]//2:, BACKGROUND_SIZE[0]//2:]]\n",
    "#             for j in range(4):\n",
    "#                 img_part_part = img_part_parts[j]\n",
    "#                 cv.imshow(f'img_{i}{j}', img_part_parts[j])\n",
    "#                 idx += 1\n",
    "#                 cv.imwrite(os.path.join('sign_imgs', 'backgrounds', f'background_{idx}.png'), img_part_parts[j])\n",
    "\n",
    "#         print(f'{categories_in_img}')\n",
    "#         key = cv.waitKey(1)\n",
    "#         if key == 27 or idx > MAX_BACKGROUNDS:\n",
    "#             break\n",
    "\n",
    "# cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irong/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/ipykernel_launcher.py:164: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "import random\n",
    "\n",
    "def draw_box(img, x, y, w):\n",
    "    img = cv.rectangle(img, (int(x-w/2), int(y-w/2)), (int(x+w/2), int(y+w/2)), 255, 2)\n",
    "    return img\n",
    "\n",
    "def load_and_augment_img(img, example_index=0, example_imgs=example_imgs):\n",
    "    # img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "    if img is not None:\n",
    "        #convert to gray\n",
    "        # img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "        # #crop to the top right quadrant\n",
    "        img = img[0:img.shape[1]//2, img.shape[1]//2:]\n",
    "        canv_dim = randint(64//2, 160//2)  ## RANGE OF DIMENSION OF THE SIGN\n",
    "        start_x = randint(0, img.shape[1]-canv_dim) if img.shape[1] > canv_dim else 0\n",
    "        start_y = randint(0, img.shape[0]-canv_dim) if img.shape[0] > canv_dim else 0\n",
    "        img = img[start_y:start_y+canv_dim, start_x:start_x+canv_dim]\n",
    "        # img = cv.resize(img, (2*SIZE[0], 2*SIZE[1]))\n",
    "    else:\n",
    "        img = randint(0,255,(2*SIZE[0], 2*SIZE[1]), dtype=np.uint8)\n",
    "\n",
    "\n",
    "    background_avg_brightness = np.mean(img)\n",
    "\n",
    "    ## EXAMPLE AUGMENTATION ##############################################################\n",
    "    #load example\n",
    "    example = example_imgs[example_index]\n",
    "    random_example_mask = random.choice(example_masks)\n",
    "    resize_ratio = max(img.shape)/max(example.shape)\n",
    "    example = cv.resize(example, (int(example.shape[1]*resize_ratio), int(example.shape[0]*resize_ratio)))\n",
    "    random_example_mask = cv.resize(random_example_mask, (example.shape[1], example.shape[0]))\n",
    "    #Make a black border aroud the example\n",
    "    example[0:2,:,:] = np.array([0,0,0])\n",
    "    example[-3:-1,:,:] = np.array([0,0,0])\n",
    "    example[:,0:2,:] = np.array([0,0,0])\n",
    "    example[:,-3:-1,:] = np.array([0,0,0])\n",
    "    random_example_mask[0:2,:,:] = 0\n",
    "    random_example_mask[-3:-1,:,:] = 0\n",
    "    random_example_mask[:,0:2,:] = 0\n",
    "    random_example_mask[:,-3:-1,:] = 0\n",
    "\n",
    "    #get example mask\n",
    "    example_mask = np.where(example == np.array([0,0,0]), np.zeros_like(example), 255*np.ones_like(example))\n",
    "    example_mask = example_mask.astype(np.uint8)\n",
    "    example_mask = cv.cvtColor(example_mask, cv.COLOR_BGR2GRAY)\n",
    "    #convert to bitmap\n",
    "    example_mask = np.where(example_mask == 0, 0, 255)\n",
    "    example_mask = example_mask.astype(np.uint8)\n",
    "    #back to BGR\n",
    "    example_mask = cv.cvtColor(example_mask, cv.COLOR_GRAY2BGR)\n",
    "    #bitwise and of the 2 masks\n",
    "    example_mask = cv.bitwise_and(example_mask, random_example_mask)\n",
    "    \n",
    "    # add noise to the example\n",
    "    std = 20\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.subtract(example, noisem)\n",
    "    noisep = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.add(example, noisep)\n",
    "\n",
    "    #set zero where example mask is zero\n",
    "    example = np.where(example_mask == np.array([0,0,0]), np.zeros_like(example), example)\n",
    "\n",
    "    #perspective transform example\n",
    "    perspective_deformation = img.shape[0]//8 ################# DEFORMATION\n",
    "    pts1 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = pts2 + np.float32(randint(0,perspective_deformation,size=pts2.shape))\n",
    "    # print(f'pts2 = \\n{pts2}')\n",
    "    new_size_x = int(np.max(pts2[:,0]) - np.min(pts2[:,0]))\n",
    "    new_size_y = int(np.max(pts2[:,1]) - np.min(pts2[:,1]))\n",
    "    M = cv.getPerspectiveTransform(pts1,pts2)\n",
    "    example = cv.warpPerspective(example,M,(new_size_x,new_size_y))\n",
    "\n",
    "    #resize example keeping proportions\n",
    "    img_example_ratio = min(img.shape[0]/example.shape[0], img.shape[1]/example.shape[1])\n",
    "    scale_factor = np.random.uniform(.5, .98) ##.15, .35############################ PARAM ##############################\n",
    "    scale_factor = scale_factor * img_example_ratio\n",
    "    example = cv.resize(example, (0,0), fx=scale_factor, fy=scale_factor)\n",
    "    #match img shape\n",
    "    example_canvas = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    #get a random position for the example\n",
    "    example_y = randint(0, img.shape[0] - example.shape[0])\n",
    "    example_x = randint(0, img.shape[1] - example.shape[1])\n",
    "    #paste example on canvas\n",
    "    example_canvas[example_y:example_y+example.shape[0], example_x:example_x+example.shape[1]] = example\n",
    "\n",
    "    #canvas mask\n",
    "    example_canvas_mask = example_canvas.copy()\n",
    "    example_canvas_mask = cv.cvtColor(example_canvas_mask, cv.COLOR_BGR2GRAY)\n",
    "    example_canvas_mask = np.where(example_canvas_mask == 0, 0, 255)\n",
    "    example_canvas_mask = example_canvas_mask.astype(np.uint8)\n",
    "    edge_of_canvas = cv.Canny(example_canvas_mask, 100, 200)\n",
    "    ker_rand = randint(2,5)\n",
    "    edge_of_canvas = cv.dilate(edge_of_canvas, np.ones((ker_rand,ker_rand)))\n",
    "    #erode mask\n",
    "    ker_rand = randint(2,5) if SIZE == (32,32) else randint(1,3)\n",
    "    kernel = np.ones((ker_rand,ker_rand),np.uint8)\n",
    "    iter_rand = randint(1,3) if SIZE == (32,32) else randint(1,3)\n",
    "    example_canvas_mask = cv.erode(example_canvas_mask,kernel,iterations=iter_rand)\n",
    "\n",
    "    #convert to hsv\n",
    "    example_canvas = cv.cvtColor(example_canvas, cv.COLOR_BGR2HSV)\n",
    "    #get the hsv channels\n",
    "    example_canvas_h = example_canvas[:,:,0]\n",
    "    example_canvas_s = example_canvas[:,:,1]\n",
    "    example_canvas_v = example_canvas[:,:,2]\n",
    "\n",
    "    # #reduce brightness\n",
    "    example_avg_brightness = np.mean(example_canvas_v)\n",
    "    if example_avg_brightness > 0.0:\n",
    "        example_avg_brightness = np.mean(example_canvas_v, where=example_canvas_v>0)\n",
    "    diff = -example_avg_brightness +background_avg_brightness\n",
    "    brightness_shift = int(round(diff) + randint(-150,150))\n",
    "    brightness_shift = np.clip(brightness_shift, -255, 255)\n",
    "    # example_canvas_v = np.clip(example_canvas_v + brightness_shift, 0, 255).astype(np.uint8)\n",
    "    if brightness_shift > 0:\n",
    "        example_canvas_v = cv.add(example_canvas_v, np.ones_like(example_canvas_v)*brightness_shift)\n",
    "    elif brightness_shift < 0:\n",
    "        example_canvas_v = cv.subtract(example_canvas_v, np.ones_like(example_canvas_v)*abs(brightness_shift))\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.25,.9)\n",
    "    example_canvas_v = np.clip(127*(1-const) + example_canvas_v*const, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #augment hue\n",
    "    hue_shift = randint(-10,10)\n",
    "    example_canvas_h = (example_canvas_h + hue_shift) % 180\n",
    "\n",
    "    #augment saturation\n",
    "    sat_shift = randint(-100,0)\n",
    "    example_canvas_s = np.clip(example_canvas_s + sat_shift, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #rebuild the channels\n",
    "    example_canvas[:,:,0] = example_canvas_h\n",
    "    example_canvas[:,:,1] = example_canvas_s\n",
    "    example_canvas[:,:,2] = example_canvas_v\n",
    "    #back to bgr\n",
    "    example_canvas = cv.cvtColor(example_canvas, cv.COLOR_HSV2BGR)\n",
    "\n",
    "    #paste canvas on img\n",
    "    img_b = img[:,:,0]\n",
    "    img_g = img[:,:,1]\n",
    "    img_r = img[:,:,2]\n",
    "    example_b = example_canvas[:,:,0]\n",
    "    example_g = example_canvas[:,:,1]\n",
    "    example_r = example_canvas[:,:,2]\n",
    "    img_b = np.where(example_canvas_mask > 0, example_b, img_b)\n",
    "    img_g = np.where(example_canvas_mask > 0, example_g, img_g)\n",
    "    img_r = np.where(example_canvas_mask > 0, example_r, img_r)\n",
    "    # img = np.where(example_canvas_mask > 0, example_canvas, img) \n",
    "    img[:,:,0] = img_b\n",
    "    img[:,:,1] = img_g\n",
    "    img[:,:,2] = img_r\n",
    "\n",
    "    ker_rand = randint(2,5)\n",
    "    blurred_img = cv.blur(img, (ker_rand,ker_rand))\n",
    "    img = np.where(edge_of_canvas ==np.array([0,0,0]), blurred_img, img)\n",
    "\n",
    "    ##########################################################################################\n",
    "    \n",
    "    # convert whole img to hsv\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "    #get the hsv channels\n",
    "    img_h = img[:,:,0]\n",
    "    img_s = img[:,:,1]\n",
    "    img_v = img[:,:,2]\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.5,.99)\n",
    "    img_v = np.clip(127*(1-const) + img_v*const, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #reduce brightness\n",
    "    brightness_shift = randint(-40,0)\n",
    "    if brightness_shift > 0:\n",
    "        img_v = cv.add(img_v, np.ones_like(img_v)*brightness_shift)\n",
    "    elif brightness_shift < 0:\n",
    "        img_v = cv.subtract(img_v, np.ones_like(img_v)*abs(brightness_shift))\n",
    "\n",
    "    #augment hue\n",
    "    hue_shift = randint(-2,2)\n",
    "    img_h = (img_h + hue_shift) % 180\n",
    "\n",
    "    #augment saturation\n",
    "    sat_shift = randint(-20,0)\n",
    "    img_s = np.clip(img_s + sat_shift, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #rebuild the channels\n",
    "    img[:,:,0] = img_h\n",
    "    img[:,:,1] = img_s\n",
    "    img[:,:,2] = img_v\n",
    "    #back to bgr\n",
    "    img = cv.cvtColor(img, cv.COLOR_HSV2BGR)\n",
    "\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = img.shape[0]//4\n",
    "    offset = np.random.randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        # img[:offset, :] = np.random.randint(0,255)\n",
    "        img = img[offset:, :]\n",
    "    elif offset < 0:\n",
    "        # img[offset:, :] = np.random.randint(0,255)\n",
    "        img = img[:offset, :]\n",
    "\n",
    "    offset_y = np.random.randint(-max_offset, max_offset)\n",
    "    # img = np.roll(img, offset_y, axis=1)\n",
    "    if offset_y > 0:\n",
    "        # img[:, :offset_y] = np.random.randint(0,255)\n",
    "        img = img[:, offset_y:]\n",
    "    elif offset_y < 0:\n",
    "        # img[:, offset_y:] = np.random.randint(0,255)\n",
    "        img = img[:, :offset_y]\n",
    "\n",
    "    min_dim = min(img.shape[0], img.shape[1])\n",
    "    #crop to square\n",
    "    img = img[:min_dim, :min_dim]\n",
    "\n",
    "    #add noise\n",
    "    std = 50\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "\n",
    "    # # crop into the img at random position\n",
    "    # zoom = randint(0, SIZE[0]//4)\n",
    "    # img = img[zoom:img.shape[0]-zoom, zoom:img.shape[1]-zoom]\n",
    "\n",
    "    #blur \n",
    "    b = randint(2,3) \n",
    "    img = cv.blur(img, (b,b))\n",
    "    \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    #convert to gray\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY) if num_channels == 1 else img\n",
    "\n",
    "    # #convert to hsv\n",
    "    # img = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('sign_imgs', 'backgrounds',  f'background_{i+1}.png'))\n",
    "    # img = cv.resize(img, (160,120))\n",
    "    # img = None\n",
    "    img = load_and_augment_img(img, example_index=(i%tot_examples))\n",
    "\n",
    "    #to bgr\n",
    "    # img = cv.cvtColor(img, cv.COLOR_HSV2BGR)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(200)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = os.path.join('sign_imgs', 'backgrounds')\n",
    "        self.data = []\n",
    "        self.class_names = []\n",
    "        self.channels = channels\n",
    "        self.all_imgs = torch.zeros((max_load*tot_examples, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "        cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "        # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN\n",
    "\n",
    "        for i in tqdm(range(max_load)):\n",
    "            img = cv.imread(os.path.join(self.folder, f'background_{i+1}.png'))\n",
    "            # img = cv.resize(img, (160,120))\n",
    "            for j in range(tot_examples):\n",
    "                img_j = load_and_augment_img(img.copy(), example_index=j)\n",
    "                if i < 50:\n",
    "                    cv.imshow('img', img_j)\n",
    "                    cv.waitKey(1)\n",
    "                    if i == 49:\n",
    "                        cv.destroyAllWindows()\n",
    "                #add a dimension to the image\n",
    "                img_j = img_j[:, :,np.newaxis] if self.channels == 1 else img_j\n",
    "                #convert to tensor\n",
    "                img_j = torch.from_numpy(img_j)\n",
    "                self.all_imgs[i*tot_examples+j] = img_j\n",
    "                self.class_names.append(example_labels[j])\n",
    "        \n",
    "        self.data = torch.from_numpy(np.array(self.data))\n",
    "        self.class_names = torch.from_numpy(np.array(self.class_names))\n",
    "        print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "        print(f'class_names: {self.class_names.shape}')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.class_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        class_label = self.class_names[idx]\n",
    "        return img, class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]/home/irong/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/ipykernel_launcher.py:164: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "100%|██████████| 5000/5000 [02:27<00:00, 33.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all imgs: torch.Size([60000, 16, 16, 3])\n",
      "class_names: torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset(max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=5*8192//3, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8192//3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13653, 3, 16, 16])\n",
      "torch.Size([13653])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, class_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    class_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, class_label) in tqdm(dataloader):\n",
    "        #one hot encode the class label\n",
    "        class_label = torch.eye(tot_classes)[class_label]\n",
    "        # Move the input and target data to the selected device\n",
    "        input, class_label = input.to(device), class_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        class_out = output[:, 0:]\n",
    "\n",
    "        #classification loss\n",
    "        assert class_label.shape == class_out.shape, f'class_label.shape: {class_label.shape}, output.shape: {output.shape}'\n",
    "\n",
    "        class_loss = 1.0*class_loss_fn(class_out, class_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = class_loss + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        class_losses.append(class_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    class_loss = np.mean(class_losses)\n",
    "    return  np.sqrt(class_loss)\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(sign_classifier, val_dataloader, regr_loss_fn, class_loss_fn, device=device):\n",
    "    sign_classifier.eval()\n",
    "    y_losses = []\n",
    "    x_losses = []\n",
    "    w_losses = []\n",
    "    class_losses = []\n",
    "    for (input, class_label) in tqdm(val_dataloader):\n",
    "        #one hot encode the class label\n",
    "        class_label = torch.eye(tot_classes)[class_label]\n",
    "        input, class_label = input.to(device), class_label.to(device)\n",
    "        output = sign_classifier(input)\n",
    "\n",
    "        class_loss = 1.0*class_loss_fn(output[:, 0:], class_label)\n",
    "    \n",
    "        class_losses.append(class_loss.detach().cpu().numpy())\n",
    "    return  np.mean(class_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  400/400\n",
      "class_loss: 0.6759 --- Val: 0.4246\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.002 #0.005\n",
    "epochs = 400 #50+\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 1e-4 #9e-4\n",
    "L2_lambda = 1e-3 #1e-2\n",
    "optimizer = torch.optim.Adam(sign_classifier.parameters(), lr=lr, weight_decay=3e-5) #wd = 2e-3# 9e-5\n",
    "\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "class_loss_fn = nn.CrossEntropyLoss()\n",
    "best_val = 100\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        class_loss = train_epoch(sign_classifier, train_dataloader, regr_loss_fn, class_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_class_loss = val_epoch(sign_classifier, val_dataloader, regr_loss_fn, class_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs}\")\n",
    "    print(f\"class_loss: {class_loss:.4f} --- Val: {val_class_loss:.4f}\")\n",
    "    if val_class_loss < best_val:\n",
    "        torch.save(sign_classifier.state_dict(), model_name)\n",
    "        print(f'Model saved as {model_name}')\n",
    "        best_val = val_class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "# val_dataloader2 = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "# sign_classifier.load_state_dict(torch.load(model_name))\n",
    "# sign_classifier.eval()\n",
    "# for (input, class_label) in tqdm(val_dataloader2):\n",
    "#     #convert img to numpy array\n",
    "#     img = input[0].detach().cpu().numpy()[0]\n",
    "#     #convert to sigle byte\n",
    "#     img = img.astype(np.uint8)\n",
    "#     input, box_label, class_label =input.to(device), box_label.to(device), class_label.to(device)\n",
    "#     output = sign_classifier(input)\n",
    "#     x = output[:, 0]\n",
    "#     y = output[:, 1]\n",
    "#     w = output[:, 2]\n",
    "#     class_out = output[:, 3:]\n",
    "#     # print(class_out)\n",
    "#     class_out = torch.argmax(class_out, dim=1)\n",
    "#     class_out = class_out.detach().cpu().numpy()\n",
    "#     # print(f\"Predicted class: {class_out[0]}, Actual class: {class_label[0]}\")\n",
    "#     class_out = class_out[0]\n",
    "#     class_out = class_names[class_out]\n",
    "#     true_class = class_names[class_label[0]]\n",
    "#     img = draw_box(img, x, y, w)\n",
    "#     #text for labels\n",
    "#     img = cv.putText(img, f\"True: {true_class}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "#     img = cv.putText(img, f\"Pred: {class_out}\", (10, 60), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "#     cv.imshow('img', img)\n",
    "#     key = cv.waitKey(0)\n",
    "#     if key == 27:\n",
    "#         break\n",
    "    \n",
    "# cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3, 5, 5)\n",
      "(8, 4, 5, 5)\n",
      "(64, 8, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(sign_classifier.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 4, 4, 'conv0', size=(10,10))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignClassifier(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Dropout(p=0.3, inplace=False)\n",
       "    (5): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout(p=0.3, inplace=False)\n",
       "    (10): Conv2d(8, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "sign_classifier.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "sign_classifier.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "sign_classifier.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(sign_classifier, dummy_input, onnx_sign_classifier_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "sign_classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 12032.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[-0.70095617 -3.3170595  -3.1655905  -1.2413334  -3.3756824  -3.1417756\n",
      "  -2.2464957  -1.6531334  -2.5218706   1.4232211 ]]\n",
      "Predictions shape: (1, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_sign_classifier_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
