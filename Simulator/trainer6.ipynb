{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/lane_keeper_small.pt'\n",
    "onnx_lane_keeper_path = \"models/lane_keeper_small.onnx\"\n",
    "max_load = 15_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class LaneKeeper(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "            nn.Conv2d(4, 4, kernel_size=7, stride=1), #out = 6\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            #normalize\n",
    "            nn.BatchNorm1d(3*3*4),\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=3*3*4, out_features=32),\n",
    "            # nn.ReLU(True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=32, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "lane_keeper = LaneKeeper(out_dim=2,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "lane_keeper.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = lane_keeper(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "    # img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "\n",
    "    #convert to gray\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "        axes_length = (randint(10, 50), randint(50, 300))\n",
    "        angle = randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (100,100))\n",
    "    noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = 5 * light\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    # cv.imshow('light', light)\n",
    "    # if cv.waitKey(0) == ord('q'):\n",
    "    #     break\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 9), randint(1, 9)))\n",
    "\n",
    "    # cut the top third of the image, let it 640x320\n",
    "    img = img[int(img.shape[0]/3):,:] ################################# /3\n",
    "    # assert img.shape == (320,640), f'img shape cut = {img.shape}'\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = 3\n",
    "    offset = randint(-max_offset, max_offset)\n",
    "    img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        img[:offset, :] = randint(0,255)\n",
    "    elif offset < 0:\n",
    "        img[offset:, :] = randint(0,255)\n",
    "\n",
    "    #reduce contrast\n",
    "    const = np.random.uniform(0.1,0.4)\n",
    "    # if np.random.uniform() > .5:\n",
    "    #     const = const*0.2\n",
    "    img = 127*(1-const) + img*const\n",
    "    img = img.astype(np.uint8)\n",
    "\n",
    "    #add noise \n",
    "    std = 60\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "    #blur \n",
    "    img = cv.blur(img, (randint(1,5),randint(1,5)))\n",
    "\n",
    "    #add random brightness\n",
    "    max_brightness = 50\n",
    "    brightness = randint(-max_brightness, max_brightness)\n",
    "    if brightness > 0:\n",
    "        img = cv.add(img, brightness)\n",
    "    elif brightness < 0:\n",
    "        img = cv.subtract(img, -brightness)\n",
    "    \n",
    "    # invert color\n",
    "    if np.random.uniform(0, 1) > 0.6:\n",
    "        img = cv.bitwise_not(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(100)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        #classification label for road ahead = 1,0,0,0,1,0,0,0,0,0,0,0,0,0,0\n",
    "        road_images_indexes = []\n",
    "        tot_lines = 0\n",
    "        with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            tot_lines = len(lines)\n",
    "            for i,line in enumerate(lines):\n",
    "                if line == '1,0,0,0,1,0,0,0,0,0,0,0,0,0,0':\n",
    "                    road_images_indexes.append(i)\n",
    "        print(f'total pure road images: {len(road_images_indexes)}')\n",
    "        road_imgs_mask = np.zeros(tot_lines, dtype=np.bool)\n",
    "        road_imgs_mask[road_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            # self.all_imgs = torch.zeros((2*max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8) #adding flipped img\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            # road images specifically are added again along with their flipped image and label\n",
    "            road_imgs = torch.zeros((2*len(road_images_indexes), SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "            road_labels = []\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "            road_idx = 0\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(max_load)):\n",
    "\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                sample = line.split(',')\n",
    "                #keep only info related to the lane, discard distance from stop line \n",
    "                sample = [sample[0], sample[1], sample[3]] #e2=lateral error, e3=yaw error point ahead, curvature\n",
    "                reg_label = np.array([float(s) for s in sample], dtype=np.float32)\n",
    "\n",
    "                #img \n",
    "                img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "\n",
    "                #check if its in the road images\n",
    "                if road_imgs_mask[i]:\n",
    "                    img_r = load_and_augment_img(img.copy())\n",
    "                    # cv.imshow('imgR', img_r)\n",
    "                    img_r = img_r[:,:,np.newaxis]\n",
    "\n",
    "                    img_l = cv.flip(img, 1)\n",
    "                    img_l = load_and_augment_img(img_l)\n",
    "                    # cv.imshow('imgL', img_l)\n",
    "                    img_l = img_l[:,:,np.newaxis]\n",
    "                    # cv.waitKey(1)\n",
    "\n",
    "                    road_imgs[2*road_idx] = torch.from_numpy(img_r)\n",
    "                    road_imgs[2*road_idx+1] = torch.from_numpy(img_l)\n",
    "                    road_labels.append(reg_label)\n",
    "                    road_labels.append(-reg_label)\n",
    "                    road_idx += 1\n",
    "\n",
    "                else:\n",
    "                    img = load_and_augment_img(img)\n",
    "                    if i < 100:\n",
    "                        cv.imshow('img', img)\n",
    "                        cv.waitKey(1)\n",
    "                        if i == 99:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(reg_label)\n",
    "                    all_img_idx += 1\n",
    "\n",
    "            #cut imgs to the right length\n",
    "            road_imgs = road_imgs[:2*road_idx]\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "\n",
    "            #concatenate all_imgs and road_imgs\n",
    "            print(f'road images: {road_imgs.shape}')\n",
    "            print(f'all images: {self.all_imgs.shape}')\n",
    "            self.all_imgs = torch.cat((self.all_imgs, road_imgs), dim=0)\n",
    "            print(f'self.data shape: {len(self.data)}')\n",
    "            print(f'road_labels shape = {len(road_labels)}')\n",
    "            self.data = np.concatenate((np.array(self.data), np.array(road_labels)), axis=0)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "            #free road_imgs from memory\n",
    "            del road_imgs\n",
    "            del road_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pure road images: 79427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [02:07<00:00, 117.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "road images: torch.Size([15812, 32, 32, 1])\n",
      "all images: torch.Size([7094, 32, 32, 1])\n",
      "self.data shape: 7094\n",
      "road_labels shape = 15812\n",
      "\n",
      "all imgs: torch.Size([22906, 32, 32, 1])\n",
      "data: (22906, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4096, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 1, 32, 32])\n",
      "torch.Size([4096, 3])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    err_losses2 = []\n",
    "    err_losses3 = []\n",
    "    # curv_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        err2 = output[:, 0]\n",
    "        err3 = output[:, 1]\n",
    "        # curv_out = output[:, 2]\n",
    "\n",
    "        err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 1]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        # Compute the losses\n",
    "        err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        # curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = err_loss3 + err_loss2 + L1_loss + L2_loss #+ curv_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    err_loss2 = np.mean(err_losses2)\n",
    "    err_loss3 = np.mean(err_losses3)\n",
    "    # curv_loss = np.mean(curv_losses)\n",
    "    return err_loss2, err_loss3\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device=device):\n",
    "    lane_keeper.eval()\n",
    "    err_losses3 = []\n",
    "    err_losses2 = []\n",
    "    # curv_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = lane_keeper(input)\n",
    "\n",
    "        regr_out = output\n",
    "        err2 = regr_out[:, 0]\n",
    "        err3 = regr_out[:, 1]\n",
    "        # curv_out = regr_out[:, 2]\n",
    "\n",
    "        err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 1]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        # curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "        loss = err_loss3 + err_loss2\n",
    "\n",
    "        err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "    return np.mean(err_losses2), np.mean(err_losses3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7/12 \n",
      "yaw_err_loss3: 0.0119,   Val: 0.0141\n",
      "lat_err_loss2: 0.0021,   Val: 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  3.97it/s]\n",
      " 65%|██████▌   | 15/23 [00:00<00:00, 203.54it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_217641/2277768394.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# if True:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0merr_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_loss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mval_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_217641/1434882309.py\u001b[0m in \u001b[0;36mval_epoch\u001b[0;34m(lane_keeper, val_dataloader, regr_loss_fn, device)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0merr_losses2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# curv_losses = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlane_keeper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.001 #0.005\n",
    "epochs = 12\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 1e-4 #9e-4\n",
    "L2_lambda = 1e-2 #1e-2\n",
    "optimizer = torch.optim.Adam(lane_keeper.parameters(), lr=lr, weight_decay=9e-5) #wd = 2e-3# 3e-5\n",
    "\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        err_loss2, err_loss3 = train_epoch(lane_keeper, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_loss2, val_loss3 = val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs} \\nyaw_err_loss3: {err_loss3:.4f},   Val: {val_loss3:.4f}\")\n",
    "    print(f\"lat_err_loss2: {err_loss2:.4f},   Val: {val_loss2:.4f}\")\n",
    "    # print(f\"curv_loss: {curv_loss}\")\n",
    "    torch.save(lane_keeper.state_dict(), model_name)\n",
    "\n",
    "#Note: sweet spot for training is around 0.016 -> 0.020, also note that training can get stuck, and loss can start improve randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 187.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lateral_err2_loss: 0.0021159208845347166\n",
      "yaw_err3_loss: 0.014104058966040611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "lane_keeper.load_state_dict(torch.load(model_name))\n",
    "err_loss2, err_loss3 = val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device)\n",
    "\n",
    "print(f\"lateral_err2_loss: {err_loss2}\")\n",
    "print(f\"yaw_err3_loss: {err_loss3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 5, 5)\n",
      "(4, 8, 5, 5)\n",
      "(4, 4, 7, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(lane_keeper.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 2, 4, 'conv0', size=(10,5))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LaneKeeper(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Conv2d(8, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(4, 4, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): BatchNorm1d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Linear(in_features=36, out_features=32, bias=True)\n",
       "    (2): Tanh()\n",
       "    (3): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "lane_keeper.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "lane_keeper.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "lane_keeper.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(lane_keeper, dummy_input, onnx_lane_keeper_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lane_keeper.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 6387.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[0.01934206 0.2978802 ]]\n",
      "Predictions shape: (1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_lane_keeper_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
