{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/lane_keeper_small.pt'\n",
    "onnx_lane_keeper_path = \"models/lane_keeper_small.onnx\"\n",
    "max_load = 150_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class LaneKeeper(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "            # nn.BatchNorm2d(8),\n",
    "            nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "            nn.Conv2d(4, 4, kernel_size=7, stride=1), #out = 6\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            #normalize\n",
    "            nn.BatchNorm1d(3*3*4),\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=3*3*4, out_features=32),\n",
    "            # nn.ReLU(True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=32, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "lane_keeper = LaneKeeper(out_dim=2,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "lane_keeper.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = lane_keeper(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from time import time, sleep\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "\n",
    "\n",
    "\n",
    "    #convert to gray\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    img = cv.resize(img, (4*SIZE[1], 4*SIZE[0]))\n",
    "\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "        axes_length = (randint(10//4, 50//4), randint(50//4, 300//4))\n",
    "        angle = randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (50,50))\n",
    "    noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = np.clip(light, 0, 51)\n",
    "    light *= 5\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    # cv.imshow('light', light)\n",
    "    # if cv.waitKey(0) == ord('q'):\n",
    "    #     break\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 5), randint(1, 5)))\n",
    "\n",
    "    # cut the top third of the image, let it 640x320\n",
    "    img = img[int(img.shape[0]/3):,:] ################################# /3\n",
    "    # assert img.shape == (320,640), f'img shape cut = {img.shape}'\n",
    "\n",
    "    #edges\n",
    "    img = cv.resize(img, (2*SIZE[1], 2*SIZE[0]))\n",
    "\n",
    "    #edges    \n",
    "    img = cv.Canny(img, 100, 200)\n",
    "\n",
    "    #blur\n",
    "    img = cv.blur(img, (3,3))\n",
    "\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "        #add random tilt\n",
    "    max_offset = 3\n",
    "    offset = randint(-max_offset, max_offset)\n",
    "    img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        img[:offset, :] = 0 #randint(0,255)\n",
    "    elif offset < 0:\n",
    "        img[offset:, :] = 0 # randint(0,255)\n",
    "\n",
    "\n",
    "    # #reduce contrast\n",
    "    # const = np.random.uniform(0.1,0.8)\n",
    "    # # if np.random.uniform() > .5:\n",
    "    # #     const = const*0.2\n",
    "    # img = 127*(1-const) + img*const\n",
    "    # img = img.astype(np.uint8)\n",
    "\n",
    "    # #add noise \n",
    "    # std = 60\n",
    "    # std = randint(1, std)\n",
    "    # noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    # img = cv.subtract(img, noisem)\n",
    "    # noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    # img = cv.add(img, noisep)\n",
    "\n",
    "    # #add random brightness\n",
    "    # max_brightness = 60\n",
    "    # brightness = randint(-max_brightness, max_brightness)\n",
    "    # if brightness > 0:\n",
    "    #     img = cv.add(img, brightness)\n",
    "    # elif brightness < 0:\n",
    "    #     img = cv.subtract(img, -brightness)\n",
    "\n",
    "    # #blur \n",
    "    # img = cv.blur(img, (randint(1,3),randint(1,3)))\n",
    "\n",
    "    # # invert color\n",
    "    # if np.random.uniform(0, 1) > 0.6:\n",
    "    #     img = cv.bitwise_not(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(50)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        #classification label for road ahead = 1,0,0,0,1,0,0,0,0,0,0,0,0,0,0\n",
    "        road_images_indexes = []\n",
    "        tot_lines = 0\n",
    "        with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            tot_lines = len(lines)\n",
    "            for i,line in enumerate(lines):\n",
    "                if line == '1,0,0,0,1,0,0,0,0,0,0,0,0,0,0':\n",
    "                    road_images_indexes.append(i)\n",
    "        print(f'total pure road images: {len(road_images_indexes)}')\n",
    "        road_imgs_mask = np.zeros(tot_lines, dtype=np.bool)\n",
    "        road_imgs_mask[road_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            # self.all_imgs = torch.zeros((2*max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8) #adding flipped img\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            # road images specifically are added again along with their flipped image and label\n",
    "            road_imgs = torch.zeros((2*len(road_images_indexes), SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "            road_labels = []\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "            road_idx = 0\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(max_load)):\n",
    "\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                sample = line.split(',')\n",
    "                #keep only info related to the lane, discard distance from stop line \n",
    "                sample = [sample[0], sample[1], sample[3]] #e2=lateral error, e3=yaw error point ahead, curvature\n",
    "                reg_label = np.array([float(s) for s in sample], dtype=np.float32)\n",
    "\n",
    "                #img \n",
    "                img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "\n",
    "                #check if its in the road images\n",
    "                if road_imgs_mask[i]:\n",
    "                    img_r = load_and_augment_img(img.copy())\n",
    "                    # cv.imshow('imgR', img_r)\n",
    "                    img_r = img_r[:,:,np.newaxis]\n",
    "\n",
    "                    img_l = cv.flip(img, 1)\n",
    "                    img_l = load_and_augment_img(img_l)\n",
    "                    # cv.imshow('imgL', img_l)\n",
    "                    img_l = img_l[:,:,np.newaxis]\n",
    "                    # cv.waitKey(1)\n",
    "\n",
    "                    road_imgs[2*road_idx] = torch.from_numpy(img_r)\n",
    "                    road_imgs[2*road_idx+1] = torch.from_numpy(img_l)\n",
    "                    road_labels.append(reg_label)\n",
    "                    road_labels.append(-reg_label)\n",
    "                    road_idx += 1\n",
    "\n",
    "                else:\n",
    "                    img = load_and_augment_img(img)\n",
    "                    if i < 100:\n",
    "                        cv.imshow('img', img)\n",
    "                        cv.waitKey(1)\n",
    "                        if i == 99:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(reg_label)\n",
    "                    all_img_idx += 1\n",
    "\n",
    "            #cut imgs to the right length\n",
    "            road_imgs = road_imgs[:2*road_idx]\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "\n",
    "            #concatenate all_imgs and road_imgs\n",
    "            print(f'road images: {road_imgs.shape}')\n",
    "            print(f'all images: {self.all_imgs.shape}')\n",
    "            self.all_imgs = torch.cat((self.all_imgs, road_imgs), dim=0)\n",
    "            print(f'self.data shape: {len(self.data)}')\n",
    "            print(f'road_labels shape = {len(road_labels)}')\n",
    "            self.data = np.concatenate((np.array(self.data), np.array(road_labels)), axis=0)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "            #free road_imgs from memory\n",
    "            del road_imgs\n",
    "            del road_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pure road images: 79427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142527/142527 [13:36<00:00, 174.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "road images: torch.Size([158854, 32, 32, 1])\n",
      "all images: torch.Size([63100, 32, 32, 1])\n",
      "self.data shape: 63100\n",
      "road_labels shape = 158854\n",
      "\n",
      "all imgs: torch.Size([221954, 32, 32, 1])\n",
      "data: (221954, 3)\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4096, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 1, 32, 32])\n",
      "torch.Size([4096, 3])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    err_losses2 = []\n",
    "    err_losses3 = []\n",
    "    # curv_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        err2 = output[:, 0]\n",
    "        err3 = output[:, 1]\n",
    "        # curv_out = output[:, 2]\n",
    "\n",
    "        err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 1]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        # Compute the losses\n",
    "        err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        # curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = err_loss3 + err_loss2 + L1_loss + L2_loss #+ curv_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    err_loss2 = np.mean(err_losses2)\n",
    "    err_loss3 = np.mean(err_losses3)\n",
    "    # curv_loss = np.mean(curv_losses)\n",
    "    return err_loss2, err_loss3\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device=device):\n",
    "    lane_keeper.eval()\n",
    "    err_losses3 = []\n",
    "    err_losses2 = []\n",
    "    # curv_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = lane_keeper(input)\n",
    "\n",
    "        regr_out = output\n",
    "        err2 = regr_out[:, 0]\n",
    "        err3 = regr_out[:, 1]\n",
    "        # curv_out = regr_out[:, 2]\n",
    "\n",
    "        err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 1]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        # curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "        loss = err_loss3 + err_loss2\n",
    "\n",
    "        err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "    return np.mean(err_losses2), np.mean(err_losses3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  27/50 \n",
      "yaw_err_loss3: 0.0121,   Val: 0.0122\n",
      "lat_err_loss2: 0.0016,   Val: 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 19/49 [00:05<00:08,  3.44it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_58038/4229017322.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# if True:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0merr_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_loss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mval_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_58038/1434882309.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Loop over the training batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Move the input and target data to the selected device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_58038/1181403174.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# img = img.float()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.001 #0.005\n",
    "epochs = 50\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 1e-4 #9e-4\n",
    "L2_lambda = 1e-2 #1e-2\n",
    "optimizer = torch.optim.Adam(lane_keeper.parameters(), lr=lr, weight_decay=9e-5) #wd = 2e-3# 3e-5\n",
    "\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        err_loss2, err_loss3 = train_epoch(lane_keeper, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_loss2, val_loss3 = val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs} \\nyaw_err_loss3: {err_loss3:.4f},   Val: {val_loss3:.4f}\")\n",
    "    print(f\"lat_err_loss2: {err_loss2:.4f},   Val: {val_loss2:.4f}\")\n",
    "    # print(f\"curv_loss: {curv_loss}\")\n",
    "    torch.save(lane_keeper.state_dict(), model_name)\n",
    "\n",
    "#Note: sweet spot for training is around 0.016 -> 0.020, also note that training can get stuck, and loss can start improve randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [00:00<00:00, 246.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lateral_err2_loss: 0.0015610808040946722\n",
      "yaw_err3_loss: 0.012218034826219082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "lane_keeper.load_state_dict(torch.load(model_name))\n",
    "err_loss2, err_loss3 = val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device)\n",
    "\n",
    "print(f\"lateral_err2_loss: {err_loss2}\")\n",
    "print(f\"yaw_err3_loss: {err_loss3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 5, 5)\n",
      "(4, 8, 5, 5)\n",
      "(4, 4, 7, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(lane_keeper.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 2, 4, 'conv0', size=(10,5))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LaneKeeper(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(8, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(4, 4, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): BatchNorm1d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Linear(in_features=36, out_features=32, bias=True)\n",
       "    (2): Tanh()\n",
       "    (3): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "lane_keeper.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "lane_keeper.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "lane_keeper.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(lane_keeper, dummy_input, onnx_lane_keeper_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lane_keeper.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 7041.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[-0.01265479  0.107762  ]]\n",
      "Predictions shape: (1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_lane_keeper_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
