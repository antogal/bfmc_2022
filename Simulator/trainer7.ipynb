{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/lane_keeper_small.pt'\n",
    "onnx_lane_keeper_path = \"models/lane_keeper_small.onnx\"\n",
    "max_load = 150_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE\n",
    "# #very good\n",
    "# class LaneKeeper(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             nn.Linear(in_features=4*4*4, out_features=16),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(in_features=16, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# lane_keeper = LaneKeeper(out_dim=2,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class LaneKeeper(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        prob = 0.2\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 4, kernel_size=5, stride=1), #out = 28\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=14\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(4, 4, kernel_size=5, stride=1), #out = 10\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=5\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(4, 32, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            #normalize\n",
    "            # nn.BatchNorm1d(3*3*4),\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=1*1*32, out_features=32),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(in_features=32, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "lane_keeper = LaneKeeper(out_dim=2,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "lane_keeper.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = lane_keeper(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from time import time, sleep\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "\n",
    "\n",
    "\n",
    "    #convert to gray\n",
    "    img = cv.resize(img, (4*SIZE[1], 4*SIZE[0]))\n",
    "\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "        axes_length = (randint(10//4, 50//4), randint(50//4, 300//4))\n",
    "        angle = randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (50,50))\n",
    "    noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = np.clip(light, 0, 51)\n",
    "    light *= 5\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    # cv.imshow('light', light)\n",
    "    # if cv.waitKey(0) == ord('q'):\n",
    "    #     break\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 5), randint(1, 5)))\n",
    "\n",
    "    # cut the top third of the image, let it 640x320\n",
    "    img = img[int(img.shape[0]/3):,:] ################################# /3\n",
    "    # assert img.shape == (320,640), f'img shape cut = {img.shape}'\n",
    "\n",
    "    #edges\n",
    "    img = cv.resize(img, (2*SIZE[1], 2*SIZE[0]))\n",
    "\n",
    "    #edges    \n",
    "    img = cv.Canny(img, 100, 200)\n",
    "\n",
    "    #blur\n",
    "    img = cv.blur(img, (3,3))\n",
    "\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "        #add random tilt\n",
    "    max_offset = 3\n",
    "    offset = randint(-max_offset, max_offset)\n",
    "    img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        img[:offset, :] = 0 #randint(0,255)\n",
    "    elif offset < 0:\n",
    "        img[offset:, :] = 0 # randint(0,255)\n",
    "\n",
    "\n",
    "    # #reduce contrast\n",
    "    # const = np.random.uniform(0.1,0.8)\n",
    "    # # if np.random.uniform() > .5:\n",
    "    # #     const = const*0.2\n",
    "    # img = 127*(1-const) + img*const\n",
    "    # img = img.astype(np.uint8)\n",
    "\n",
    "    # #add noise \n",
    "    # std = 60\n",
    "    # std = randint(1, std)\n",
    "    # noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    # img = cv.subtract(img, noisem)\n",
    "    # noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    # img = cv.add(img, noisep)\n",
    "\n",
    "    # #add random brightness\n",
    "    # max_brightness = 60\n",
    "    # brightness = randint(-max_brightness, max_brightness)\n",
    "    # if brightness > 0:\n",
    "    #     img = cv.add(img, brightness)\n",
    "    # elif brightness < 0:\n",
    "    #     img = cv.subtract(img, -brightness)\n",
    "\n",
    "    # #blur \n",
    "    # img = cv.blur(img, (randint(1,3),randint(1,3)))\n",
    "\n",
    "    # # invert color\n",
    "    # if np.random.uniform(0, 1) > 0.6:\n",
    "    #     img = cv.bitwise_not(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(50)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        #classification label for road ahead = 1,0,0,0,1,0,0,0,0,0,0,0,0,0,0\n",
    "        road_images_indexes = []\n",
    "        tot_lines = 0\n",
    "        with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            tot_lines = len(lines)\n",
    "            for i,line in enumerate(lines):\n",
    "                if line == '1,0,0,0,1,0,0,0,0,0,0,0,0,0,0':\n",
    "                    road_images_indexes.append(i)\n",
    "        print(f'total pure road images: {len(road_images_indexes)}')\n",
    "        road_imgs_mask = np.zeros(tot_lines, dtype=np.bool)\n",
    "        road_imgs_mask[road_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            # self.all_imgs = torch.zeros((2*max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8) #adding flipped img\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            # road images specifically are added again along with their flipped image and label\n",
    "            road_imgs = torch.zeros((2*len(road_images_indexes), SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "            road_labels = []\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "            road_idx = 0\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(max_load)):\n",
    "\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                sample = line.split(',')\n",
    "                #keep only info related to the lane, discard distance from stop line \n",
    "                sample = [sample[0], sample[1], sample[3]] #e2=lateral error, e3=yaw error point ahead, curvature\n",
    "                reg_label = np.array([float(s) for s in sample], dtype=np.float32)\n",
    "\n",
    "                #img \n",
    "                img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "\n",
    "                #check if its in the road images\n",
    "                if road_imgs_mask[i]:\n",
    "                    img_r = load_and_augment_img(img.copy())\n",
    "                    # cv.imshow('imgR', img_r)\n",
    "                    img_r = img_r[:,:,np.newaxis]\n",
    "\n",
    "                    img_l = cv.flip(img, 1)\n",
    "                    img_l = load_and_augment_img(img_l)\n",
    "                    # cv.imshow('imgL', img_l)\n",
    "                    img_l = img_l[:,:,np.newaxis]\n",
    "                    # cv.waitKey(1)\n",
    "\n",
    "                    road_imgs[2*road_idx] = torch.from_numpy(img_r)\n",
    "                    road_imgs[2*road_idx+1] = torch.from_numpy(img_l)\n",
    "                    road_labels.append(reg_label)\n",
    "                    road_labels.append(-reg_label)\n",
    "                    road_idx += 1\n",
    "\n",
    "                else:\n",
    "                    img = load_and_augment_img(img)\n",
    "                    if i < 100:\n",
    "                        cv.imshow('img', img)\n",
    "                        cv.waitKey(1)\n",
    "                        if i == 99:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(reg_label)\n",
    "                    all_img_idx += 1\n",
    "\n",
    "            #cut imgs to the right length\n",
    "            road_imgs = road_imgs[:2*road_idx]\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "\n",
    "            #concatenate all_imgs and road_imgs\n",
    "            print(f'road images: {road_imgs.shape}')\n",
    "            print(f'all images: {self.all_imgs.shape}')\n",
    "            self.all_imgs = torch.cat((self.all_imgs, road_imgs), dim=0)\n",
    "            print(f'self.data shape: {len(self.data)}')\n",
    "            print(f'road_labels shape = {len(road_labels)}')\n",
    "            self.data = np.concatenate((np.array(self.data), np.array(road_labels)), axis=0)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "            #free road_imgs from memory\n",
    "            del road_imgs\n",
    "            del road_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pure road images: 79427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [01:20<00:00, 185.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "road images: torch.Size([15812, 32, 32, 1])\n",
      "all images: torch.Size([7094, 32, 32, 1])\n",
      "self.data shape: 7094\n",
      "road_labels shape = 15812\n",
      "\n",
      "all imgs: torch.Size([22906, 32, 32, 1])\n",
      "data: (22906, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 1, 32, 32])\n",
      "torch.Size([8192, 3])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    err_losses2 = []\n",
    "    err_losses3 = []\n",
    "    # curv_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        err2 = output[:, 0]\n",
    "        err3 = output[:, 1]\n",
    "        # curv_out = output[:, 2]\n",
    "\n",
    "        err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 1]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        # Compute the losses\n",
    "        err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        # curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = err_loss3 + err_loss2 + L1_loss + L2_loss #+ curv_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    err_loss2 = np.mean(err_losses2)\n",
    "    err_loss3 = np.mean(err_losses3)\n",
    "    # curv_loss = np.mean(curv_losses)\n",
    "    return err_loss2, err_loss3\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device=device):\n",
    "    lane_keeper.eval()\n",
    "    err_losses3 = []\n",
    "    err_losses2 = []\n",
    "    # curv_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = lane_keeper(input)\n",
    "\n",
    "        regr_out = output\n",
    "        err2 = regr_out[:, 0]\n",
    "        err3 = regr_out[:, 1]\n",
    "        # curv_out = regr_out[:, 2]\n",
    "\n",
    "        err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 1]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        # curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "        loss = err_loss3 + err_loss2\n",
    "\n",
    "        err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "    return np.mean(err_losses2), np.mean(err_losses3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  100/100 \n",
      "yaw_err_loss3: 0.0699,   Val: 0.0684\n",
      "lat_err_loss2: 0.0286,   Val: 0.0285\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.001 #0.005\n",
    "epochs = 30\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 1e-4 #9e-4\n",
    "L2_lambda = 2e-2 #1e-2\n",
    "optimizer = torch.optim.Adam(lane_keeper.parameters(), lr=lr, weight_decay=9e-5) #wd = 2e-3# 3e-5\n",
    "regr_loss_fn1 = nn.MSELoss() #before epochs/2\n",
    "regr_loss_fn2 = nn.L1Loss() #after epochs/2 for finetuning\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        regr_loss_fn = regr_loss_fn1 if epoch < epochs//2 else regr_loss_fn2\n",
    "\n",
    "        err_loss2, err_loss3 = train_epoch(lane_keeper, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_loss2, val_loss3 = val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs},  loss = {regr_loss_fn} \\nyaw_err_loss3: {err_loss3:.4f},   Val: {val_loss3:.4f}\")\n",
    "    print(f\"lat_err_loss2: {err_loss2:.4f},   Val: {val_loss2:.4f}\")\n",
    "    # print(f\"curv_loss: {curv_loss}\")\n",
    "    torch.save(lane_keeper.state_dict(), model_name)\n",
    "\n",
    "#Note: sweet spot for training is around 0.016 -> 0.020, also note that training can get stuck, and loss can start improve randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 213.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lateral_err2_loss: 0.028536437079310417\n",
      "yaw_err3_loss: 0.06839476525783539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "lane_keeper.load_state_dict(torch.load(model_name))\n",
    "err_loss2, err_loss3 = val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device)\n",
    "\n",
    "print(f\"lateral_err2_loss: {err_loss2}\")\n",
    "print(f\"yaw_err3_loss: {err_loss3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 5, 5)\n",
      "(4, 4, 5, 5)\n",
      "(32, 4, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdAAAACHCAYAAACmoQj7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAIfElEQVR4nO3dX2jV9xnH8ffjYrKqU7GYuGq1tnO22DIGZTD2B0QvehOKOrB2uA1ayhjsYhRLZTpwVuvVemMFEUQ2ph11dnWDwnoTXMuod4XWi9FtjQ51Omui1QSn+e4ikYrMP3l8Vtv1/QJBc375nHM85/DOL4GcaK0hSZLGZ8LtvgGSJH0aGVBJkhIMqCRJCQZUkqQEAypJUoIBlSQpwYBKkpRgQKVPoYh4PCL6I+JcRPwuImbc7tskfdYYUOlTJiIWAduB1UAPcB7YdltvlPQZZEClAhFxd0Tsi4iTEXEqIrZGxISIWDd2pngiIn4ZEdPGjr8nIlpEfD8iDkfEvyLip2OX3RURQ1eeVUbEV8eOmQh8F/h9a+1Aa+1DYD2wPCK+cDvuu/RZZUClWxQRnwP+APQD9wCzgZeAH4z9WQzcC0wBtl716d8EFgJLgJ9FxAOttaPAn4EVVxz3OLC3tfZvYBHw9uULWmt/BS4AX669Z5Kux4BKt+5rwF3AmtbaudbacGvtDUbPFH/RWvvb2JniWuCxiOi44nM3tNaGWmtvMxrFr4x9fDewCiAiAnhs7GMwGuLBq27DIOAZqPQxMqDSrbsb6G+tXbzq43cxelZ6WT/QwejPLS87fsXfzzMaR4DfAl+PiC8C3wZGgD+NXfYhMPWq65oKnM3eAUnj13HjQyTdwBFgbkR0XBXRo8C8K/49F7gI/BOYc73B1trpiPgjsBJ4AHipffTWSe/y0ZkqEXEv0AX85VbviKSb5xmodOsOAseALRExOSI+HxHfAPYAP4mI+RExBdgM/Oa/nKley27ge8B3+OjbtwC/Bnoj4lsRMRn4ObCvteYZqPQxMqDSLWqtXQJ6gS8Bh4F/MHrmuBP4FXAA+DswDPx4HNP7gQXA8bGfkV6+vneBHzIa0hOM/uzzR7d8RySNS/iG2pIkjZ9noJIkJRhQSZISDKgkSQkGVJKkBAMqSVKCAZUkKcGASpKUYEAlSUowoJIkJYzrl8lPnz69zZo1q+zKL1y4ULb1wQcflG0B3HnnnWVbEydOLNs6fvw4AwMDUbE1bdq01t3dXTEFwNDQUNlW5XMDYOrUq9+85JPhxIkTnDlzpuTxBIiI0l8tdv/995dtTZhQ+/X68PBw2dalS5fKtk6dOsXZs2fLXqM9PT03PvAmHTt2rGxr4cKFZVsA77zzTtnW7Nmzy7ZOnjx5zdfouAI6a9YsduzYUXOrgCNHjpRt7dmzp2wLYPXq1WVblV90PPXUU2Vb3d3dvPDCC2V7lS+A/v7+Gx80Do888kjpXpWnn376dt+E69q5c2fZ1qRJk8q2AN57772yrYGBgbKtjRs3lm319PSwdevV78Get3nz5rKtvr6+si2ABQsWlG1t2bKlbOuZZ5655mV+C1eSpAQDKklSggGVJCnBgEqSlGBAJUlKMKCSJCUYUEmSEgyoJEkJBlSSpAQDKklSggGVJCnBgEqSlGBAJUlKMKCSJCUYUEmSEgyoJEkJBlSSpISO8Rx8+vRp9u7dW3blS5cuLdt66623yrYA1q5dW7Y1ODhYtnXp0qWyrZGREc6fP1+29+yzz5ZtrVmzpmwLoK+vr2yrp6enbGt4eLhsC2DmzJmsWLGibK/yuXvw4MGyLYD58+eXbVXetnPnzpVtHTt2jE2bNpXtvfjii2VbEVG2BXDo0KGyrQcffLBsa2Rk5JqXeQYqSVKCAZUkKcGASpKUYEAlSUowoJIkJRhQSZISDKgkSQkGVJKkBAMqSVKCAZUkKcGASpKUYEAlSUowoJIkJRhQSZISDKgkSQkGVJKkBAMqSVKCAZUkKaFjPAd3dXVx3333lV354sWLy7aee+65si2AXbt2lW1t2rSpbGvy5MllW52dncydO7dsb/fu3WVb1Y/n888/X7Y1b968sq3Ozs6yLYBJkybx8MMPl+0tWbKkbOvMmTNlWwDvv/9+2db+/fvLtgYGBsq27rjjDhYtWlS29+abb5ZtLVu2rGwLoK+vr2xr3bp1ZVs7duy45mWegUqSlGBAJUlKMKCSJCUYUEmSEgyoJEkJBlSSpAQDKklSggGVJCnBgEqSlGBAJUlKMKCSJCUYUEmSEgyoJEkJBlSSpAQDKklSggGVJCnBgEqSlGBAJUlKMKCSJCV0jOfgrq4u5s+fX3blhw4dKtvq7u4u2wJ49dVXy7Y2bNhQttVaK9saGhoqfQweeuihsq2XX365bAtg6dKlZVuvv/562dbw8HDZFsDFixc5depU2d7evXvLtlatWlW2BbB8+fKyrcrn25NPPlm2NWPGjNL/t0cffbRs68CBA2VbAL29vWVb+/btK9u63nPDM1BJkhIMqCRJCQZUkqQEAypJUoIBlSQpwYBKkpRgQCVJSjCgkiQlGFBJkhIMqCRJCQZUkqQEAypJUoIBlSQpwYBKkpRgQCVJSjCgkiQlGFBJkhIMqCRJCR3jOTgi6OzsLLvyXbt2lW1t27atbAtg2bJlZVtDQ0NlWyMjI2Vb/f39PPHEE2V7r7zyStnWwoULy7YAJkyo+1pxzpw5ZVuVryeA6dOn09vbW7Y3ZcqUsq2VK1eWbQEcPny4bGvBggVlW11dXWVbR48eZePGjWV7a9euLdtav3592RbA9u3by7Zee+21sq3BwcFrXuYZqCRJCQZUkqQEAypJUoIBlSQpwYBKkpRgQCVJSjCgkiQlGFBJkhIMqCRJCQZUkqQEAypJUoIBlSQpwYBKkpRgQCVJSjCgkiQlGFBJkhIMqCRJCQZUkqQEAypJUkK01m7+4IiTQP//7uboJsxrrc2sGPLx/EQoezzBx/QTwtfo/5drPp7jCqgkSRrlt3AlSUowoJIkJRhQSZISDKgkSQkGVJKkBAMqSVKCAZUkKcGASpKUYEAlSUr4DwMDwZdoGmfyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 576x144 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj4AAAJ5CAYAAACubzp4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAA9kklEQVR4nO3deZTddX3/8fd3tntn7txZM8lknZBAFpKYiAES0hjWxIIgGlTE4nbcqo2ttGqlLe2hBWvpsUdKUVvAAhUQfyiLEowsBoIJWTBACCGZZDIJZJvJ7EvubN/fHyGn+flzpu/XVweln+fjnJ5T5ZVP3vPJXV7ceO47iuPYAAAAQpD3ux4AAADgzULxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHwO9cFEWXRFG0LoqitiiKDkVRdFsURdnf9VwA/veh+AD4fVBuZv9gZhPMbLaZTTSzm36nEwH4X4niA+DXiqJochRFP4yiqCmKoqNRFN0SRVFeFEV/HUVRYxRFR6IouiuKovI38lOjKIqjKPpoFEX7oihqjqLor974ZxOiKOqNoqjqpPPf/kamMI7je+I4fiyO4544jlvN7D/MbMnv5icH8L8ZxQfA/yeKonwz+7GZNZrZVDv+Ccx9ZvaxN/7vPDObZmalZnbLr/zyPzCzmWZ2gZldF0XR7DiOD5jZejNbeVLuKjP7P3Ec9/+aEd5pZi//dn4aAPhvEbu6APyqKIoWm9nDZjY+juOBk/77J8zsgTiOb33jP880s21mVmxmk8yswcwmx3H82hv/fKOZfSOO4/uiKPqkmV0Vx/H5URRFZrbPzD4cx/HTv/J7X2Rm95vZ2XEc7xztnxVAWPjEB8CvM9nMGk8uPW+YYMc/BTqh0cwKzGzcSf/doZP+/x47/qmQmdkDZrY4iqLxdvwTnSEze+bkw6MoWmRm95jZFZQeAKOh4Hc9AIDfS/vNbEoURQW/Un4OmFndSf95ipkNmNlhO/6Jz7DiOG6NomiNmX3Qjv8PmO+LT/rIOYqit9vxT5k+EcfxE7+dHwMA/l984gPg19loZgfN7B+jKMpEUZSOomiJmd1rZl+MouiUKIpKzexGM/v+r/lkaDj3mNlHzOyKN/5/MzOLomiumT1mZqviOH7kt/mDAMDJKD4A/j9xHA+a2aVmdqod/9/ivGbHP6m5w8zuNrOn7fj/nueYma0Sjn7YzE4zs0NxHL9w0n//52ZWY2a3R1HU9cb/8T9uBvBbx/+4GQAABINPfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEo0AJFxYWxqlUyp0vKipyZ/v7+5VRrKyszJ1taWmRzj527FhzHMc1yq+prq6O6+rq3PmBgQF39uDBg8oolsvl3NlsNiudfeDAAfluoiiKoyhy55U/W/VxU1JS4s42NzdLZ5uZfDepVCrOZDLufEdHhztbXl6ujGLjxo1zZ5V7NDPbsmWLfDdmx++nuLjYnVceZ+rPoDxnjxw5Ip0dx7F/8DcUFhbGymtsX1+fcrY0yymnnOLOKq9PZma7d++WHztFRUVxOp1255XHmPq4aW1tdWfjOJbO7ujoSPR6LObdWeW128wsL8//2Ut3d7d0dl9f37B3IxWfVCpl8+fPd+cnTpzozqovFBdccIE7e//990tnb9u2rVH6BWZWV1dna9eudeeVJ8M//MM/SLPs3bvXnV26dKl09nXXXSffTRRFUgn+gz/4A3dWLYVnnHGGO3vbbbdJZ5uZfDeZTMaWL1/uzj/22GPu7IoVK6RZ/vzP/9ydfcc73iGdHUWRfDdmx9+Qzj33XHe+oMD/krZgwQJpFqUIf/Ob35TOTqKoqMjmzp3rzjc2+v8Ixo8fL81y9913u7M7d+6Uzl65cqX82Emn07Zw4UJ3/m1ve5s7qz72H3jgAXdWKadmZqtXr070vFIoJXjZsmXS2UqJXL9+vXR24wgPeP6qCwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCIa2sGBgYkFZLzJw50509cOCAMopt2rTJnf3EJz4hnX3NNddI+RPy8/Pd2Z/+9KfurPpV3UuWLHFn1ZUPSRQUFNjYsWPd+V27drmzbW1t0izKV6orX2NvZvbiiy9KebPjzyllFUJ7e7s7O2HCBGmWL33pS+6sukYlqVQqZcoOvJ6enlHJmplddtll7uztt98+anOckJeXZ8o+qsOHD7uzytoYM23XlbomJ4lcLme7d+9255XHmLrDT1n/8e1vf1s6O4lsNmtnnXWWO9/V1eXOKu9rZmbvfve73dnrr79eOvujH/3osP+MT3wAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBjSyoo4jm1gYMCd/+UvfykPNBrOO++8Uf898vLyrKSkxJ1vaGgYtVmy2aw7m2TNgkq9mziO3dmOjg5pFmXFRW1trXR2krvMz8+X/rxKS0vd2cbGRmmWjRs3urP33XefdHZSeXl5lslk3Pnq6mp3duvWrdIsF198sTurrGV49tlnpTlO6O/vt0OHDrnzBQX+l/vly5dLs0yfPt2dfe2116Szk8jPz7fKykp3XnmuKKs/zLT1H2PGjJHOVtdnmB1/3Lz++uvuvLIWRXmMmZlt2bLFnb3tttuks0fCJz4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACIa0WEPdudTa2urOqjtKpkyZ4s4uWLBAOjuJnp4eae/IM888486q86dSKXdW3UeVRBRFlp+f786XlZW5szU1NdIs06ZNc2dnzpwpnb1mzRopb3Z8t43y2H/HO97hzqp7fCZPnuzOKo+x30RFRYW95z3vced/9rOfubNTp06VZnnsscfc2UcffdSdXbhwoTTHCf39/dLeK+X3ef/73y/N0tTU5M5+/etfl85OYmhoyI4dOzYqZyu7q8zMrr32Wnd2z5490tm33367lDfTd26OGzfOna2vr5dm6enpcWf3798vnT0SPvEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGCM6sqK7u5ud1ZZ92Bm1t7e7s62tbVJZyfR09Njzz//vDu/bt06d/a0006TZslkMlJ+tKXTaZsxY4Y739LS4s4qX9lvpt3NF7/4Rensv/zLv5TyZse/Wj+Xy7nzysoNZWWMmUlf8d/V1SWdndTAwIAdOXLEnT9w4IA7q64eUNYJ7Nixw51NulqhsLBQWjPyrne9y51VXufNzDZu3OjOKq99SVVXV9uHP/xhd155Xfj7v/97aZZ7773Xnb3pppuks5OsrMjPz7eKigp3Xlndk81mpVmU17558+ZJZ4+ET3wAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEIwojmN/OIqazKxx9Mb5vVEXx3GN8gu4m+FxN8PjbkYWyP1wNyPjeTU87mZ4w96NVHwAAADeyvirLgAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQjAIlXFxcHGez2VEZpLCwUMp3d3e7s8XFxdLZhw4dala/BjyTycQVFRXufEdHhzs7btw4ZRQrKipyZ/v6+qSzd+/eLd9NZWVlPGHCBHde+bNVv3m8tbXVna2qqpLObmxslO+muLg4Li8vd+dravzH9/b2KqNYS0uLO5vL5aSze3p65LsxM6uoqIhra2vdeeXxcPToUWkWJZ9Op93Z/v5+GxgYiKRhzKysrCxWHg/Kn1llZaU0SyqVcmeV57eZ2Y4dO+THjvq4UV4X2tvblVGke1ffq3p7e+W7KSoqiktKStz5U045xZ3Nz89XRrG2tjZ3tqBAqiv26quvDns30knZbNauuOIK6Tf3mjRpkpR/7rnn3NnTTz9dOvsf//Ef5T0mFRUV9vnPf96d/9nPfubO/umf/qk0S11dnTvb0NAgnb1y5Ur5biZMmGD33HOPO79x40Z3dmhoSJrlgQcecGc/8IEPSGd/6lOfku+mvLzcrr76anf+M5/5jDv70ksvSbPce++97qz6uNm8eXOi3UC1tbV2xx13uPNK2bvrrrukWZT89OnT3dndu3dLc5xQU1NjN954ozvf2Oj/I3j/+98vzaK8Oa5fv146+5xzzpEfO7W1tfbv//7v7vwPf/hDd/YnP/mJNEt9fb07O3PmTOnsrVu3yndTUlJiS5cudefvvvtud1b5l38zswcffNCdHTNmjHT20qVLh70b/qoLAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIIhrawoLS21c845x51XvgZc3dU1Z84cd3bLli3S2Uk0Nzfbd77zHXf+kksucWePHTsmzfLyyy+7s3l5o999oyiS9ocpu8nGjx8vzbJ582Z39oUXXpDOTqKyslJajaHsiyotLZVmUb4u/8knn5TOTqqgoED6qnrlua6slTAzW758uTu7c+dOd1Zdu3LCnj177Morr3Tn165d687+/Oc/l2bp7+93Z/fv3y+dnZTy2qbs9Zo8ebI0h7JCaNOmTdLZSVRWVkorSZ544gl3tqurS5plcHDQnf1tPm74xAcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgiGtrMhkMrZkyRJ3Xvk66oceekgZxWbNmuXO5nI56ewk+vr6bN++fe78+eef784qX39vZrZt2zZ3VlmXkFQ6nbbZs2e784899pg7W1VVJc1SU1PjztbX10tnJ1FcXCzdzYYNG9zZiy66SJqltbXVnf3Qhz4knX3LLbdI+ROOHDliN998szt/xRVXuLOZTEaaZcqUKe7s/fff7862tLRIc5xQWFhoY8eOdecXLlzozu7Zs0ea5dFHH3VnU6mUdHZSyioQZb2L8h5oZjYwMODOdnd3S2crrwcni6LInX3uuefcWeUxZmb2yiuvuLPq83UkfOIDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBIu7q6urps3bp17vz8+fPd2S1btiijWENDgzublzf6/a6kpMROP/10dz6OY3d2165d0izKHHPmzJHOTqK/v98OHjzozp966qnu7M9//nNplqeeesqdfTPu5siRI9IeqzFjxriz27dvl2bZvXu3O3v06FHp7KRyuZy0N2rTpk3u7LnnnivNojyvstmsO/vqq69Kc5xQUFAg7Z57+umn3Vllh5KZ2bhx49zZZcuWSWcnMTg4aB0dHe78WWed5c62tbVJsyi72CZOnCidnWRX18DAgB0+fNidV96rXnrpJWkW5fV7/Pjx0tkj4RMfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAiGvLLi2Wefdeebmprc2ZtvvlkZRVpZMW3aNOnsJMrKymzFihXu/IIFC9zZ5557TpqlsLDQnT3ttNOks5NQV52Ul5e7s8o9mpl997vfdWeTfB28Kj8/3yoqKtz5AwcOuLPK48DMrLOz05299957pbOT6uvrs8bGRnd+7dq17uz5558vzaKsUqmqqnJnlfUWJ+vr67P9+/e78w888IA7e9lll0mzKKtg1qxZI52dRHFxsc2dO9edV9ZQLF26VJpFOVtdy7Bq1Sopb2aWSqVsxowZ7vxovT6ZmfS+oDz//id84gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYERxHPvDUdRkZv7FOW9ddXEc1yi/gLsZHnczPO5mZIHcD3czMp5Xw+Nuhjfs3UjFBwAA4K2Mv+oCAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMEokMIFBXFRUZE7r2RVURS5swUF0o9pzc3NzerXgJeWlsZVVVXufCaTcWf7+/uVUWzfvn3ubF6e1n1zuZx8N1EUSV8PXlPjP155HJhpP29HR4d0dk9Pj3w3FRUVcW1trTs/ODjozvb29iqjWHFxsTurPre3b98u342Z/tiprKx0Z9WfQXnOtrS0uLM9PT2Wy+W0B7KZpVKpuKSkxJ0fGBhwZ9XnVTqddmeVPyMzs507d8qPnaqqqnjixInuvLLBQLlHM7OhoSF3Vnl+m5nt2bNHvpvCwsJY+fOaMmWKO6vOr9zl7t27pbPNbNi7kRpBUVGRzZgxw52fOnWqO6uuzsjPz3dnlUJiZnb77bfLe0yqqqrsL/7iL9z5RYsWubOvvfaaNMsXvvAFd1Z5szMzq6+vH/UdLytXrnRn1Tcv5Y3i8ccfl87evHmzfDe1tbV2xx13uPNHjx51Z1955RVplnnz5rmzypuKmdn8+fPflN1AF110kTurvKCbmZ155pnu7H333efOPvXUU9IcJ5SUlNi5557rzjc3N7uzqVRKmmX27Nnu7Hvf+17p7AsuuEB+7EycONEefPBBd155Az58+LA0y7Fjx9zZzs5O6ewrrrhCvpt0Om0LFixw57/1rW+5s0rhN9Nez973vvdJZ9sI+8j4qy4AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACIa0siKTydjZZ5/tzis7WRoaGpRR7MCBA+6sshLAzOz222+X8mbH97F0d3e788pXwitfeW6mrRPo6emRzk4ilUpZXV2dO79nz55Rm+W8885zZ1esWCGdvXnzZnUcM9N2+SiP+9LSUmmORx991J097bTTpLOTKi4uln4vZa2EuoutrKzMnd2+fbs7q+5UO6G7u9s2btzozs+ZMyfR7+Nx8cUXu7OFhYWjNscJqVTKpk+f7s7v2LHDnVV3dSlrdb7yla9IZycxZcoUaQ3F3Llz3Vn1brZs2eLOKitIzMwuv/zyYf8Zn/gAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDCklRV9fX22f/9+d37q1Knu7P3336+MYgsWLHBny8vLpbOT6OrqsvXr17vzX/3qV93Z6667TppF+Yr/J554Qjo7iWw2a+985zvd+fr6end2w4YN0ixjx451Zz/ykY9IZ99www1S3uz4SgblK+Ffeukld1ZZi2Jmtm/fPnd269at0tlJjRkzxj71qU+588pX5hcUSC9/1tLS4s5eeuml7uzdd98tzXFCf3+/tMKkpqbGnb3wwgulWRYuXOjOvvjii9LZScRxbP39/e688niOokiapbq62p3dvXu3dHYSqVRKem9ua2tzZ/Pz86VZnnnmGXdWfT0eCZ/4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAY0rIadTdMT0+PO6vskTEzO3z4sDur7lZJorS01BYvXuzO//jHP3Znf/GLX0izKHvAli1bJp396U9/WsqbmaXTaZs1a5Y7f+jQIXd2ypQp0izKbq//+I//kM5OIj8/3yoqKtz5bDbrznZ1dUmzKLuNlNeB30R5ebldcskl7ryyL095TJqZPfroo+5sWVmZO5v09amkpMRmz57tzr/tbW9zZ6+44gppFuXnVXYJJtXX12eNjY3u/K5du9xZ9fW4uLjYnVXfB5uamqS8mVl3d7c999xz7vzMmTPd2XQ6Lc3yr//6r+7sRRddJJ09Ej7xAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgSCsrBgYG7OjRo+78j370I3dW/dr2XC7nzr797W+Xzk6isrLSPvjBD7rzf/VXf+XONjc3S7NMmzbNnZ0+fbp0dhJFRUU2efJkd37FihXurPI4MDPbs2ePO6t+/XoScRxbX1+fO6983X9DQ4M0i7I6Q7nH39TAwIA7qzz2lZU6ZmY333yzO/vFL37RnR0aGpLmOPnXdXd3u/NnnHGGO7to0SJplmuvvdadvfTSS6Wzk8jlclZfX+/OFxT43wr/7M/+TJpl06ZN7qyySiKpvr4+279/vztfVFTkzi5ZskSa5Utf+pI7e8MNN0hnj4RPfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQjCiOY384iprMrHH0xvm9URfHcY3yC7ib4XE3w+NuRhbI/XA3I+N5NTzuZnjD3o1UfAAAAN7K+KsuAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAhGgRJOp9NxJpNx548dO+bORlGkjGK5XM6dLSwslM7u7e1tTrD/RNr9odxjaWmpcrTl5fn7bE9Pj3R2e3u7fDdVVVXxpEmTlN/Dne3s7FRGsaKiInc2m81KZ9fX18t3U1xcHCu/T0lJiTvb39+vjCLnFU1NTfLdmJkVFhbGqVTKnVeeVyrleajcZUtLi3V1dWkvgMfniaurq935mhr/9R86dEia5fXXX3dna2trpbMPHTokP3ZSqZT0XlVZWenOqu9V5eXl7mxra6t0dkNDw6i/Hh88eNCdVd7zzbT3KuW128ysubl52LuRik8mk7E//MM/dOd37drlzhYUSKPYzp073dkJEyZIZ7/44oujvsBt7ty57uyyZcuks5UHyEsvvSSd/dBDD8l3M2nSJHvkkUfc+dWrV7uzTz75pDTLlClT3Fn13i+77DL5brLZrK1cudKdP/PMM91Z9c3rwIED7qz64n/LLbckek6lUimbN2+eO79o0SJ3Vt1TuHjxYnf2yJEj7uxNN90kzXFCdXW1XXvtte78Zz7zGXf2a1/7mjSLMsfHP/5x6eyvfe1r8mMnk8nYihUr3Pn3ve997qz6L9IXX3yxO/vAAw9IZ1911VWJXo9/8pOfuPPXX3+9O7tjxw5pFuVf+pTXbjOz73znO8PeDX/VBQAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBkPZE9PX1STtZlK/2bmtrU0aR9r10dHRIZydRXl5u5557rjt/yimnuLPKXhUzs+bmZne2r69POjuJ7u5u27hxozv/6quvurMvvviiNMsnPvEJd/bnP/+5dHYSURSZsotKeSwfPnxYmkXZV3TeeedJZ99yyy1S/oR0Om0zZsxw5/fv3+/Odnd3S7Moe4W+8Y1vuLN33nmnNMcJJSUltmDBAndeeTwoO8DMtNcodT1REkVFRTZx4kR3vqmpyZ1Vd40pq2De9a53SWcn0dPTY5s2bXLnBwcH3dlp06ZJsygrLtTX+pHwiQ8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABEP67vDy8nJbvny5O79+/Xp3Vv16/ZaWFnc2Pz9fOjuJ6upqu/rqq935xsZGd3bcuHHSLA0NDe7s/PnzpbNXr14t5c3Mjh49anfffbc739XV5c4q6y3MtHUeF154oXT217/+dSlvZpbL5WzPnj3u/JEjR9zZqVOnSrPU1NS4s8XFxdLZSQ0ODkprOtLptDsbRZE0y7Zt29zZH//4x+5se3u7NMcJcRxL6wQ6Ozvd2aefflqa5YorrnBn34zX40wmY2eddZY7PzQ05M6qKze2bNnizq5cuVI6O4n8/HxpPc2ECRPcWXUNzOTJk91ZZe3R/4RPfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDGnpyLhx4+yaa65x56+88kp3trS0VBnFDhw44M5WVFRIZydRUFBg1dXV7nxra6s729fXJ83S29vrzip7kJIqKCiwMWPGuPOTJk1yZ5XHgZnZ9OnT3Vl1R1oS3d3d0g6aq666yp2dNWuWNEs2m3Vn1X1FSR07dszq6+vd+QULFrizym4yM20P0a233urOKvvXThZFkeXl+f/d9cknn3Rn58yZk2Qkl8LCwlE7+2TKTrA4jt1Z5c7NtJ2VdXV10tlJZLNZO++889z5F1980Z1tamqSZlHeM2fOnCmdPdIeRz7xAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgSN87H0WRpVIpKe+VTqeVUaSvGFfXGiSRzWbt3HPPdee3b9/uzg4NDUmzKOshtmzZIp2dhLrO48ILL3RnFy5cKM3S3Nzszi5evFg6O4mBgQE7fPiwO3/WWWe5s2PHjpVmUb7i/8wzz5TOTiqVSklf46+8LvT09Eiz7N69253dunWrdHYSeXl50qqfhoYGd1ZdSZLJZNzZY8eOSWcnEcextLpHWUOhrvk5dOiQO/tmPK8GBwetvb3dnVf+vNQ/W+VxU1tbK53NygoAAACj+AAAgIBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCJlt00URU1m1jh64/zeqIvjuEb5BdzN8Lib4XE3IwvkfribkfG8Gh53M7xh70YqPgAAAG9l/FUXAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAhGgRJOp9NxJpNR8u5sKpVSRrGjR4+6swUF0o9pLS0tzerXgFdWVsYTJkxw59vb26WZFENDQ6N29sGDB+W7KSwsjIuKitz56upqd1Z5HJiZRVHkzg4ODkpnHzt2TL6b/Pz8uLCw0J1Xvmm9pKREGUV6vhYXF0tnNzQ0yHdjZpaXlxfn5fn//ay8vNydVX5eM+01qqGhQTo7jmP/A/MN6XQ6zmaz7rzy2FdfQ0bztX7Pnj3yY6esrCweN26cO5+fn+/Oqq8Lyl0eOXJEOrurq0u+m1QqFZeWlrrzymNM3QShnN3YqG3ZGOlupEaQyWTs4osvdudnzpzpzk6dOlUZxe655x53tqKiQjr73nvvlfeYTJgwQZppzZo17qz6ItTb2+vOKi+GZmZ/93d/J99NUVGRzZ07153/8Ic/7M7efffd0ixKCW5ra5PO3rFjh3w3hYWFNmnSJHd+YGDAnT3jjDOkWZTn6/z586WzP/ShDyXaDZSXlye9OK5YscKdPf3006VZlNeoq6++Wjo7iWw2a+95z3vceeVfPnp6eqRZZs2a5c6ecsop0tlXXnml/NgZN26cfeMb33DnlcKs/ktrLpdzZ7/5zW9KZ69bt06+m9LSUul5cv7557uzfX190iznnnuuO/u5z31OOnvt2rXD3g1/1QUAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwZBWVrS0tNh//dd/ufPK128vXbpUGcW2bNnizm7YsEE6O4ni4mLpa/wfeeQRd1ZdWaF83byyoyapgYEBO3TokDvf3d3tzqp72Gpq/GttXn31VensJHK5nO3evdudX7JkiTur7AAzM2ttbXVnZ8yYIZ2dVCqVslNPPdWdX7VqlTurfr1+ZWWlO3v22We7s9u2bZPmOCGXy9nevXvdeeXP7LXXXpNmmTdvnjurPi6TKCwstMmTJ7vzCxYscGd/8IMfSLMor7GbN2+Wzk6ivLxcWj2lrL554YUXpFmU9wVldYaZ2dq1a4f9Z3ziAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBkL7vf/z48fbJT37SnW9oaHBnf/SjHymj2Jw5c9zZqqoq6ewkKy4GBwetra3Nne/v73dne3t7pVk2bdrkznZ0dEhnJ9HX12f79u1z53/605+6sy+//LI0y7XXXuvOKmtRzLSVDydEUWRFRUXufHV1tTtbXFwszVJeXj4q2d9EaWmptM4ml8tJZyuUs5U1G7t27ZLmOKG/v98OHz7sziurYFKplDTLpEmT3FllvUVSeXl50vNK+TM4evSoNIuyGmVgYEA6O4lMJmOLFi1y55XH8sMPPyzNsmbNGnd2/Pjx0tkj4RMfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAARD2tU1YcIEu/766935Sy+91J3t7OxURrEzzjjDnZ0+fbp0dhItLS123333ufP/9m//5s4uXrxYmmXJkiXu7BNPPCGd/Wbo6upyZydOnCid/b73vc+dffbZZ6WzDxw4IOXNzAoLC6UdNMquLnWflpJ/M55TZmaTJ0+2m266yZ3fvHmzO5tOp6VZnn/+eXd22bJl7uy6deukOU4YGhqSXjeV3VVjx46VZpk/f747+9vcuTScvLw8aRebst/w61//ujSLsivyzdjV1d3dLf28lZWV7uy0adOkWb761a+6s3feead09kj4xAcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgiGtrOjr67P9+/e78xUVFe7sCy+8oIwifd38ddddJ52dxODgoLW3t7vzR48edWcnT54szaJ8xfjs2bOlszds2CDlzY6vZVC+Al+5m5aWFmkW5WvsH3nkEensKIqkvNnxr9ZXHsvKn1dzc7M0S2FhoTvb2toqnf2byM/Pd2eVP4O9e/dKc1x22WXu7Hve8x53Vnm8n6yvr88aGxvd+QULFrizU6ZMkWZR1lAoz8Gk+vv77eDBg+688lz57Gc/K83y4IMPSvnRVlxcbHPmzHHnlef6hRdeKM1yzz33uLPPPPOMdPZI+MQHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGI4jj2h6Ooycz8y2HeuuriOK5RfgF3MzzuZnjczcgCuR/uZmQ8r4bH3Qxv2LuRig8AAMBbGX/VBQAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBKFDCmUwmrqqqcufz8/Pd2b6+PmUUKy4udmcrKyuls7ds2dKs7j8pKiqK0+m0Oz8wMODO1tbWKqNYU1OTO6uuLOnu7pbvJooi6TdR7rGgQHoI28yZM93ZXbt2SWd3dHTId1NaWjpqz6mhoSFlFIuiyJ1NpVLS2Tt37pTv5o3fJy4pKXHnlfsZHByUZslms+6s8vp06NAha29v91/+f88TV1dXu/NlZWXurPIaYnb8Z/BS7tHMrLOzc9RfcwoLC93ZoqIi5WibOnWqO9vQ0CCd3dPTI99NJpOJKyoq3PnW1lZ39tixY8oo0vuP8vg1G/n1WHrXqKqqsmuuucadVx7gjY3azrR58+a5sx/4wAeks6Mokhe4pdNpO/vss9155YXlK1/5ijTLrbfe6s4qBczMbMOGDaO+3O7UU091Z5UnsJnZM888485efPHF0tmrV6+W76aqqsq+/OUvu/PKk199EcrL838AfMopp0hnX3jhhYkeNyUlJXbBBRe485lMxp3t7u6WZlm2bJk7e/rpp7uzn/vc56Q5TqiurrbrrrvOnV++fLk7++1vf1ua5YYbbnBnFy5cKJ391FNPjfprTk2NvztMnjxZOvuOO+5wZz/ykY9IZ2/ZskW+m4qKCvvjP/5jd/6BBx5wZ1955RVpllwu584uWrRIOnvNmjXD3g1/1QUAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwZBWVlRWVtp73/ted/7xxx93Z5WvrjYze/rpp91ZZQVCUqlUyqZPn+7OK7vJPvShD0mzvP766+7s9773PensJEpKSmzu3LnuvLJjqre3V5rlsssuc2eVPUhJjR071v7kT/7Enf/Wt77lzqp38+ijj7qz6jqMpPLy8qTdSMraDXWX2fr1693ZVatWubPq7qoTUqmUtDpk0qRJ7mxPT480i7KGQlkrklRhYaGNHTvWnVdmUt+rlPVEP/3pT6Wzx4wZI+XNjv/Zbt261Z3fu3evOztu3DhpFmXH24YNG6SzR8InPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDGllRXt7u61evdqd7+rqcmeVFRRmZs8995w7e84550hnJzE4OGjt7e3uvPKV8CtWrJBmUc4uKSmRzk4iiiJplcDAwIA7W1lZKc2yb98+d7aurk46O4mjR4/aXXfd5c5/7nOfc2d/+MMfSrNccskl7mxBgfTSYc8++6yUP2FgYMBaW1vd+TiO3dmKiooEE/n87Gc/c2c7OjoS/z7K82rHjh3urLJKwMxs/vz57uxv8vN6qSuElBUsy5cvl2b5/ve/784uW7ZMOjuJbDZr73znO915ZQWS+pxS1sYcOHBAOnskfOIDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBIC3fiOLbBwUF3/m//9m/d2f/8z/9URrHe3l539gc/+IF0dhKFhYU2fvx4d/6+++5zZy+//HJpFmXvjLqTJ4nS0lJbsmSJO3/vvfe6s93d3dIsyh6whQsXSmc//PDDUt7MrKioyCZOnOjOK/u3lB07ZibtmhszZox0dlIdHR322GOPufPnnXeeO6s8T8y0/WS33XabO9vc3CzNccLg4KC1tLS488qf76xZs6RZlD1KxcXF0tlJDA0NSbsii4qK3NmpU6dKs6xZs8adveOOO6Szkxg7dqx94QtfcOfvvPNOd7atrU2a5cwzz3Rnt2/fLp396quvDvvP+MQHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIIhraxQPfTQQ+5sfX29dPauXbvc2UsuuUQ6+8EHH5TyZsdXJ6xfv96dX7BggTurfB28mVllZaU7W1ZWJp2dRGdnp61du9adnzRp0qjNonyNfS6XG7U5Tv499u7dOypnq1+tn5+f785u3LhRnCaZgoICq6qqcueVVRo9PT3yLF7ZbNadVe79ZHEcW39/vzv/zDPPuLMVFRXSLMrKh76+PunspKIocmeHhobcWWUdieprX/vaqJ19wuDgoLS+RHkdVN/H9+zZ4852dnZKZ4+ET3wAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEIwojmN/OIqazKxx9Mb5vVEXx3GN8gu4m+FxN8PjbkYWyP1wNyPjeTU87mZ4w96NVHwAAADeyvirLgAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQjAIlXFFREdfW1rrzfX19/kEKpFGsrKzMnR0aGpLO/uUvf9msfg14SUlJXFFR4c7n5+e7s+l0WhnF8vL8fVa9m/r6evluqqqq4kmTJrnzx44dc2dTqZQyipQfHByUzt66dat8N9lsNh4zZow7r8yvfiv7/v373dne3l7pbDOT78bMrLi4OFae64WFhe6s+thXfua2tjbp7DiOI+kXmNmYMWPiqVOnuvMHDhxwZ5ubm6VZSktL3dni4mLp7AMHDsiPnVQqFWcyGXdeeY1VHzc9PT3ubGdnp3S2JXheVVdXx5MnT3bnX3/9dXd24sSJyijS+35XV5d09quvvjrs3Uhto7a21m677TZ3ft++fe6s8uJvZrZ8+XJ3VnngmZllMhl5j0lFRYV9+tOfdufLy8vd2VNPPVWaRXnC53I56eyLL75YvptJkybZo48+6s5v27bNnZ0xY4Y0y7Rp09zZjo4O6ezy8nL5bsaMGWPXX3+9O19XV+fODgwMSLNcc8017uwLL7wgnW0JdwOVlZXZlVde6c4rBVt9k1Eelz/60Y+ks5OYOnWqbd682Z2/7rrr3Fnldd7MbNmyZe7snDlzpLP/5m/+Rn7sZDIZu+CCC9z52bNnu7Nq6d+yZYs7+9RTT0lnW4Ln1eTJk+3xxx935//6r//anb3xxhulWaqqqtzZX/ziF9LZS5YsGfZu+KsuAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAiGtLLi2LFj9sorr7jzhw8fdmeVPUFm2uqBoqIi6ewkBgYG7OjRo+68srJC2V1lpq3oWLx4sXR2EkVFRTZhwgR3fv369e6suo6ksdH/De/K16knVV5ebitWrHDnlbUJ6m4bZY4jR45IZx88eFDKn3Ds2DHbtWuXO9/e3u7OKnu9zMzmzZvnziqvk3v37pXmOGFgYED6c9i+fbs7q+z7M9PuMsE+KlkURdJOMOV5NXbsWGmW6upqd7amRltn19TUJOXNju8aU95TlDUw//Iv/yLNorzGqqtORsInPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDGllRS6Xs4aGBnd+aGjInZ0yZYoyiq1evdqdfTPWMgwODlpra6s7r3xte29vrzSL8jXgyte6J5XL5aRVERUVFe6s8nXqZmYFBf6H/I4dO6Szk8jPz5d+XmU1QHd3tzTL7Nmz3dn3v//90tk333yzlD8hl8vZzp073fn+/n53Vn3slJWVubPvfe973dnvfve70hwnDA0NSa8Nygqhs846S5olnU67s8pakaTy8vKkmfLz893Zjo4OaZbTTjvNnVVXUKxdu1bKmx1f56GsJJk6dao7e/XVV0uzjB8/3p29/vrrpbNHwic+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAiGtKursLBQ2q1x8OBBd7aoqEgZxZqbm93ZTZs2SWcnpew/UXYuKT+rmdlHP/pRd/af/umfpLOTiOPY+vr63PmWlhZ39ujRo9IsqVTKnVX2iyU1ODhoXV1d7rzyWFD2D5lpO9LOOOMM6eykBgcHpZ1ju3btcmez2aw0i/LYWblypTv74IMPSnOc0NPTYy+88II7rzx2pk+fLs2yd+9ed3bWrFnS2UnFcezOKo8F9W6UnXATJkyQzk6yq2toaMhyuZz86zzU9/G6ujp3ds2aNeo4w+ITHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIhrSyIi8vz0pLS9155euola+lNzO78cYb3dnOzk7p7CSKiops4sSJ7nxtba07+/GPf1yaRfmK9B07dkhnJ9HW1mYPP/ywO6+sRWlqapJmWbx4sTv75S9/WTo7iaGhIWllhbLOQ1kjYGb2wQ9+0J2dO3eudPbHPvYxKX9CXl6elZSUJPq1/5OdO3dK+XXr1rmzR44ccWeT/ny5XE5a0aE81y+44AJplvnz57uz6mt9Eup7VTqddmcPHz4szaKsd1GfJ6tWrZLyZmZ9fX3W0NDgzmcyGXdWXbmhrHkqKJDqysi/72/tJAAAgN9zFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACEYUx7E/HEVNZtY4euP83qiL47hG+QXczfC4m+FxNyML5H64m5HxvBoedzO8Ye9GKj4AAABvZfxVFwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIRoESLioqiouLi9350fxW6JKSEnf28OHD6vHN6teAV1ZWxhMnTnTn+/v73dnOzk5lFOnsbDYrnd3Q0CDfTSaTiSsrK935/Px8d7agQHoI2+DgoDvb2Ch/q7t8N1EUjdqTZNy4cVI+k8m4s8qfkZnZrl275LsxM6uoqIjHjx/vzudyOXc2L2/0/r1PuZ9Dhw5Ze3t7pP4eURTFys+gPNerqqqkWcrLy93Zo0ePSmfv379/1F9zlNdM5Xminl1Toz1FfvnLX8p3k06nY+WxoLzn9/T0KKNIz1e1T3R3dw97N9K7RnFxsZ1zzjnuvPImo/5QCxYscGf/+Z//WTrbEuwxmThxot1///3ufFNTkzv7xBNPSLMcPHjQnV22bJl09tVXXy3fTWVlpa1atcqdV56U6gtFe3u7O/upT31KOtvehP03UeR/f/yjP/oj6eyzzz7bna2oqJDOXr58eaK7GT9+vN11113ufH19vTubSqWkWZS7V950P/OZz0hznJCXlye9KS1dutSdveqqq6RZ3vWud7mz3/ve96SzV61aleg15/Of/7w7f+jQIXd20aJF0izKv3h/8pOflM7OZrPy3WSzWVu5cqU7P3fuXHf2+eefl2ZpaGhwZ5UCaWb27LPPDns3/FUXAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAARDWllRVlZmy5cvd+eVVQvqbpjCwkJ3VvnKbTOzbdu2SXkzs3Q6baeffro7r6y3SKfT0iy7d+92Z0899VTp7CSiKJLWAyirAdQ9bHV1de7snDlzpLNffvllKW92/GdV/nwvv/xyd1bdp6WsUTnjjDOks5NKpVI2ffp0d37r1q3urLJSx8ysurranR07dqw7q7yW/eqvU/YDKvd45plnSrMoa3KUvV5JpdNpmzVrljuvrL6ZMGGCNIuyakFZQZJUQUGB9H7b0tLizpaWlkqzKPf+4IMPSmePhE98AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAY0sqK4uJimz17tju/evVqd3bfvn3KKNLXkStrCsySraxQPfnkk+5sNpuVzla+In39+vXS2UlEUSStT+ju7nZnOzs7pVnmzZvnziprNpKKokha0TF58mR3Vl0NsH37dnf23e9+t3R2Uu3t7fbII4+488rr04svvijNoqyHUFY4KM/Xk+VyOauvr3fnx40b5862tbVJsyj5OI6ls5PIZDK2ePFid37Xrl3ubGVlpTRLa2urO9vX1yednURxcbG0Xkl5jc3L0z5L2bNnjztbUCDVlRGfV3ziAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgSMsvBgcHraury51X9o5UVVUpo1hHR4c7+9nPflY6+yc/+YmUNzs+z+OPP+7O33fffe7s5ZdfLs2ycOFCd3br1q3S2UkMDAxYS0uLO19bW+vOqrttZsyY4c4qu42SGhoasp6eHnd+6tSp7uzzzz8vzTI4OOjOjh07Vjo7qfb2dmnn38c+9jF3dtKkSdIsyj6qTCbjzv4mu6uGhobc2fPPP9+dVZ4nZmYbNmxwZ9/+9rdLZycxNDQkvVeNHz/enT106JA0y7p169xZ5XGTVCaTsXPOOcedX7t2rTvb2NgozaLsZSwqKpLO7u3tHfaf8YkPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAARDWlnR1NRkt956qztfX1/vziprFsyOr0Hweve73y2dncSuXbvsoosucufz8vydU/1K+82bN7uzGzdulM5Oorq62q666ip3/he/+IU7W1xcLM2ifDX9rFmzpLOV1QonVFVV2YoVK9z5ggL/U1ZZ/WFm1tzc7M6m02np7KRaW1vt+9//vjuvPNeXLl0qzVJXV+fO3nnnne5sf3+/NMfJ8vPz3dmzzz7bnb355pulOVpbW93Z+fPnS2cnUVhYKD3+S0tL3VllzYKZ2ZgxY9zZ5cuXS2cnkUqlbNq0ae78Qw895M52dnZKsyhrYNrb26WzR8InPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIRqTsgYqiqMnMGkdvnN8bdXEc1yi/gLsZHnczPO5mZIHcD3czMp5Xw+Nuhjfs3UjFBwAA4K2Mv+oCAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMH4v7OlyjA+HoypAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 720x720 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(lane_keeper.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 1, 4, 'conv0', size=(8,2))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 4, 4, 'conv1', size=(5,5)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 8, 8, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LaneKeeper(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout(p=0.2, inplace=False)\n",
       "    (11): Conv2d(4, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "lane_keeper.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "lane_keeper.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "lane_keeper.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(lane_keeper, dummy_input, onnx_lane_keeper_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lane_keeper.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 8965.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[-0.03652993  0.01066652]]\n",
      "Predictions shape: (1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_lane_keeper_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
