{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/lane_keeper_small.pt'\n",
    "onnx_lane_keeper_path = \"models/lane_keeper_small.onnx\"\n",
    "max_load = 250_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE\n",
    "# #very good\n",
    "# class LaneKeeper(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             nn.Linear(in_features=4*4*4, out_features=16),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(in_features=16, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# lane_keeper = LaneKeeper(out_dim=2,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class LaneKeeper(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        prob = 0.2\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 4, kernel_size=5, stride=1), #out = 28\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=14\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(4, 4, kernel_size=5, stride=1), #out = 10\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=5\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(4, 128, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            #normalize\n",
    "            # nn.BatchNorm1d(3*3*4),\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=1*1*128, out_features=32),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(in_features=32, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "lane_keeper = LaneKeeper(out_dim=2,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "lane_keeper.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = lane_keeper(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from time import time, sleep\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "    #convert to gray\n",
    "    img = cv.resize(img, (4*SIZE[1], 4*SIZE[0]))\n",
    "\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "        axes_length = (randint(10//4, 50//4), randint(50//4, 300//4))\n",
    "        angle = randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (50,50))\n",
    "    noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = np.clip(light, 0, 51)\n",
    "    light *= 5\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    # cv.imshow('light', light)\n",
    "    # if cv.waitKey(0) == ord('q'):\n",
    "    #     break\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 5), randint(1, 5)))\n",
    "\n",
    "    # cut the top third of the image, let it 640x320\n",
    "    img = img[int(img.shape[0]/3):,:] ################################# /3\n",
    "    # assert img.shape == (320,640), f'img shape cut = {img.shape}'\n",
    "\n",
    "    #edges\n",
    "    img = cv.resize(img, (2*SIZE[1], 2*SIZE[0]))\n",
    "\n",
    "    r = randint(0, 5)\n",
    "    if r == 0:\n",
    "        #dilate\n",
    "        kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.dilate(img, kernel, iterations=1)\n",
    "    elif r == 1:\n",
    "        #erode\n",
    "        kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.erode(img, kernel, iterations=1)\n",
    "\n",
    "\n",
    "    #edges    \n",
    "    img = cv.Canny(img, 100, 200)\n",
    "\n",
    "    #blur\n",
    "    img = cv.blur(img, (3,3))\n",
    "\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    # #get max brightness\n",
    "    # max_brightness = np.max(img)\n",
    "    # ratio = 255.0/max_brightness\n",
    "    # #normalize\n",
    "    # img = (img*ratio).astype(np.uint8)\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = 3\n",
    "    offset = randint(-max_offset, max_offset)\n",
    "    img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        img[:offset, :] = 0 #randint(0,255)\n",
    "    elif offset < 0:\n",
    "        img[offset:, :] = 0 # randint(0,255)\n",
    "    \n",
    "    # #add salt and pepper noise\n",
    "    # sp_noise = randint(0, 4, size=img.shape, dtype=np.uint8)\n",
    "    # sp_noise = np.where(sp_noise == 0, np.zeros_like(img), 255*np.ones_like(img))\n",
    "    # # img = cv.bitwise_xor(img, sp_noise)\n",
    "\n",
    "\n",
    "    # #reduce contrast\n",
    "    # const = np.random.uniform(0.1,0.8)\n",
    "    # # if np.random.uniform() > .5:\n",
    "    # #     const = const*0.2\n",
    "    # img = 127*(1-const) + img*const\n",
    "    # img = img.astype(np.uint8)\n",
    "\n",
    "    #add noise \n",
    "    std = 80\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "    # #add random brightness\n",
    "    # max_brightness = 60\n",
    "    # brightness = randint(-max_brightness, max_brightness)\n",
    "    # if brightness > 0:\n",
    "    #     img = cv.add(img, brightness)\n",
    "    # elif brightness < 0:\n",
    "    #     img = cv.subtract(img, -brightness)\n",
    "\n",
    "    # #blur \n",
    "    # img = cv.blur(img, (randint(1,3),randint(1,3)))\n",
    "\n",
    "    # # invert color\n",
    "    # if np.random.uniform(0, 1) > 0.6:\n",
    "    #     img = cv.bitwise_not(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(5000):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(100)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        #classification label for road ahead = 1,0,0,0,1,0,0,0,0,0,0,0,0,0,0\n",
    "        road_images_indexes = []\n",
    "        tot_lines = 0\n",
    "        with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            tot_lines = len(lines)\n",
    "            for i,line in enumerate(lines):\n",
    "                if line == '1,0,0,0,1,0,0,0,0,0,0,0,0,0,0':\n",
    "                    road_images_indexes.append(i)\n",
    "        print(f'total pure road images: {len(road_images_indexes)}')\n",
    "        road_imgs_mask = np.zeros(tot_lines, dtype=np.bool)\n",
    "        road_imgs_mask[road_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            # self.all_imgs = torch.zeros((2*max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8) #adding flipped img\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            # road images specifically are added again along with their flipped image and label\n",
    "            road_imgs = torch.zeros((2*len(road_images_indexes), SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "            road_labels = []\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "            road_idx = 0\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(max_load)):\n",
    "\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                sample = line.split(',')\n",
    "                #keep only info related to the lane, discard distance from stop line \n",
    "                sample = [sample[0], sample[1], sample[3]] #e2=lateral error, e3=yaw error point ahead, curvature\n",
    "                reg_label = np.array([float(s) for s in sample], dtype=np.float32)\n",
    "\n",
    "                #img \n",
    "                img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "\n",
    "                #check if its in the road images\n",
    "                if road_imgs_mask[i]:\n",
    "                    img_r = load_and_augment_img(img.copy())\n",
    "                    # cv.imshow('imgR', img_r)\n",
    "                    img_r = img_r[:,:,np.newaxis]\n",
    "\n",
    "                    img_l = cv.flip(img, 1)\n",
    "                    img_l = load_and_augment_img(img_l)\n",
    "                    # cv.imshow('imgL', img_l)\n",
    "                    img_l = img_l[:,:,np.newaxis]\n",
    "                    # cv.waitKey(1)\n",
    "\n",
    "                    road_imgs[2*road_idx] = torch.from_numpy(img_r)\n",
    "                    road_imgs[2*road_idx+1] = torch.from_numpy(img_l)\n",
    "                    road_labels.append(reg_label)\n",
    "                    road_labels.append(-reg_label)\n",
    "                    road_idx += 1\n",
    "\n",
    "                else:\n",
    "                    img = load_and_augment_img(img)\n",
    "                    # cv.putText(img, f'{reg_label[0]*10.0}', (5,5), cv.FONT_HERSHEY_SIMPLEX, 0.4,255, 1)\n",
    "                    MAX_SHOW = 1000\n",
    "                    max_show = MAX_SHOW\n",
    "                    if i < max_show:\n",
    "                        cv.imshow('img', img)\n",
    "                        key = cv.waitKey(1)\n",
    "                        if i == max_show-1:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(reg_label)\n",
    "                    all_img_idx += 1\n",
    "\n",
    "            #cut imgs to the right length\n",
    "            road_imgs = road_imgs[:2*road_idx]\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "\n",
    "            #concatenate all_imgs and road_imgs\n",
    "            print(f'road images: {road_imgs.shape}')\n",
    "            print(f'all images: {self.all_imgs.shape}')\n",
    "            self.all_imgs = torch.cat((self.all_imgs, road_imgs), dim=0)\n",
    "            print(f'self.data shape: {len(self.data)}')\n",
    "            print(f'road_labels shape = {len(road_labels)}')\n",
    "            self.data = np.concatenate((np.array(self.data), np.array(road_labels)), axis=0)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "            #free road_imgs from memory\n",
    "            del road_imgs\n",
    "            del road_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pure road images: 126213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 245182/245182 [13:19<00:00, 306.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "road images: torch.Size([252426, 32, 32, 1])\n",
      "all images: torch.Size([118969, 32, 32, 1])\n",
      "self.data shape: 118969\n",
      "road_labels shape = 252426\n",
      "\n",
      "all imgs: torch.Size([371395, 32, 32, 1])\n",
      "data: (371395, 3)\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 1, 32, 32])\n",
      "torch.Size([8192, 3])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    err_losses2 = []\n",
    "    err_losses3 = []\n",
    "    # curv_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        err2 = output[:, 0]\n",
    "        err3 = output[:, 1]\n",
    "        # curv_out = output[:, 2]\n",
    "\n",
    "        err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 1]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        # Compute the losses\n",
    "        err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        # curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = err_loss3 + err_loss2 + L1_loss + L2_loss #+ curv_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    err_loss2 = np.mean(err_losses2)\n",
    "    err_loss3 = np.mean(err_losses3)\n",
    "    # curv_loss = np.mean(curv_losses)\n",
    "    return err_loss2, err_loss3\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device=device):\n",
    "    lane_keeper.eval()\n",
    "    err_losses3 = []\n",
    "    err_losses2 = []\n",
    "    # curv_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = lane_keeper(input)\n",
    "\n",
    "        regr_out = output\n",
    "        err2 = regr_out[:, 0]\n",
    "        err3 = regr_out[:, 1]\n",
    "        # curv_out = regr_out[:, 2]\n",
    "\n",
    "        err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 1]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        # curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "        loss = err_loss3 + err_loss2\n",
    "\n",
    "        err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "    return np.mean(err_losses2), np.mean(err_losses3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15/30,  loss = MSELoss() \n",
      "yaw_err_loss3: 0.0146,   Val: 0.0148\n",
      "lat_err_loss2: 0.0016,   Val: 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 8/41 [00:04<00:18,  1.81it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_42825/4122303408.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mregr_loss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr_loss_fn1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mregr_loss_fn2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0merr_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_loss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mval_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_42825/1434882309.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Loop over the training batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Move the input and target data to the selected device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.001 #0.005\n",
    "epochs = 30\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 1e-4 #9e-4\n",
    "L2_lambda = 1e-2 #1e-2\n",
    "optimizer = torch.optim.Adam(lane_keeper.parameters(), lr=lr, weight_decay=9e-5) #wd = 2e-3# 3e-5\n",
    "regr_loss_fn1 = nn.MSELoss() #before epochs/2\n",
    "regr_loss_fn2 = nn.MSELoss() #after epochs/2 for finetuning\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        regr_loss_fn = regr_loss_fn1 if epoch < epochs//2 else regr_loss_fn2\n",
    "        err_loss2, err_loss3 = train_epoch(lane_keeper, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_loss2, val_loss3 = val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs},  loss = {regr_loss_fn} \\nyaw_err_loss3: {err_loss3:.4f},   Val: {val_loss3:.4f}\")\n",
    "    print(f\"lat_err_loss2: {err_loss2:.4f},   Val: {val_loss2:.4f}\")\n",
    "    # print(f\"curv_loss: {curv_loss}\")\n",
    "    torch.save(lane_keeper.state_dict(), model_name)\n",
    "\n",
    "#Note: sweet spot for training is around 0.016 -> 0.020, also note that training can get stuck, and loss can start improving randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 372/372 [00:01<00:00, 230.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lateral_err2_loss: 0.0015594441210851073\n",
      "yaw_err3_loss: 0.014791475608944893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "lane_keeper.load_state_dict(torch.load(model_name))\n",
    "err_loss2, err_loss3 = val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device)\n",
    "\n",
    "print(f\"lateral_err2_loss: {err_loss2}\")\n",
    "print(f\"yaw_err3_loss: {err_loss3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 5, 5)\n",
      "(4, 4, 5, 5)\n",
      "(128, 4, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x144 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(lane_keeper.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 1, 4, 'conv0', size=(8,2))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 4, 4, 'conv1', size=(5,5)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 8, 8, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LaneKeeper(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout(p=0.2, inplace=False)\n",
       "    (11): Conv2d(4, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "lane_keeper.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "lane_keeper.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "lane_keeper.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(lane_keeper, dummy_input, onnx_lane_keeper_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lane_keeper.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 4000.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[0.01462647 0.04006106]]\n",
      "Predictions shape: (1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_lane_keeper_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
