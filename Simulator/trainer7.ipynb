{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/lane_keeper_small.pt'\n",
    "onnx_lane_keeper_path = \"models/lane_keeper_small.onnx\"\n",
    "max_load = 150_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE\n",
    "# #very good\n",
    "# class LaneKeeper(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             nn.Linear(in_features=4*4*4, out_features=16),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(in_features=16, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# lane_keeper = LaneKeeper(out_dim=2,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class LaneKeeper(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        prob = 0.2\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 4, kernel_size=5, stride=1), #out = 28\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=14\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(4, 4, kernel_size=5, stride=1), #out = 10\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=5\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(4, 32, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            #normalize\n",
    "            # nn.BatchNorm1d(3*3*4),\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=1*1*32, out_features=32),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(in_features=32, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "lane_keeper = LaneKeeper(out_dim=2,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "lane_keeper.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = lane_keeper(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from time import time, sleep\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "\n",
    "\n",
    "\n",
    "    #convert to gray\n",
    "    img = cv.resize(img, (4*SIZE[1], 4*SIZE[0]))\n",
    "\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "        axes_length = (randint(10//4, 50//4), randint(50//4, 300//4))\n",
    "        angle = randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (50,50))\n",
    "    noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = np.clip(light, 0, 51)\n",
    "    light *= 5\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    # cv.imshow('light', light)\n",
    "    # if cv.waitKey(0) == ord('q'):\n",
    "    #     break\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 5), randint(1, 5)))\n",
    "\n",
    "    # cut the top third of the image, let it 640x320\n",
    "    img = img[int(img.shape[0]/3):,:] ################################# /3\n",
    "    # assert img.shape == (320,640), f'img shape cut = {img.shape}'\n",
    "\n",
    "    #edges\n",
    "    img = cv.resize(img, (2*SIZE[1], 2*SIZE[0]))\n",
    "\n",
    "    #edges    \n",
    "    img = cv.Canny(img, 100, 200)\n",
    "\n",
    "    #blur\n",
    "    img = cv.blur(img, (3,3))\n",
    "\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "        #add random tilt\n",
    "    max_offset = 3\n",
    "    offset = randint(-max_offset, max_offset)\n",
    "    img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        img[:offset, :] = 0 #randint(0,255)\n",
    "    elif offset < 0:\n",
    "        img[offset:, :] = 0 # randint(0,255)\n",
    "\n",
    "\n",
    "    # #reduce contrast\n",
    "    # const = np.random.uniform(0.1,0.8)\n",
    "    # # if np.random.uniform() > .5:\n",
    "    # #     const = const*0.2\n",
    "    # img = 127*(1-const) + img*const\n",
    "    # img = img.astype(np.uint8)\n",
    "\n",
    "    # #add noise \n",
    "    # std = 60\n",
    "    # std = randint(1, std)\n",
    "    # noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    # img = cv.subtract(img, noisem)\n",
    "    # noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    # img = cv.add(img, noisep)\n",
    "\n",
    "    # #add random brightness\n",
    "    # max_brightness = 60\n",
    "    # brightness = randint(-max_brightness, max_brightness)\n",
    "    # if brightness > 0:\n",
    "    #     img = cv.add(img, brightness)\n",
    "    # elif brightness < 0:\n",
    "    #     img = cv.subtract(img, -brightness)\n",
    "\n",
    "    # #blur \n",
    "    # img = cv.blur(img, (randint(1,3),randint(1,3)))\n",
    "\n",
    "    # # invert color\n",
    "    # if np.random.uniform(0, 1) > 0.6:\n",
    "    #     img = cv.bitwise_not(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(50)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        #classification label for road ahead = 1,0,0,0,1,0,0,0,0,0,0,0,0,0,0\n",
    "        road_images_indexes = []\n",
    "        tot_lines = 0\n",
    "        with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            tot_lines = len(lines)\n",
    "            for i,line in enumerate(lines):\n",
    "                if line == '1,0,0,0,1,0,0,0,0,0,0,0,0,0,0':\n",
    "                    road_images_indexes.append(i)\n",
    "        print(f'total pure road images: {len(road_images_indexes)}')\n",
    "        road_imgs_mask = np.zeros(tot_lines, dtype=np.bool)\n",
    "        road_imgs_mask[road_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            # self.all_imgs = torch.zeros((2*max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8) #adding flipped img\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            # road images specifically are added again along with their flipped image and label\n",
    "            road_imgs = torch.zeros((2*len(road_images_indexes), SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "            road_labels = []\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "            road_idx = 0\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(max_load)):\n",
    "\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                sample = line.split(',')\n",
    "                #keep only info related to the lane, discard distance from stop line \n",
    "                sample = [sample[0], sample[1], sample[3]] #e2=lateral error, e3=yaw error point ahead, curvature\n",
    "                reg_label = np.array([float(s) for s in sample], dtype=np.float32)\n",
    "\n",
    "                #img \n",
    "                img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "\n",
    "                #check if its in the road images\n",
    "                if road_imgs_mask[i]:\n",
    "                    img_r = load_and_augment_img(img.copy())\n",
    "                    # cv.imshow('imgR', img_r)\n",
    "                    img_r = img_r[:,:,np.newaxis]\n",
    "\n",
    "                    img_l = cv.flip(img, 1)\n",
    "                    img_l = load_and_augment_img(img_l)\n",
    "                    # cv.imshow('imgL', img_l)\n",
    "                    img_l = img_l[:,:,np.newaxis]\n",
    "                    # cv.waitKey(1)\n",
    "\n",
    "                    road_imgs[2*road_idx] = torch.from_numpy(img_r)\n",
    "                    road_imgs[2*road_idx+1] = torch.from_numpy(img_l)\n",
    "                    road_labels.append(reg_label)\n",
    "                    road_labels.append(-reg_label)\n",
    "                    road_idx += 1\n",
    "\n",
    "                else:\n",
    "                    img = load_and_augment_img(img)\n",
    "                    if i < 100:\n",
    "                        cv.imshow('img', img)\n",
    "                        cv.waitKey(1)\n",
    "                        if i == 99:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(reg_label)\n",
    "                    all_img_idx += 1\n",
    "\n",
    "            #cut imgs to the right length\n",
    "            road_imgs = road_imgs[:2*road_idx]\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "\n",
    "            #concatenate all_imgs and road_imgs\n",
    "            print(f'road images: {road_imgs.shape}')\n",
    "            print(f'all images: {self.all_imgs.shape}')\n",
    "            self.all_imgs = torch.cat((self.all_imgs, road_imgs), dim=0)\n",
    "            print(f'self.data shape: {len(self.data)}')\n",
    "            print(f'road_labels shape = {len(road_labels)}')\n",
    "            self.data = np.concatenate((np.array(self.data), np.array(road_labels)), axis=0)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "            #free road_imgs from memory\n",
    "            del road_imgs\n",
    "            del road_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pure road images: 79427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142527/142527 [12:25<00:00, 191.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "road images: torch.Size([158854, 32, 32, 1])\n",
      "all images: torch.Size([63100, 32, 32, 1])\n",
      "self.data shape: 63100\n",
      "road_labels shape = 158854\n",
      "\n",
      "all imgs: torch.Size([221954, 32, 32, 1])\n",
      "data: (221954, 3)\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 1, 32, 32])\n",
      "torch.Size([8192, 3])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    err_losses2 = []\n",
    "    err_losses3 = []\n",
    "    # curv_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        err2 = output[:, 0]\n",
    "        err3 = output[:, 1]\n",
    "        # curv_out = output[:, 2]\n",
    "\n",
    "        err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 1]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        # Compute the losses\n",
    "        err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        # curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = err_loss3 + err_loss2 + L1_loss + L2_loss #+ curv_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    err_loss2 = np.mean(err_losses2)\n",
    "    err_loss3 = np.mean(err_losses3)\n",
    "    # curv_loss = np.mean(curv_losses)\n",
    "    return err_loss2, err_loss3\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device=device):\n",
    "    lane_keeper.eval()\n",
    "    err_losses3 = []\n",
    "    err_losses2 = []\n",
    "    # curv_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = lane_keeper(input)\n",
    "\n",
    "        regr_out = output\n",
    "        err2 = regr_out[:, 0]\n",
    "        err3 = regr_out[:, 1]\n",
    "        # curv_out = regr_out[:, 2]\n",
    "\n",
    "        err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 1]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        # curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "        loss = err_loss3 + err_loss2\n",
    "\n",
    "        err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "    return np.mean(err_losses2), np.mean(err_losses3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30/30,  loss = L1Loss() \n",
      "yaw_err_loss3: 0.0703,   Val: 0.0684\n",
      "lat_err_loss2: 0.0277,   Val: 0.0275\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.001 #0.005\n",
    "epochs = 30\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 1e-4 #9e-4\n",
    "L2_lambda = 2e-2 #1e-2\n",
    "optimizer = torch.optim.Adam(lane_keeper.parameters(), lr=lr, weight_decay=9e-5) #wd = 2e-3# 3e-5\n",
    "regr_loss_fn1 = nn.MSELoss() #before epochs/2\n",
    "regr_loss_fn2 = nn.L1Loss() #after epochs/2 for finetuning\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        regr_loss_fn = regr_loss_fn1 if epoch < epochs//2 else regr_loss_fn2\n",
    "\n",
    "        err_loss2, err_loss3 = train_epoch(lane_keeper, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_loss2, val_loss3 = val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs},  loss = {regr_loss_fn} \\nyaw_err_loss3: {err_loss3:.4f},   Val: {val_loss3:.4f}\")\n",
    "    print(f\"lat_err_loss2: {err_loss2:.4f},   Val: {val_loss2:.4f}\")\n",
    "    # print(f\"curv_loss: {curv_loss}\")\n",
    "    torch.save(lane_keeper.state_dict(), model_name)\n",
    "\n",
    "#Note: sweet spot for training is around 0.016 -> 0.020, also note that training can get stuck, and loss can start improve randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [00:00<00:00, 302.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lateral_err2_loss: 0.027474619448184967\n",
      "yaw_err3_loss: 0.06841150671243668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "lane_keeper.load_state_dict(torch.load(model_name))\n",
    "err_loss2, err_loss3 = val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device)\n",
    "\n",
    "print(f\"lateral_err2_loss: {err_loss2}\")\n",
    "print(f\"yaw_err3_loss: {err_loss3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 5, 5)\n",
      "(4, 4, 5, 5)\n",
      "(32, 4, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x144 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj4AAAJ5CAYAAACubzp4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAA9fUlEQVR4nO3de3SddZn+//tJ9k52zuc0TZo2pWBpoRRKgakcFBBcqOiI4gGRsyAsu5QZHe3AiKKMistxRIdRYUQFEegIjl9FQRAo5WRbaO3RtLRJz22Spoecd/Z+vn/Url9/fk28r0erMp/3a61Za2a8+uHup8/eudh17TuK49gAAABCUPDXHgAAAOAvheIDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig+Av7ooit4aRdHiKIr2RlG0M4qiu6MoqvhrzwXgfx+KD4C/BVVm9nkzazazGWbWYmZf/qtOBOB/JYoPgD8oiqLWKIoejqKoK4qiniiKvhFFUUEURTdHUdQZRdHuKIq+H0VR1e/ybVEUxVEUXR5F0eYoirqjKLrpd/9ZcxRFg1EU1R52/km/y6TjOL4/juNfxHE8EMdxr5ndZWan/3V+5wD+N6P4APh/RFFUaGY/NbNOM2uzg5/APGBmV/zuf842s6PMrNzMvvF7v/wMM5tuZuea2aejKJoRx/F2M3vBzN51WO4SM/vvOI6zf2CEs8xs9Z/ndwMA/5+IXV0Afl8URfPM7CdmNjGO49HD/v9PmtmP4ji+83f/93QzW2VmJWY2ycw2mVlrHMdbf/ef/9rM/i2O4weiKLrGzC6J4/icKIoiM9tsZh+I43jR7/2zzzOzh8zstDiO24/07xVAWPjEB8Af0mpmnYeXnt9ptoOfAh3SaWYpM5tw2P9v52H/+4Ad/FTIzOxHZjYviqKJdvATnbyZPXv44VEU/Z2Z3W9m76b0ADgSUn/tAQD8TdpiZpOjKEr9XvnZbmZTDvu/J5vZqJntsoOf+IwpjuPeKIoeN7P32sH/AvMD8WEfOUdRdJId/JTpqjiOn/zz/DYA4P+PT3wA/CG/NrMdZvbFKIrKoijKRFF0upn90MxujKJoahRF5Wb2r2b24B/4ZGgs95vZZWb27t/972ZmFkXR8Wb2CzObH8fx//lz/kYA4HAUHwD/jziOc2Z2oZkdbQf/uzhb7eAnNd8xs3vNbJEd/O/zDJnZfOHon5jZMWa2M47jFYf9///RzBrM7L+iKOr73f/wX24G8GfHf7kZAAAEg098AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAAQjJYVTqbi4uNidz+fz7mwURcookpqaGim/ffv27jiOG5RfU1BQEKdS/ussKSlRzlZGOaJ32dvbK99NZWVl3NjY6M739fW5s5MmTVJGsd7eXnd2165d0tn9/f3y3ZSXl8d1dXXu/OjoqDurPjdKvqFB+m3asmXL5LsxM6uoqJDup7Cw0J2N41iaRbmfbDbrzvb09FhfX5/8oq2vr4/b2trc+a6uLndWmd/MbGhoyJ2tqKiQzt68ebP87KTT6TiTybjzZWVl7qzyGjTTnkkla2a2Y8cO+W6Ki4tj5fer/Kzavn27Mop07+p7TkdHx5h3IxWf4uJimzFjhjvf398vna1QfrhffPHF0tk33XRTp/QLzCyVSll9fb07f+KJJ7qzygvYzCydTruz6pv/woUL5btpbGy022+/3Z1/8cUX3dkvfOEL0iw/+tGP3Nk77rhDOvu5556T76aurs4WLFjgzis/vJQ3LDPtTej666+Xzo6iSL4bs4P3c9NNN7nz1dXV7uzw8LA0S3l5uTur/AD40pe+JM1xSFtbmy1dutSdv/POO93ZHTt2SLOsW7fOnT3vvPOks6+77jr52clkMnbyySe786eeeqo7q7wGzbR/8VZL4a233irfTVlZmZ1//vnu/MyZM93ZW265RZpl1qxZ7uyHP/xh6ewrrrhizLvhr7oAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBjSyop8Pi99zfurr77qzra0tCijSDuafvWrX0lnJxFFkRUVFbnzylfaq/uoTjjhBHf217/+tXR2EtXV1XbRRRe582vWrHFnu7u7pVlWrVrlzqpfTZ9EQUGBlZaWuvPKHrM9e/YkGclFWUHyp4jj2HK5nDuvPPvPP/+8NIvyXB577LHurLJi5nBDQ0O2du1ad765udmdVd7LzLQ1MytXrpTOTmJgYMBefvlld155xpYsWSLNcvnll7uz7373u6Wzb731VilvdvB5mzhxojs/ZcoUd/aaa66RZunp6XFnOzsTbb35g/jEBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCIa2siKLIUin/L6murj4iWTOzpqYmd1ZZU5DU6Oio9PXbyu83iiJplunTp7uzGzZskM5OKo5jd/all15yZ0877TRpDuXs9vZ26ewk1K/WV54b5Wv4zcxGRkbc2a1bt0pnJ6Wu9FCe/RdeeEGa5Uc/+pE7e9ttt7mzBQXJ/v0zm83a7t273XnlNajONDQ05M4uXrxYOjuJfD5vAwMD7nx5ebk7q/wMNDOrq6tzZ+fNmyednURpaanNnj37iJz9hje8Qcrfdddd7mx9fb06zpj4xAcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwZCWjgwODtry5cvd+ZKSEv8g4v6T888/353t7e2Vzl63bp2UNzu4G6avr8+d7+/vd2eV/UNmZrNmzXJnlf1iZmY/+MEPpLyZ2fbt2+2WW25x5w8cOODOrly5UpplyZIl7qy6ryifz0t5s4P7sTZv3uzOK8/NpEmTpFmU3UbKn9GfoqCgQHofUfZvKTvAzEzaqbZr1y53NpvNSnMcMjIyYp2dne58Op12Z9X3hdraWne2q6tLOjuJdDptEyZMcOd/+ctfurPqDrx3vOMd7qy6lzGJVCol7Q9T3hd27NghzXLppZe6sytWrJDOHg+f+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMKQ9EQUFBdLXvM+ePdudbW5uVkaxD33oQ+7s2rVrpbOffPJJKW9mVlxcbK2tre58JpNxZzs6OqRZlK/4/8hHPiKdPX/+fClvZtbX12eLFy925+fOnevOtre3S7Ocdtpp7qz61frLli2T8mZmcRxLqy6UNRpf//rXpVmUNQXXXXeddHZSuVxOWgWjrHBQvorfzGzatGnu7L333uvOqushDsnlcrZ//353XnlfUNZbmJm98sor7uyWLVuks5MYHR2V7jWOY3f22muvlWZ5+umn3dnGxkbp7KSU1RhlZWXu7EUXXSTNMXnyZHf2C1/4gnT2ePjEBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBiJQdJVEUdZmZfxnOa9eUOI4blF/A3YyNuxkbdzO+QO6Huxkfr6uxcTdjG/NupOIDAADwWsZfdQEAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYKSUcFlZWVxdXe3Oj46OurO7d+9WRrG2tjZ3tqysTDp79erV3erXgKdSqbi4uNidr6+vd2f37NmjjGJ1dXXubGFhoXT2xo0b5buJokj6evDS0lJ3dtq0acrR0jO5f/9+6ext27bJd1NZWRk3NPh/SVFRkTurfit7d3e3O9vT0yOdbWby3ZiZlZeXx8rznMlk3NlUSnr7s23btrmz+/btk86O4ziSfoGZ1dbWxq2tre78wMCAO1tVVSXNopytPpfr1q2Tn536+vpY+RkxPDzszq5atUoZxSZPnuzO1tTUSGevWLFCvptMJhNXVFS488r7sXKPZmb5fN6d7erqks62cd5zpFd+dXW1XX/99e688gP7q1/9qjKK3XLLLe7svHnzpLOPPfZYeY9JcXGxzZw5052/+uqr3dkHH3xQmuXSSy91Z9U3uIsvvviI73hR7vGRRx6RzlZePI899ph09oIFC+S7aWhosC996UvufEtLizuby+WkWe6++2539nvf+550tiXcDVRXV2ef/OQn3fkZM2a4s7W1tdIsN910kzv7s5/9TDo7idbWVukZfeWVV9zZCy64QJpFOXtkZEQ6++/+7u/kZ6etrc2WLl3qzr/66qvu7NFHHy3NsmDBAnf2Pe95j3R2XV2dfDcVFRV20UUXufOzZ892Zzs7tXEOHDjgzv7nf/6ndLaN857DX3UBAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDCklRUjIyPW0dHhzu/du9edVfZcqZQ9MkkNDQ3Z2rVr3fnKykp3try8XJpF+Sr+5uZm6ewk6uvr7V3vepc7/7rXvc6d3bhxozSLsm+pr69POjuJyspKO/fcc915ZddYQYH27zVve9vb3FnlHs3MnnjiCSl/yOjoqPQ+kk6n3Vn193Deeee5s8pqFHX30yH5fF76yn9lj9maNWukWZTVN1OnTpXOTiKXy0m79kpKStzZD37wg9Isc+bMcWeVZz2pfD5v/f397vyvfvUrd/aEE06QZlHe65VVTGZm991335j/GZ/4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwpJUV6tfHDw8Pu7NNTU3KKNLXr4+MjEhnJ6F+DXhPT487W11dLc2irDWYPn26dHYSLS0t9rnPfc6df/TRR93ZJ598Upoll8u5s0NDQ9LZSRQWFlpNTY07r7z+lDUCKuUZ+1NEUSStoVBW6iivVzOzWbNmubOf+tSn3NlPfOIT0hyHqHej/H5//vOfS7Mo71Gtra3S2UmoK4SU18o555wjzdLQ0ODOHsnVTYfkcjlp1cng4KA7u3z5cmmWnTt3urP33nuvdDYrKwAAAIziAwAAAkLxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBkHZ1FRQUWFFRkTvf2Njozqp7kcrLy93ZyZMnS2cnkU6npX1jnZ2d7qyyX8pM238yMDAgnZ1EKpWS9tUsXbrUnVX3UfX19bmzBQVH/t8LRkZGbOvWre68sqNu//790izKa7C+vl46O6nCwkKrrKx05ydMmODOKruczMweeOABd/bb3/62O3vbbbdJcxySSqWsrq7OnZ85c6Y7u379emmWe+65x52tqKiQzk4inU5bS0uLO79p0yZ3dteuXdIsynuO+ppNQn1NKfvguru7pVmU/XEvvviidPZ4+MQHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIIhrawoLS21U045xZ0vLi52Z7dt26aMYl1dXe7sxIkTpbOTUlZLHHPMMe7s7NmzpTkee+wxd3b+/PnS2Un09/dLXzeezWbdWXWdh/J1+T09PdLZSWSzWdu+fbs7r6z+mDp1qjTL3Llz3dknn3xSOjup0dFR6bU+PDzszipfl29m9v3vf9+dPfPMM93ZpM9ZPp+Xfr/Ka2XatGnSLPv27XNn3/ve90pnJ5FOp6XXirJWQlmFYabdjbKSJqmCggIrLS1155VnTLlzM60jvP/975fOHg+f+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGFEcx/5wFHWZWeeRG+dvxpQ4jqWlI9zN2LibsXE34wvkfrib8fG6Ght3M7Yx70YqPgAAAK9l/FUXAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAhGSgnX19fHbW1t7nw+n3dnd+zYoYxiPT097mwURdLZIyMj3erXgJeUlMRVVVXu/KRJk9zZrq4uZRQbGRlxZ3fv3i2dnc/n5btRn5stW7a4s4ODg8ooNjo66s4WFhZKZ/f19cl3U1tbG7e0tLjze/bskWZSKK+TpqYm6exly5bJd2NmVlpaKr2ulPcc9dkvLy93Z/v7+93ZOI4tjmPtTcrMysrK4urqand+4sSJ7qx6N8p71PDwsHR2HMfys5NOp+NMJuPODwwMuLNFRUXKKNK9FxcXS2evW7dOvptUKhUr/xxlu4Pys0eVy+XUXzLm3UjFp62tzZYuXerOKw/Trbfeqoxi999/vzur/gDr6OiQ95hUVVXZZZdd5s7ffvvt7uy3vvUtaZbOTv/4d9xxh3R2f3+/fDfqc/Oxj33Mnf3Nb34jzaIUB+UHrpnZokWL5LtpaWmxhx9+2J1fuHChO6u+UShvhv/0T/8knR1FUaLdQFVVVXb55Ze780NDQ+7s1772NWmWOXPmuLMvvPCCO6uU8cNVV1fb9ddf787ffPPN7uzXv/51aRblPWrDhg3S2cPDw/Kzk8lkbO7cue78yy+/7M62trZKs3zqU59yZ6dNmyad/frXv16+m+LiYps5c6Y7rzyfHR0d6jhuBw4ckPK5XG7Mu+GvugAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGNLKir1790pfr6+srFC/Xv/v//7v3dn/+I//kM5OoqioyCZPnuzOv+9973NnlR1BZgf/nLyUnUJJLVu2TNoDde2117qzqZT0CEt7c9LptHR2EoWFhVZTU+POd3d3u7Pq/rtTTjnFnd20aZN0dlLZbNZ27drlzp900knu7IIFC6RZNm/e7M62t7e7s8qf6eFGR0elX6usTlCeBTOT1vV8/vOfl85Wd3uZHfx50tvb686fe+657qyyZ9Hs4PoML3UH3l+C8vNEWYVhpq24UFcIjbe6iU98AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAY0vf9l5SU2KxZs9z522+/3Z1duXKlMoqdffbZ7mw+n5fOTiKbzdrOnTvd+ebmZnf2V7/6lTTLiSee6M6qKx9GR0elvJlZfX29XXTRRe68st6ivr5emqWoqMidVe8miVwuZ/v27XPn161b584effTR0izK+pKCgr/MvzOp97N+/Xp39uqrr5ZmueOOO9zZM88805194oknpDkO6e/vtyVLlrjzyhoKdU2Est7lxhtvlM6+9dZbpbyZ2eDgoK1YscKdv/LKK93ZhoYGaZa+vj53Vl2HkcTo6Kh1dXW589ls1p1V1suYmbTmSVmB9cfwiQ8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgiEtI0qlUtKekqefftqdvfPOO5VRbO3ate7s3LlzpbOXLl0q5c0O7gM7cOCAO19YWOjOnn/++dIsyv6T3bt3S2f//Oc/l/JJKPuoSkpKpLPr6urc2crKSunsJA4cOGCLFi1y55X5Ozo6pFkaGxvd2X/7t3+Tzk4qnU5bU1OTO3/DDTe4szNmzJBmyWQy7qzymk3yfmN2cKedsiNL2aOk7g9T9lFVV1dLZydRUlJi06dPd+eVmdTXlbITTvnz/FMo+xCVmXK5nDRHVVWVOxvHsXT2ePjEBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCIa2siOPYRkdH3fnHH3/cnX311VeVUewDH/iAO/uTn/xEOjuJKIqsuLjYnU+l/Ff/pje9SZqlpaXFnS0tLZXOTrKyYmhoyNrb2915ZfWHco9mZkVFRUdkjqRSqZTV1ta689OmTXNnP/vZz0qzDA8Pu7PHHXecdHZSmUxGWj3wxS9+0Z2dP3++NIuyluGhhx5yZ3t7e6U5Dunr67NnnnnGnb/yyivdWeW9zMxs2bJl7uxb3/pW6ewkRkdHpRUdynPzsY99TJrl05/+tDt7wQUXSGcnUVhYKK2KWLFihTurvucod/OWt7xFOns8fOIDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBEcRz7w1HUZWadR26cvxlT4jhuUH4BdzM27mZs3M34Arkf7mZ8vK7Gxt2Mbcy7kYoPAADAaxl/1QUAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwUgp4bq6uri1tdV/eMp//ODgoDKKDQ0NHZE5zMza29u71f0npaWlcXV1tTtfUlLizg4PDyuj2OjoqDubyWSkszs7O+W7SaVScVFRkZKXZlJUVla6s7W1tdLZK1eulO+muLg4Li0tdeeVexwZGVFGkZ4z9fVqZvLdmOn309fX586qz77y7EycONGd7ejosO7u7kgaxg6+51RVVbnzAwMD7qz656v8fhsatMdg2bJl8rOTTqdj5c9X+bNV51fWQu3bt086O8n7cWVlZdzY2OjOFxYWurN79uxRRrHu7m539uSTT5bOHu+5kX7CtLa22pNPPunO19TUuLNr1qxRRrG1a9e6s+qDevbZZ8sL3Kqrq+26665z54899lh3dtOmTdIsysM0Y8YM6exrrrlGvpuioiKbPn26O6/8eSklz8zs/PPPd2cvueQS6ewpU6bId1NaWmrnnnuuOz958mR3Vn1uOjo63Nnly5dLZ1vCpYjq/Tz33HPurPJMmpk0x7/8y7+4s3PnzpXmOKSqqsquvvpqd37ZsmXu7MqVK6VZFixY4M5++MMfls6Ookh+djKZjJ144onu/AUXXODOfuhDH5Jmyefz7uyjjz4qnX3VVVfJd9PY2Gi33367O6/8C+B9990nzfJf//Vf7uzSpUuls8d7bvirLgAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIhrSyIpVKWV1dnTuv7M1pb29XRrFJkya5szfccIN0dhKpVEpa0aGsHlB3LmWzWXd2165d0tlJDA0NSStGlN1Myq4iM20dhvJnlFRpaamdcMIJ7vwxxxzjzs6ZM0eaRXkNKitXzMweeOABKX9ILpeT9v+Ul5e7s+l0WpplyZIl7qyy/kN9fR+STqetqanJnX/Tm97kzp500knSLDt27HBn//u//1s6O4mBgQFprcqNN97oziq7q8zMNmzY4M6qq0KSyOfz0jOn7Ae89NJLpVlOOeUUd/bb3/62dPZ4+MQHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIIhrazIZrPSV5MrKytGR0eVUaS1BqtXr5bOTqKoqMhaW1vdeWWtxN69e6VZiouL3Vn169eTiOPYhoeH3fn9+/e7s8pXnpuZnX322e6s+kwmUVFRYeecc447Pzg46M6mUtLLW/r9NjY2Smf/KSsrlOdBeQ2qz75yP8rvV1nJcbg4jqX3ka1bt7qzJSUl0izKe1TSZ0GRz+elnz8FBf7PAJR7NDNbs2aNOxvHsXR2EmVlZTZ37lx3fvfu3e7sxIkTpVmUlUPnn3++dPZ4+MQHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGQlvmo+0+ef/55d/aZZ55RRrHa2lp3try8XDq7t7dXypsd3H/y+te/3p3v7Ox0Z7u6uqRZfv3rX7uzlZWV0tlJFBYWWkVFhTs/adIkd/atb32rNMvUqVPdWWW/WFIFBQXS3rktW7a4s8rONjNtt1d1dbV0dlKFhYXSP0t57RYVFUmzKDuaVq1a5c4qu5AOV1RUZC0tLe68so9qxYoV0iwdHR3u7FNPPSWdnURdXZ1deOGF7ryyw2/Tpk3SLAcOHHBn1WfyL0F5X9i5c6d09r59+9zZBx98UDr73HPPHfM/4xMfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAiGtLIil8tJX7/929/+1p097rjjlFHsu9/9rjubZAWFKp1O24QJE9z59vZ2d1b9GnNlHca6deuks5NIpVJWX1/vzk+fPt2dVc41M3vppZfcWXXVSRLZbNa2b9/uzitrE+bNmyfNcsYZZ7izn//856WzkyooKLCSkhJ3vr+/353t7u6WZlHeR5Q5khoZGZGeh2w2684qr0Ez7b2+qalJOltdg2Cmv+dUVVW5s8qdmx1c9eT1l1hZkcvlpNVTU6ZMcWfLysqkWfbu3evOPvHEE9LZ4+ETHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEI4rj2B+Ooi4z8y+Ceu2aEsdxg/ILuJuxcTdj427GF8j9cDfj43U1Nu5mbGPejVR8AAAAXsv4qy4AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEIyUEi4rK4tramr8h6f8x0dRpIwiUc/etGlTt/o14NXV1XFzc7M7n8/n3dmenh5lFCspKTkic5iZbdu2Tb6boqKiuLS01J0vLi52Z9U/21wu585mMhnp7K1bt8p3k06nY+WfMzQ05M6Ojo4qo1htba07q7y2zcx2794t342ZWU1NjfS6GhkZcWc3b94szaL8OfX19bmz+Xze4jiW3wArKirihgb/lVZVVbmzBw4ckGbZvXv3ETvbzORnp66uLm5tbXXnt2/f7s6q7zllZWXurLpJoaOjQ76bioqKuL6+3p1Xfkbs3btXGUW6m+rqaunstWvXjnk30rtXTU2NfeQjH3HnlctNp9PKKFZQ4P+wSn2TvuSSS+Q9Js3Nzfb973/fnVfeoL/3ve9Js8ycOdOdHR4els7+5Cc/Kd9NaWmpveENb3DnjzrqKHe2sLBQmkV5033d614nnf3xj39cvptMJmNz585151etWuXOdnd3S7NccMEF7mxjY6N09le/+tVEu4Gam5vthz/8oTu/bds2d1Z5LzMzmz59ujv77LPPurMDAwPSHIc0NDTYv/7rv7rzb3nLW9zZp556Sprla1/72hE72xLslWptbbXHH3/cnf/MZz7jzhYVFUmznHbaae6s+n585ZVXyndTX18v/X6V5/MnP/mJNMupp57qzr7jHe+Qzj755JPHvBv+qgsAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgiHtcigoKDBl59IZZ5zhzv72t79VRrFXXnnFnW1qapLOTiKdTpuyUyibzbqzH/zgB6VZXn75ZXdWWYGQVC6Xs3379rnzym6YF198UZrllFNOcWff/va3S2d//OMfl/JmB/f+KGs3brvtNndW2eNkZva+973Pnf3xj38snf3Vr35Vyh8yMDAgPc/KmpzZs2dLsyirRZSVFUnl83kbHBx05zs7/dsNlB1KZmbnn3++O5tgZYVsYGDAfvOb37jzyl4vda2EcvbkyZOls5Ooq6uzyy+/3J2/66673NnKykpplocfftidVbrHH8MnPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDGllRSaTsWOPPdadV7LLly9XRrEf/vCH7uz8+fOls5MYGhqy9evXu/PKV5NXV1dLsyxevNidLS8vl85OorS01ObMmePOb9u2zZ3t7u6WZlHufdKkSdLZSbS1tdndd9/tzq9cudKd7e/vl2Z5//vf784uWLBAOjupKIqsqKjInVeyb3rTm6RZnn76aXe2uLjYnVXWThxuZGTENm/e7M4r61pqamqkWZTXSiaTkc4eGhqS8mYH73TFihXu/Omnn+7Orl69WpqloqLCnVXf65NQ18CsWbPGnf3oRz8qzXLzzTe7s8pr6o/hEx8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABEPa1TUyMmJbtmxx5xcuXOjO7t27VxnF2tvb3dlcLiednUQcx9LOnalTp7qzv/3tb6VZlP1bTU1N0tlJFBYWSjMtWrTInT3ppJOkWd75zne6syUlJdLZSWSzWdu1a5c7f+GFF7qz9913nzTLVVdd5c7edNNN0tl/KQcOHHBnp02bJp392GOPubPTp093Z5WdUofLZrO2detWd763t9edPfHEE6VZlNf3GWecIZ39xBNPSHkzs76+Pnvuuefc+Q0bNrizyjNmZtbc3OzOzp49Wzo7if7+fnvppZfc+VmzZrmzBQXaZymtra3u7I033iidPR4+8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYEgrKwoLC626utqd7+rqcmcHBgaUUaympsadVb9iPKlUyn+dylqGnp4eaY577rnHnb3iiiuks5PI5XLSSpKdO3e6s+pXvCvrP9Q1KklFUeTOPvXUU+5sW1ubNMfpp5/uzip/Rn8K9dlJp9Pu7Lve9S5pFuXZ+ehHPyqdncTo6Kh1d3e780uWLHFnzzzzTGmWPXv2uLPK2hWzZCsr9u7da4888og739DQ4M5eeuml0izKa+WnP/2pdHYSZWVldtppp7nzn/vc59xZZf2O2cH1GV4nn3yydPayZcvG/M/4xAcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwYjiOPaHo6jLzDqP3Dh/M6bEcexf3mLczXi4m7FxN+ML5H64m/HxuhobdzO2Me9GKj4AAACvZfxVFwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEI6WEa2pq4paWFne+t7fXnR0eHlZGsYYG/3qSsrIy6exly5Z1q/tPSkpK4qqqKnd+79697qx6N6mU/49VuUczsx07dsh3U1hYGCszTZo0yZ2tqalRRrHu7m53dsuWLdLZ+XxevpuioqK4pKTEnc9ms+7s4OCgMoqksrJSyu/fv1++GzOz2tpa6T1H+T0PDQ1JsyjPcH19vTvb0dFh3d3dkTSMmVVVVcVNTU3ufC6Xc2f7+vqkWfL5vDurvmbb29vlZ6egoCAuKPD/e315ebk7q75nKq/v/fv3S2d3dnbKd5NKpeJ0Ou3OK6+ToqIiZRRT5qitrZXO3rJly5h3IxWflpYWe/jhh935Bx54wJ3dtGmTMordcMMN7uwpp5winR1FkbzAraqqyi677DJ3XrnHV199VZqlrq7Onb3uuuuksz/zmc/Id5NKpWzixInu/Be/+EV39uKLL5Zm+c53vuPOfvSjH5XO7uvrk++mpKTETj/9dHd+x44d7uzy5culWZQfFPPmzZPOfuyxxxItRWxpabH/+Z//cedXrVrlzq5evVqaRfmBd80117izc+fOleY4pKmpyb71rW+58/v27XNnFy9eLM3S39/vzr73ve+Vzn7jG98oPzsFBQVSmTnzzDPd2WuvvVaaZdasWe7sL3/5S+nsa6+9Vr6bdDptRx99tDuvvKaUf2k1M2tsbHRnP/CBD0hnz58/f8y74a+6AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAY0sqKkZERabWE8nXU6l6h559/3p1N+pXwikwmYzNmzHDnlf1Dc+bMkWZ55pln3NmXX35ZOjuJkZER6+z0f7P6cccd586q869fv96dzWQy0tnqfiOzgzuOlK/7V1Z/qPNs3LjRna2oqJDOTiqXy0n7i5T9gMqeKzOzrVu3urPKnreRkRFpjsMpO7KU/WqzZ8+W5mhvb3dn9+zZI52dRC6Xk1Z0KKsWlD1sZtozWV1dLZ2dRHFxsU2bNs2dnz59ujur7tNSVt+sXLlSOns8fOIDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGQVlbEcSx9RXpbW5s7OzQ0pIxiL774ojurfOV2UpWVlXbeeee584899pg7G0WRNIvyNeCLFi2Szk4iiiJLpfyP2qpVq9xZ9evjla+xP+ecc6SzH3roISlvZjY6Omq7d+9255W1A8cff7w0S3l5uTv79NNPS2cnNTQ0JD0PXV1d7qyyNkbNL1++3J1V1/Uckk6npVULq1evdmez2aw0i/KsbdiwQTo7iXQ6bQ0NDe78CSec4M6qq06Ue9++fbt0dhJ1dXV2ySWXuPPKn5fyPJqZFRT4P3upqqqSzh73n/tnOwkAAOBvHMUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIIh7erKZDJ2zDHHuPMvvPCCO9vY2KiMIu1WWbBggXR2EkVFRdKekoGBAXd2z5490iwzZsxwZ9V9Rb29vVLezKy4uNiOOuood/6MM85wZx9++GFplnQ67c6OjIxIZydRWFhodXV17vzw8LA7293dLc2iPDfKXjozsy9/+ctS/pDR0VHpmdu4caM7q+wJMjObPHmyO/voo4+6s8r+uMNlMhl73ete585v2rTJnX311VelWdauXevOvvTSS9LZSZSVldmpp57qzp999tnS2YqdO3e6s+quqyTiOLZcLufOK++Z69atk2aZOXOmO6t0jz+GT3wAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBjSyoooiiyTybjz2WzWnVW+FtvM7LjjjnNnly9fLp2dxMjIiHV2drrz5eXl7mx7e7s0i/KV6hdccIF09qpVq6S8mVk+n7ehoSEp71VZWSnNsn37dndWXeeRlPL18crrT13JoLwGb7/9dunspCsr1NeVst6loqJCmkV59r/5zW9KZycxMjJiW7dudecLCwvdWeX1ama2YcMGd3bRokXS2UkUFRXZ1KlT3fljjz3WnVXfA5XXrPpMJjE8PGzr16935xcvXuzOfv3rX5dmmT59ujt7ww03SGePh098AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABCMKI5jfziKuszMvzjntWtKHMcNyi/gbsbG3YyNuxlfIPfD3YyP19XYuJuxjXk3UvEBAAB4LeOvugAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMFJKuLq6Om5qanLnCwr8verAgQPKKLZ161Z3try8XDq7r6+vW/0a8LKysrimpsadr66udmez2awyiu3Zs8ed7e7uls42M/luMplMrPwZ9PT0uLPKM2amPQvpdFo6u6enR76b+vr6uK2tzZ1X/mzVb2XP5XLurPL8mpmtXLlSvhszs6qqqnjChAnuvHI/fX190iyjo6PubGFhoXRuLpeLpGHMrLa2Nm5tbXXnlbvJ5/PSLNu3b3dn1ddsPp+Xn53S0tK4qqrKna+oqHBnR0ZGlFGk15X6mt22bZt8N5WVlXFjY6Pyz3Bnj+T7cX19vXT2mjVrxrwbqfg0NTXZ3Xff7c6XlZW5s88884wyit14443u7Jw5c6SzFy1aJO8xqampsfnz57vzF154oTu7Y8cOaZaFCxe6s9/61reksy3Bjpfy8nJ7+9vf7s7fc8897qzyjJmZzZs3z51tbm6Wzr7nnnvku2lra7OlS5e68/fdd587q/ygNjPbt2+fO/uOd7xDOnvq1KmJdgNNmDDBvvGNb7jzDzzwgDv77LPPSrMohVz5lyDlX+IO19raar/4xS/c+QcffNCdHRgYkGb59Kc/7c6qr9n9+/fLz05VVZVdffXV7vyZZ57pzm7evFmaRSnYaqn61Kc+Jd9NY2Oj3X777e78TTfd5M6qHzKcccYZ7uwVV1whnX3iiSeOeTf8VRcAAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABENaWVFeXi59xfRdd93lzg4ODiqjmLJrRNmVktTQ0JCtW7fOnZ82bZo7+853vlOaRflqeuXP08xs8eLFUt7s4Fe2P/fcc+78lClT3Nk3vvGN0izKV6qrO9KSyOfz0rP/4x//2J1VdjOZafeu7Db6U/T399tLL73kzv/sZz9zZ2fMmCHNorxWfvnLX7qz6n6mQ3K5nPX29rrzGzdudGe//e1vS7PMmjXLnV27dq10dhKlpaV2wgknuPPKio5USvqxaRdffLE7++qrr0pnJ1FdXW0XXXSRO3/LLbe4s8r6HdXxxx//ZzuLT3wAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBjSd2/39PTYd7/7XXf+lVdecWfnzJmjjCLlX3jhBensJAYHB+03v/mNO19cXOzOvu1tb5NmUdZ5KDMnVVhYaNXV1e78iSee6M4qX5VvZrZ582Z3tru7Wzo7id27d9u///u/u/PKGopVq1ZJsyh/Ro8//rh0dlJVVVV2wQUXuPNPPPGEOzt58mRpFmWlx7Zt26SzkxgaGrL169e782eddZY7u3XrVmmWzs5Od3Z4eFg6OyllFYjy7KfTaWmOpqYmd7asrEw6O4k4jqU/g4aGBnd20qRJ0iwjIyPu7IQJE6Szx8MnPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIhrSra//+/dKOnsHBQXdW2YNjZjZx4kR3tqSkRDp73759Ut7MLJvN2q5du9x5ZUdJJpORZpk+fbo7+4Mf/EA6O4nq6mq78MIL3XllV9eGDRukWZQ/W3UnTxI9PT12//33u/NDQ0PubF1dnTSLstuor69POjup0tJSmzt3rjv/iU98wp3du3evNMujjz7qzir7mZLuhOvr67Pnn3/enb/++uvd2YqKCmkWZUfali1bpLO7urqkvNnBnz1r1qxx55X5Z8+eLc2Sy+Xc2ZqaGunsJHp6euzee+915z/4wQ+6s8rzaKY9C5/85Cels8fDJz4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAxpZYX6NeBXX321O6uuZTjqqKPc2UmTJkln79y5U8qbmaVSKWtoaJDyXu3t7dIsyjqM3t5e6ewkJk6caDfffLM7r3x9vLImxMysrKzMnVXuMalUKmXV1dXuvPKMPfLII9IsBw4ccGePOeYY6eyk4jiW1nScddZZ7qz67JSXl7uzzzzzjDsbRZE0xyGjo6PSOof169e7s29+85ulWQYGBtxZ5TVoZvbZz35WypuZ7dixw2699VZ3vq2tzZ0999xzpVmU1U27d++Wzk5ieHjYNm7c6M5ns1l3duvWrdIsynvfzJkzpbPH6yp84gMAAIJB8QEAAMGg+AAAgGBQfAAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYERxHPvDUdRlZp1Hbpy/GVPiOPYvRTLuZjzczdi4m/EFcj/czfh4XY2NuxnbmHcjFR8AAIDXMv6qCwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEI6WEq6qq4gkTJrjzRUVF7uzw8LAyihUU+Dtbe3u7dLaZdatfA15SUhJXVVW58xMnTnRnR0ZGlFFscHDQne3o6JDOjuNYvptMJhOXl5e7821tbe6sOn86nXZnoyiSzt6xY4d8N3V1dXFra+sRmUl9TSlnDwwMSGd3dHTId2NmVltbG0+aNMmdV95ztm3bJs2i/J6V12Aul7NcLqc9bGZWU1MTNzc3u/MlJSXu7N69e6VZstmsO9vX1yedvWfPHvnZqa+vj4/U+4i67SCTybizhYWF0tlbtmyR76ampiZuaWlx53t7e91Z5TkwO/jse6n33tvbO+bdSMVnwoQJdscdd7jzRx11lDu7YcMGZRQrLS11Z88++2zpbEuwx6SqqsouvfRSd/7mm292Z7ds2SLNsmLFCnf2qquuks7OZrPy3ZSXl9vb3vY2d/673/2uO3vFFVdIsyg/KIqLi6WzP/OZz8h309raao8//rg7r/xg37hxozSLcvbLL78snX355Zcn2g00adIk++lPf+rOT5482Z3953/+Z2mW5cuXu7OrVq1yZ3fu3CnNcUhzc7M9+OCD7vzxxx/vzj788MPSLN3d3e7s4sWLpbPvvfde+dlpa2uzpUuXuvPK+6D6w3369OnubHV1tXT2/Pnz5btpaWmxhx56yJ1XngX1XyYOHDjgzqofACxcuHDMu+GvugAAQDAoPgAAIBgUHwAAEAyKDwAACAbFBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGNLKioKCAlN2Lg0NDbmz/f39yijW1dXlzqqrB9QdR2ZmtbW1dskll7jzO3bscGfVlRXK18crawrM9K9rNzu4XmTu3Lnu/J133imdrdizZ487q35FehLZbNZ27drlzk+ZMsWdrayslGZZsmTJETs7qb6+Pnv++efd+W9+85vu7MKFC6VZ3v3ud7uz69evd2eVvYOHy+fz0k6wr3zlK+6sskrAzKRn+C/xuuru7rZ77rnHnVfeF2pqaqRZtm/f7s4qK3WSymQyNnPmTHdeWamzb98+aRblfUR51v8YPvEBAADBoPgAAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGDIKytKSkrceWV1Qj6fV0ax73znO+7sKaecIp29ePFiKW928G7Kysrc+b6+Pnd22rRp0izbtm1zZ9VVIUkUFBRIqyU2bdrkzq5bt06aRVm5Mm/ePOnsJNLptDU1NbnznZ2d7mxLS4s0i/J18+q9J5XNZqX1LqtWrXJn1ftR1hr8JQwNDdnq1avd+d7eXnf2xz/+sTTLnDlz3NmJEydKZycxMDBgL7/8sjt/2WWXubPq3ShrKNRVIUnkcjnbu3evO79hwwZ3dnR0VJpFWQ+l/Hz9Y/jEBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBkHZ15XI5acdUYWGhO6vscjIzmzBhgjt76qmnSmcn2dWVy+Vs//797ryyf0vZz2Sm7Zj6yle+Ip39j//4j1Le7OA+lo0bN7rzt912mzur7jHbvXu3O1tfXy+dnUQqlbKGhgZ3fmBgwJ1Vfq9mZmeccYY7e//990tnJ6XuB1y2bJk7q77nTJo0yZ1VdsIVFCT798+RkRHbvn27O6/sOlJ2gJmZNTY2urNDQ0PS2UnEcWyDg4Pu/Je//OUjNstZZ511xM5Ooq+vz5599ll3XtmjuXDhQmmW97znPe7sn3OPGZ/4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBoPgAAIBgUHwAAEAwpJUVFRUV9oY3vMGdf+WVV9zZrq4uZRT78Ic/7M7GcSydnUQul7Pe3l53XplJWYVhZtbW1ubO/sM//IN0dpKVFblcznp6etx5ZR3J61//emmW0dFRd3ZkZEQ6O4nR0VHr7u4+ImfPmDFDyisrOh555BHp7C984QtS/pA4jqU/h+bmZnd26dKl0ixvfvOb3dldu3a5s8oz+fu/TnnfrKurc2evv/56aZYdO3a4szNnzpTOTmLfvn32i1/8wp3ftm2bO3vllVdKs0RR5M4uWbJEOjuJwcFBaSVJbW3tEcmamT311FPubEtLi3T2ePjEBwAABIPiAwAAgkHxAQAAwaD4AACAYFB8AABAMCg+AAAgGBQfAAAQDIoPAAAIBsUHAAAEg+IDAACCQfEBAADBiJSdUVEUdZlZ55Eb52/GlDiOG5RfwN2MjbsZG3czvkDuh7sZH6+rsXE3YxvzbqTiAwAA8FrGX3UBAIBgUHwAAEAwKD4AACAYFB8AABAMig8AAAgGxQcAAASD4gMAAIJB8QEAAMGg+AAAgGD8X/4xxrFA2mM1AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 720x720 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(lane_keeper.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 1, 4, 'conv0', size=(8,2))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 4, 4, 'conv1', size=(5,5)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 8, 8, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LaneKeeper(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout(p=0.2, inplace=False)\n",
       "    (11): Conv2d(4, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "lane_keeper.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "lane_keeper.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "lane_keeper.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(lane_keeper, dummy_input, onnx_lane_keeper_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lane_keeper.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 5074.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[-0.02668524 -0.00435136]]\n",
      "Predictions shape: (1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_lane_keeper_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
