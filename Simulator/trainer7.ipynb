{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/lane_keeper_small.pt'\n",
    "onnx_lane_keeper_path = \"models/lane_keeper_small.onnx\"\n",
    "max_load = 150_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE\n",
    "# #very good\n",
    "# class LaneKeeper(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             nn.Linear(in_features=4*4*4, out_features=16),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(in_features=16, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# lane_keeper = LaneKeeper(out_dim=2,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class LaneKeeper(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 4, kernel_size=5, stride=1), #out = 28\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=14\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Conv2d(4, 4, kernel_size=5, stride=1), #out = 10\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=5\n",
    "            nn.Conv2d(4, 32, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            #normalize\n",
    "            # nn.BatchNorm1d(3*3*4),\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=1*1*32, out_features=16),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(in_features=16, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "lane_keeper = LaneKeeper(out_dim=2,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "lane_keeper.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = lane_keeper(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from time import time, sleep\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "\n",
    "\n",
    "\n",
    "    #convert to gray\n",
    "    img = cv.resize(img, (4*SIZE[1], 4*SIZE[0]))\n",
    "\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "        axes_length = (randint(10//4, 50//4), randint(50//4, 300//4))\n",
    "        angle = randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (50,50))\n",
    "    noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = np.clip(light, 0, 51)\n",
    "    light *= 5\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    # cv.imshow('light', light)\n",
    "    # if cv.waitKey(0) == ord('q'):\n",
    "    #     break\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 5), randint(1, 5)))\n",
    "\n",
    "    # cut the top third of the image, let it 640x320\n",
    "    img = img[int(img.shape[0]/3):,:] ################################# /3\n",
    "    # assert img.shape == (320,640), f'img shape cut = {img.shape}'\n",
    "\n",
    "    #edges\n",
    "    img = cv.resize(img, (2*SIZE[1], 2*SIZE[0]))\n",
    "\n",
    "    #edges    \n",
    "    img = cv.Canny(img, 100, 200)\n",
    "\n",
    "    #blur\n",
    "    img = cv.blur(img, (3,3))\n",
    "\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "        #add random tilt\n",
    "    max_offset = 3\n",
    "    offset = randint(-max_offset, max_offset)\n",
    "    img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        img[:offset, :] = 0 #randint(0,255)\n",
    "    elif offset < 0:\n",
    "        img[offset:, :] = 0 # randint(0,255)\n",
    "\n",
    "\n",
    "    # #reduce contrast\n",
    "    # const = np.random.uniform(0.1,0.8)\n",
    "    # # if np.random.uniform() > .5:\n",
    "    # #     const = const*0.2\n",
    "    # img = 127*(1-const) + img*const\n",
    "    # img = img.astype(np.uint8)\n",
    "\n",
    "    # #add noise \n",
    "    # std = 60\n",
    "    # std = randint(1, std)\n",
    "    # noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    # img = cv.subtract(img, noisem)\n",
    "    # noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    # img = cv.add(img, noisep)\n",
    "\n",
    "    # #add random brightness\n",
    "    # max_brightness = 60\n",
    "    # brightness = randint(-max_brightness, max_brightness)\n",
    "    # if brightness > 0:\n",
    "    #     img = cv.add(img, brightness)\n",
    "    # elif brightness < 0:\n",
    "    #     img = cv.subtract(img, -brightness)\n",
    "\n",
    "    # #blur \n",
    "    # img = cv.blur(img, (randint(1,3),randint(1,3)))\n",
    "\n",
    "    # # invert color\n",
    "    # if np.random.uniform(0, 1) > 0.6:\n",
    "    #     img = cv.bitwise_not(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(500):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(50)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        #classification label for road ahead = 1,0,0,0,1,0,0,0,0,0,0,0,0,0,0\n",
    "        road_images_indexes = []\n",
    "        tot_lines = 0\n",
    "        with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            tot_lines = len(lines)\n",
    "            for i,line in enumerate(lines):\n",
    "                if line == '1,0,0,0,1,0,0,0,0,0,0,0,0,0,0':\n",
    "                    road_images_indexes.append(i)\n",
    "        print(f'total pure road images: {len(road_images_indexes)}')\n",
    "        road_imgs_mask = np.zeros(tot_lines, dtype=np.bool)\n",
    "        road_imgs_mask[road_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            # self.all_imgs = torch.zeros((2*max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8) #adding flipped img\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            # road images specifically are added again along with their flipped image and label\n",
    "            road_imgs = torch.zeros((2*len(road_images_indexes), SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "            road_labels = []\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "            road_idx = 0\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(max_load)):\n",
    "\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                sample = line.split(',')\n",
    "                #keep only info related to the lane, discard distance from stop line \n",
    "                sample = [sample[0], sample[1], sample[3]] #e2=lateral error, e3=yaw error point ahead, curvature\n",
    "                reg_label = np.array([float(s) for s in sample], dtype=np.float32)\n",
    "\n",
    "                #img \n",
    "                img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "\n",
    "                #check if its in the road images\n",
    "                if road_imgs_mask[i]:\n",
    "                    img_r = load_and_augment_img(img.copy())\n",
    "                    # cv.imshow('imgR', img_r)\n",
    "                    img_r = img_r[:,:,np.newaxis]\n",
    "\n",
    "                    img_l = cv.flip(img, 1)\n",
    "                    img_l = load_and_augment_img(img_l)\n",
    "                    # cv.imshow('imgL', img_l)\n",
    "                    img_l = img_l[:,:,np.newaxis]\n",
    "                    # cv.waitKey(1)\n",
    "\n",
    "                    road_imgs[2*road_idx] = torch.from_numpy(img_r)\n",
    "                    road_imgs[2*road_idx+1] = torch.from_numpy(img_l)\n",
    "                    road_labels.append(reg_label)\n",
    "                    road_labels.append(-reg_label)\n",
    "                    road_idx += 1\n",
    "\n",
    "                else:\n",
    "                    img = load_and_augment_img(img)\n",
    "                    if i < 100:\n",
    "                        cv.imshow('img', img)\n",
    "                        cv.waitKey(1)\n",
    "                        if i == 99:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(reg_label)\n",
    "                    all_img_idx += 1\n",
    "\n",
    "            #cut imgs to the right length\n",
    "            road_imgs = road_imgs[:2*road_idx]\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "\n",
    "            #concatenate all_imgs and road_imgs\n",
    "            print(f'road images: {road_imgs.shape}')\n",
    "            print(f'all images: {self.all_imgs.shape}')\n",
    "            self.all_imgs = torch.cat((self.all_imgs, road_imgs), dim=0)\n",
    "            print(f'self.data shape: {len(self.data)}')\n",
    "            print(f'road_labels shape = {len(road_labels)}')\n",
    "            self.data = np.concatenate((np.array(self.data), np.array(road_labels)), axis=0)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "            #free road_imgs from memory\n",
    "            del road_imgs\n",
    "            del road_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pure road images: 79427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142527/142527 [14:28<00:00, 164.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "road images: torch.Size([158854, 32, 32, 1])\n",
      "all images: torch.Size([63100, 32, 32, 1])\n",
      "self.data shape: 63100\n",
      "road_labels shape = 158854\n",
      "\n",
      "all imgs: torch.Size([221954, 32, 32, 1])\n",
      "data: (221954, 3)\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 1, 32, 32])\n",
      "torch.Size([8192, 3])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    err_losses2 = []\n",
    "    err_losses3 = []\n",
    "    # curv_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        err2 = output[:, 0]\n",
    "        err3 = output[:, 1]\n",
    "        # curv_out = output[:, 2]\n",
    "\n",
    "        err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 1]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        # Compute the losses\n",
    "        err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        # curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = err_loss3 + err_loss2 + L1_loss + L2_loss #+ curv_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    err_loss2 = np.mean(err_losses2)\n",
    "    err_loss3 = np.mean(err_losses3)\n",
    "    # curv_loss = np.mean(curv_losses)\n",
    "    return err_loss2, err_loss3\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device=device):\n",
    "    lane_keeper.eval()\n",
    "    err_losses3 = []\n",
    "    err_losses2 = []\n",
    "    # curv_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = lane_keeper(input)\n",
    "\n",
    "        regr_out = output\n",
    "        err2 = regr_out[:, 0]\n",
    "        err3 = regr_out[:, 1]\n",
    "        # curv_out = regr_out[:, 2]\n",
    "\n",
    "        err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 1]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        # curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "        loss = err_loss3 + err_loss2\n",
    "\n",
    "        err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "    return np.mean(err_losses2), np.mean(err_losses3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30/30 \n",
      "yaw_err_loss3: 0.0142,   Val: 0.0144\n",
      "lat_err_loss2: 0.0017,   Val: 0.0017\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.001 #0.005\n",
    "epochs = 30\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 1e-4 #9e-4\n",
    "L2_lambda = 2e-2 #1e-2\n",
    "optimizer = torch.optim.Adam(lane_keeper.parameters(), lr=lr, weight_decay=9e-5) #wd = 2e-3# 3e-5\n",
    "\n",
    "regr_loss_fn = nn.MSELoss()\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        err_loss2, err_loss3 = train_epoch(lane_keeper, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_loss2, val_loss3 = val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs} \\nyaw_err_loss3: {err_loss3:.4f},   Val: {val_loss3:.4f}\")\n",
    "    print(f\"lat_err_loss2: {err_loss2:.4f},   Val: {val_loss2:.4f}\")\n",
    "    # print(f\"curv_loss: {curv_loss}\")\n",
    "    torch.save(lane_keeper.state_dict(), model_name)\n",
    "\n",
    "#Note: sweet spot for training is around 0.016 -> 0.020, also note that training can get stuck, and loss can start improve randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [00:00<00:00, 308.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lateral_err2_loss: 0.0017424924299120903\n",
      "yaw_err3_loss: 0.014439919963479042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "lane_keeper.load_state_dict(torch.load(model_name))\n",
    "err_loss2, err_loss3 = val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device)\n",
    "\n",
    "print(f\"lateral_err2_loss: {err_loss2}\")\n",
    "print(f\"yaw_err3_loss: {err_loss3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 5, 5)\n",
      "(4, 4, 5, 5)\n",
      "(32, 4, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(lane_keeper.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 2, 4, 'conv0', size=(10,5))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 8, 4, 'conv1', size=(10,20)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 4, 4, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LaneKeeper(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(4, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=16, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "lane_keeper.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "lane_keeper.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "lane_keeper.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(lane_keeper, dummy_input, onnx_lane_keeper_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lane_keeper.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 9646.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[-0.01456089  0.10664476]]\n",
      "Predictions shape: (1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_lane_keeper_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
